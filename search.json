{"entries":[{"title":"Programming Blog","url":"/index.html","tags":[],"date":null,"categories":[],"body":" page of "},{"title":"Unsubscribe","url":"/unsubscribe.html","tags":[],"date":null,"categories":[],"body":"I'm sorry to see you leaving :( Your email Unsubscribe You can always subscribe again. "},{"title":"Puzzle Driven Development","url":"/2009/03/04/pdd.html","tags":["pdd"],"date":"2009-03-04 00:00:00 +0000","categories":[],"body":"PDD, or Puzzle Driven Development, is a method used to break down programming tasks into smaller ones and enable their implementation in parallel. The PDD method is used widely in XDSD methodology. The method is pending a USPTO patent (application no. 12/840,306 ). Let's review the method with an example. Say, for instance, you are a programmer and have been tasked to design and implement a Java class. This is the formal task description: \"class DBWriter has to extend java.io.Writer abstract class and save all incoming data into the database\". You have one hour to implement this task. It is obvious to you that one hour is not enough because the problem is much bigger and requires more work than the slotted time allows. Additionally, there are a numerous unknowns: What information do we need to save, and in what format? What is the DB schema? Is it an SQL or NoSQL database? How to connect to the DB? JDBC? JPA? DAO? How to handle exceptions? Let's keep all these unknowns in mind as we try to solve the problem on the highest level of abstraction. Of course, we start with a test: import org.junit.* ; import static org . mockito . Mockito .*; public class DBWriterTest { @Test void testSavesDataIntoDatabase () throws Exception { DataBridge mapper = mock ( DataBridge . class ); Writer writer = new DBWriter ( mapper ); try { writer . write ( \"hello, world!\" ); } finally { writer . close (); } verify ( mapper ). insert ( \"hello, world!\" ); } } In the above test, we define the expected behavior of the class. The test fails to compile because there are two missing classes: DataBridge and DBWriter . Let's implement the bridge first: import java.io.IOException ; public interface DataBridge { void insert ( String text ) throws IOException ; } Next, the writer itself: import java.io.IOException ; import java.io.Writer ; import java.utils.Arrays ; public class DBWriter implements Writer { private DataBridge bridge ; public DBWriter ( DataBridge brdg ) { this . bridge = brdg ; } @Override void flush () throws IOException { } @Override void close () throws IOException { } @Override void write ( char [] cbuf , int off , int len ) throws IOException { String data = new String ( Arrays . copyOfRange ( cbuf , off , off + len )); this . bridge . insert ( data ); } } Using the above code, we solve the problem. In the example, we successfully designed, implemented and tested the required DBWriter class. Subsequently, the class can now immediately can be used \"as is\" by other classes. Of course, the implementation is not finished, since we are not writing anything to the database. Furthermore, we still aren't answering the majority of questions asked in the sample scneario. For instance, we still don't know exactly how the database needs to be connected, its type (SQL or NoSQL,) the correct data format and so on. However, we've already made a number of important architectural assumptions, which allowed us to implement the class and make it usable by other classes. Now it's time to identify the unknowns in our code and mark them with puzzles. Every puzzle is a request for refinement. We want to ask someone else to help us refine and correct our assumptions. Here is the first puzzle we need to add: public interface DataBridge { /** * @todo #123 I assumed that a simple insert() method will be * enough to insert data into the database. Maybe it's * not true, and some sort of transaction support may be * required. We should implement this interface and create * an integration test with a database. */ void insert ( String text ) throws IOException ; } The puzzle has three elements: @todo tag, #123 locator and a comment. Locator displays the following: \"The puzzle was created while working with ticket #123\". Let’s add one more puzzle: void write ( char [] cbuf , int off , int len ) throws IOException { // @todo #123 I assumed that the data should be sent to the database // as its received by the writer. Maybe this assumption // is wrong and we should aggregate data into blocks/chunks // and then send them to the data bridge. String data = new String ( Arrays . copyOfRange ( cbuf , off , off + len )); this . bridge . insert ( data ); } This puzzle indicates one of our concerns because we are not sure that the architectural decision is right. Actually, the design is very primitive at the moment and very likely to be incorrect. To refine it and refactor, we require more information from the task specifier. The task is finished. Now, you can reintegrate your branch into master and return the ticket to whoever assigned it to you. His task now is to find other people who will be able to resolve the puzzles we just created. Every puzzle created now will produce other puzzles, which will be resolved by other people. Consequtly, our simple one-hour task can potentially generate hundreds of other tasks, which may take days or even years to complete. Nevertheless, your goal of working with your specific task is to finish it as soon as possible and reintegrate your branch into master . Best Practices There are a few simple rules that help you to place puzzles correctly. First, you should put your @todo annotations at the point where your code hits a stub. For example, in a unit test. You're implementing a test and it fails because the class has not yet been implemented. You skip the test with the @Ignore annotation and add a @todo puzzle to its Javadoc. Second, your puzzle should remain as near as possible to the code element that is hitting the stub. Say thay you have a unit test that has three test methods. All of them fail now because the class has not been implemented. The best approach would be to ignore every one of them and create three (!) puzzles. Each one of the puzzles should explain what you expect from the class and how it should be implemented. Third, be as descriptive as possible. Your puzzle will soon be a task definition for someone else. So, explain clearly what you expect the next person to implement, how to do it, which documentation to use and so on and so forth. There should be enough information present that the next person assigned to the puzzles is ables implement your required classes without additional input from you! BTW, puzzle collection process can be automated by means of our PDD Ruby gem . "},{"title":"D29, a prototype","url":"/2013/12/29/proto.html","tags":["programming"],"date":"2013-12-29 00:00:00 +0000","categories":[],"body":"D29 is a prototype of a new programming language and a development platform. Well, actually, not a prototype yet, but just an idea. As it looks to me, the languages we have now (even the most modern ones) are still close to COBOL/C and far from being truely elegant and modern. Would be great if we can design a language/platform, which will be a mix of object oriented programming and functional programming, and will have all the features listed below, out-of-the-box. Key principles: everything is an object byte and bytes are the only built-in types strict compile-time static analysis Native support of: UML and OCL build automation aspect oriented programming continuous integration and delivery deployment , incl. staged one collective code ownership revision control test driven development integration testing mocking serialization (to XML, JSON, binary) documentation preferably with CNL requirements traceability components repository (a la Sonatype Nexus) RAII tracing (aka logging) internationalization and localization generic programming software licenses multithreading deep objects immutability manual testing performance testing (and others) assertions versioning class invariants design patterns (e.g. Adapter , Bridge and Decorator ) object cloning object persistence authentication and authorization ACID transactions distributed objects and their horizontal scalability asynchronous methods backward compatibility of runtime platforms object metadata, like life time, ownership, etc. Maybe native support of: cloud computing Features: no mutable objects ( why? ) no public/protected object properties no static properties/methods ( why? ) no global variables no pointers no NULL ( why? ) no scalar types, like int , float , etc. no unchecked exceptions no interface-less classes multiple inheritance (!) no operator overloading all methods are either final or abstract no mutability of method arguments no reflection no instanceof operator ( why? ) no root class (like, for example, Object in Java) instant object destruction instead of garbage collection Maybe: native support of Java classes/libraries compilation into Java byte code If interested to contribute, email me. Maybe we'll do something together :) "},{"title":"First Post","url":"/2014/04/06/introduction.html","tags":[],"date":"2014-04-06 00:00:00 +0000","categories":[],"body":"This is the first post on my new blog. Therefore, it's not about anything in particular - just an introduction and my way of saying hello. This blog will be primarily about software development ideas. As my About Me page says, I'm passionate about software quality, and will write solely about my ideas and views on it. Anyway, welcome to my new blog. Together, let's see how this works out! :) ps. BTW, I purchased the Cambria font just for this new blog. It cost, €98. Nevertheless, I think its a good investment for this new venture. "},{"title":"Movies for Thanasis","url":"/2014/04/06/movies-for-thanasis.html","tags":["movies"],"date":"2014-04-06 00:00:00 +0000","categories":[],"body":"Sometime ago, I recommended a list of movies to a friend of mine after he told me was losing all interest in \"Hollywood.\" Level C titles are supposed to be impossible to understand unless you've seen (and understood) their prequels -- listed in sections A and B. So, start browsing the lists in sections A and post your comments if you have any. :) Level A True Romance (1993) Pulp Fiction (1994) Kill Bill (2003) Doberman (1997) La fille sur le pont (1999) Irreversible (2002) Fear and Loathing in Las Vegas (1998) Perdita Durango (1997) Golden Balls (1993) The Million Dollar Hotel (2000) Y Tu Mamá También (2001) Reservoir Dogs (1992) Trainspotting (1996) Fight Club (1999) Arizona Dream (1992) Black Cat, White Cat (1998) Buffalo '66 (1998) Jamon Jamon (1992) Natural Born Killers (1994) Thursday (1998) Bullet (1996) Level B Delicatessen (1991) Bernie (1996) Hardmen (1996) Revolver (2005) Science of Sleep (2006) Cashback (2006) El Crimen Perfecto (2004) El día de la bestia (1995) La comunidad (2000) The Happiness of the Katakuris (2001) 99 francs (2007) Combien tu m'aimes? (2005) Ying xiong (2002) Brutti, sporchi e cattivi (1976) Level C What Just Happened (2008) Happiness (1998) Before the Devil Knows You Are Dead (2007) No Country for Old Men (2007) A Serious Man (2009) "},{"title":"PhantomJS as an HTML Validator","url":"/2014/04/06/phandom.html","tags":["phandom","phantomjs","testing"],"date":"2014-04-06 00:00:00 +0000","categories":[],"body":" I created phandom.org a few months ago, but yesterday finally found the time to make some needed changes to it. So, now is a good time to explain how I'm using Phandom in some of my unit tests. Before I get started, though, I should say a few words about phantomjs , which is a JavaScript interface for WebKit. WebKit, on the other hand, is a web browser without a user interface. WebKit is a C++ library that enables manipulation of HTML content, through DOM calls. For example, this is a simple JavaScript located code in example.js : 1 2 3 4 5 6 7 8 var page = require ( 'webpage' ). create (); page . open ( 'http://google.com' , function () { console . log ( 'loaded!' ); phantom . exit ( 0 ); } ); We run phantomjs from the command line with the following code: $ phantomjs example.js Phantomjs creates a page object (provided by webpage module inside phantomjs), and then asks it to open() a Web page. The object communicates with WebKit and converts this call into DOM instructions. After which, the page loads. The phantomjs engine then terminates on line 6. WebKit renders a web page with all neccessary components such as CSS, JavaScript, ActionScript, etc, just as any standard Web browser would. So far so good, and this is the traditional way of using phantomjs. Now, on to giving you an idea of how Phandom (which stands for \"PhantomJS DOM\") works inside Java unit tests: To test this, let's give phantomjs an HTML page and ask him to render it. When the page is ready, we'll ask phantomjs to show us how this HTML looks in WebKit. If we see the elements we need and desire, — we're good to go. Let's use the following example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import com.rexsl.test.XhtmlMatchers ; import org.hamcrest.MatcherAssert ; import org.phandom.Phandom ; public class DocumentTest { @Test public void rendersValidHtml () { Document doc = new Document (); // This is the method we're testing. It is supposed // to return a valid HTML without broken JavaScript // and with all required HTML elements. String html = doc . html (); MatcherAssert . assertThat ( XhtmlMatchers . xhtml ( new Phandom ( html ). dom ()), XhtmlMatchers . hasXPath ( \"//p[.='Hello, world!']\" ) ); } } When we use the above code, here is what happens. First, we get HTML html as a String from doc object, and then pass it to Phandom as an argument. Then, on line 13, we call the Phandom.dom() method to get an instance of the class org.w3c.dom.Document . If our HTML contains any broken JavaScript code, method dom() produces a runtime exception and the unit test faila. If HTML is clean and WebKit is able to render it without problems, the test passes. I'm using this mechanism in a few different projects,and it works quite well. Therefore, I highly recommend it. Of course, you shouldn't forget that you must have phantomjs installed on your build machine. In order to avoid unit test failures when phantomjs is not available or present, I've created the following supplementary method: public class DocumentTest { @Test public void rendersValidHtml () { Assume . assumeTrue ( Phandom . installed ()); // the rest of the unit test method body... } } Enjoy and feel free to report any bugs or problems you encounter to: Github issues :) "},{"title":"Xembly, an Assembly for XML","url":"/2014/04/09/xembly-intro.html","tags":["xembly","xml","xsl","xsd"],"date":"2014-04-09 00:00:00 +0000","categories":[],"body":" I use XML in almost every one of my projects. And, despite all the fuss about JSON/YAML, I honestly believe that XML is one of the greatest languages ever invented. Also, I believe that the beauty of XML reveals itself when used in combination with related technologies. For example, you can expose your data in XML and render it for the end-user using XSL stylesheet . Another example would be when you validate the same data, before rendering, to ensure that the structure is correct. You can do this with the XSD schema. Alternatively, you can pick specific data elements from the entire document by using XPath queries. Essentially, these three technologies, XSL, XSD schema and XPath, are what makes XML so powerful. However, there can be times when XML falls short. For instance, imagine you have an existing document that needs to be modified just slightly. For example, let's use the following: <accounts> [...] <acc id= '34' > <name> Jeffrey </name> <balance> 305 </balance> </acc> <acc id= '35' > <name> Walter </name> <balance> 50090 </balance> </acc> [...] </accounts> The above code represents a list of accounts. Each account has its own id and several child elements. In our example, we need to find the account belonging to Jeffrey and increase its balance by 500 . How would we do this? Well, there are a few possible solutions: SAX-parse the document, change the balance and save the stream; DOM-parse it, find the element with XPath, change the value and then print it; apply a parametrized XSL stylesheet; apply XQuery small script to make changes All of these methods have their own drawbacks. However, all of them have one particular problem in common — they are very verbose. With each of the above methods, you need at least a page of code to perform this rather simple operation. Furthermore, if the logic of the operation becomes more complex, the amount of needed code grows much faster than you may expect. Simply put, XML lacks a tool for primitive data manipulations within a document. Perhaps, it is this shortcoming that makes XML unpopular with some. Anyway, here is a tool I created a few month ago: Xembly . It is an imperative language with a few simple directives and resembles Assembly in style. Thus, the name - Xembly. With Xembly, there are no loops, conditions or variables - just a sequence of directives with arguments. Let's create a simple example. Say, for instance, we want to add a new account number 36 to our list document. The code would look like: 1 2 3 4 5 6 7 8 XPATH '/ accounts '; ADD ' account '; ATTR ' id ' , ' 36 '; ADD ' name '; SET ' Donny '; UP ; ADD ' balance '; SET ' 3400 '; The above should be intuitively clear, but I'll explain just in case. First, the XPATH directive points us to the element found by the \"/accounts\" XPath query. This will be our root element. We assume here that it exists in the document. Therefore, if it is absent, our Xembly script will fail with a runtime exception. Next, the ADD directive on line 2 creates a new XML element without any children or attributes. Then, the ATTR directive sets an attribute for this element. The code then adds the new child element name and sets its text value to \"Donny\" using the SET directive. Finally, we move our pointer back to account element using UP , add the balance child element and set its value to \"3400\" . Our balance changing task can be expressed in Xembly with the following code: XPATH '/ accounts / account [ name =\" Jeffrey \"]/ balance '; XSET ' . + 500 '; The XSET directive sets the element text value, similar to SET , but calculates it beforehand using the provided XPath expression . + 500 . Xembly performs all manipulations through DOM. Consequently, Xembly can be implemented inside any language that has a built-in DOM implementation. In the meantime, there is only one implementation of Xembly language — in Java. Here is how it works: 1 2 3 4 5 6 7 Iterable < Directive > directives = new Directives () . xpath ( \"/accounts\" ) . add ( \"account\" ) . attr ( \"id\" , \"36\" ) . add ( \"name\" ). set ( \"Donny\" ). up () . add ( \"balance\" ). set ( \"3400\" ); new Xembler ( directives ). apply ( document ); In this snippet, I'm using a supplementary script builder, Directives , which enables generation of directives in a fluent way. Then, I use Xembler class, which is similar to \"assembler\", to apply all specified directives to the document object of class org.w3c.dom.Document . Additionally, Xembly can be used to build XML documents from scratch and as a replacement for traditional DOM building. A quick example: System . out . println ( new Xembler ( new Directives (). add ( \"html\" ) . add ( \"head\" ) . add ( \"title\" ) . set ( \"Hello, world!\" ) ). xml () ); The above snippet produces the following output: <html> <head> <title> Hello, world! </title> </head> </html> For me, this appears to be more simple and compact. As usual, your bug reports and suggestions are always welcomed. Please send to Github issues :) "},{"title":"How much do you pay per line of code?","url":"/2014/04/11/cost-of-loc.html","tags":["xdsd","mgmt"],"date":"2014-04-11 00:00:00 +0000","categories":["jcg"],"body":" Yes, I know, \"line of code\" (LoC) is a very wrong metric . There are tons of articles written about it, as well as famous books. However, I want to compare two projects in which I have participated recently and discuss some very interesting numbers. Project #1: Traditionally Co-located The first project I was a part of was performed by a traditionally co-located group of programmers. There were about 20 of them (I'm not counting managers, analysts, product owners, SCRUM masters, etc.) The project was a web auctioning site with pretty high traffic numbers (over two million page views per day). The code base size was about 200k lines, of which 150k was PHP, 35k JavaScript and the remainder CSS, XML, Ruby, and something else. I'm counting only non-empty and non-comment lines of code, using cloc.pl . It was a commercial project, so I can't disclose its name. Brazil (1985) by Terry Gilliam The team was co-located in one office in Europe where everybody was working \"from nine 'til five\". We had meetings, lunches, desk-to-desk chats and lots of other informal communications. All tasks were tracked in JIRA. Project #2: Extremely Distributed The second project was an open source Java product, developed by an extremely distributed team of about 15 developers. We didn't have any chats or any other informal communications. We discussed everything in Github issues. The code base was significantly smaller with only about 30k lines, of which about 90% was Java and the rest in XML. Shaolin Temple (1982) by Chang Hsin Yen Maturity of Development Both projects hosted their code bases on Github. Both teams were developing in feature branches - even for small fixes. Both teams used build automation, continuous integration, pre-flight builds, static analysis and code reviews. This indicates the maturity of the project teams. Both projects satisfied the requirements of their users. I'm mentioning this to emphasize that both projects produced valuable and useful lines of code. There was no garbage and almost no code duplication . Show Me the Money In both projects, my role was called that of lead architect, and I knew their economics and financials. Besides that, I had access to both Git repositories, so I can measure how many new lines (or changed lines) were introduced by both teams in, say, a three-month period. Now, let's see the numbers. The first project (the one that was co-located) was paying approximately €50,000 annually to a good developer, which was about $5,600 per month or $35 per hour. The second one (the extremely distributed project) was paying $20-35 per hour, for completed tasks only according to one of the principles of XDSD . The first one, in three months, produced 59k new lines and removed 29k in changes in the master branch, which totals 88k lines of code. The project resulted in about 10,000 man hours to produce these lines (20 programmers, three months, 170 working hours per month) — which equates to about $350k. Therefore, the project cost a whopping $3.98 per line The second project, in the same three month period, produced 45k new lines and removed 9k, which comes to 54k in all. To complete this work, we spent only $7k (approximately 350 working hours in 650 tasks). Thus, the project cost merely: ¢13 per line This also means that programmers were writing approximately 150 lines per hour or over a thousand per day. The Mythical Man-Month talks about 10 lines per day, which is a hundred times less than we saw in our project. $350k vs $7k, $3.98 vs ¢13? What do you think? How to Validate the Numbers? If you're curious, I'm using hoc to get the numbers from Git (it is explained in Hits-of-Code Instead of SLoC ). You can validate the numbers for the second project here on Github: jcabi/jcabi-github . Conclusion What I'm trying to express with these numbers is that distributed programming is much more effective, money-wise, than a co-located team. Again, I can hear you saying that \"line of code\" is not a proper metric. But, come on, $0.13 vs. $3.98? Thirty times more expensive? The Big Lebowski (1998) by Joel Coen It's not about metrics any more. It's about preventing wasteful man hours and the huge waste of money that comes with them? Can We Do the Same? Of course, the same results can't be achieved by just telling your programmers to work from home and never come to the office. XDSD is not about that. XDSD is about strict quality principles, which should be followed by the entire team. And when these principles are in place — you pay thirty times less. By the way, this is what people say about their projects: $12–103: crazyontap.com $15–40: betterembsw.blogspot.nl over $5: joelonsoftware.com What are your numbers? Please post your comments below. "},{"title":"Fluent Java HTTP Client","url":"/2014/04/11/jcabi-http-intro.html","tags":["jcabi","http","java"],"date":"2014-04-11 00:00:00 +0000","categories":[],"body":" In the world of Java, there are plenty of HTTP clients from which to choose. Nevertheless, I decided to create a new one because none of the other clients satisfied fully all of my requirements. Maybe, I'm too demanding. Still, this is how my jcabi-http client interacts when you make an HTTP request and expect a successful HTML page in return: 1 2 3 4 5 6 7 8 String html = new JdkRequest ( \"https://www.google.com\" ) . uri (). path ( \"/users\" ). queryParam ( \"id\" , 333 ). back () . method ( Request . GET ) . header ( \"Accept\" , \"text/html\" ) . fetch () . as ( RestResponse . class ) . assertStatus ( HttpURLConnection . HTTP_OK ) . body (); I designed this new client with the following requirements in mind: Simplicity For me, this was the most important requirement. The client must be simple and easy to use. In most cases, I need only to make an HTTP request and parse the JSON response to return a value. For example, this is how I use the new client to return a current EUR rate: 1 2 3 4 5 6 String rate = new JdkRequest ( \"http://www.getexchangerates.com/api/latest.json\" ) . header ( \"Accept\" , \"application/json\" ) . fetch () . as ( JsonResponse . class ) . json (). readArray (). getJsonObject ( 0 ) . getString ( \"EUR\" ); I assume that the above is easy to understand and maintain. Fluent Interface The new client has to be fluent, which means that the entire server interaction fits into one Java statement. Why is this important? I think that fluent interface is the most compact and expressive way to perform multiple imperative calls. To my knowledge, none of the existing libraries enable this type of fluency. Testable and Extendable I'm a big fan of interfaces, mostly because they make your designs both cleaner and highly extendable at the same time. In jcabi-http , there are five interfaces extended by 20 classes. Request is an interface, as well as Response , RequestURI , and RequestBody exposed by it. Use of interfaces makes the library highly extendable. For example, we have JdkRequest and ApacheRequest , which make actual HTTP calls to the server using two completely different technologies: (JDK HttpURLConnection and Apache Http Client, respectively). In the future, it will be possible to introduce new implementations without breaking existing code. Say, for instance, I want to fetch a page and then do something with it. These two calls perform the task differently, but the end results are the same: String uri = \"http://www.google.com\" ; Response page ; page = new JdkRequest ( uri ). fetch (); page = new ApacheRequest ( uri ). fetch (); XML and JSON Out-of-the-Box There are two common standards that I wanted the library to support right out of the box. In most cases, the response retrieved from a server is in either XML or JSON format. It has always been a hassle, and extra work, for me to parse the output to take care of formatting issues. jcabi-http client supports them both out of the box, and it's possible to add more formats in the future as needed. For example, you can fetch XML and retrieve a string value from its element: String name = new JdkRequest ( \"http://my-api.example.com\" ) . header ( \"Accept\" , \"text/xml\" ) . fetch () . as ( XmlResponse . class ) . xml (). xpath ( \"/root/name/text()\" ). get ( 0 ); Basically, the response produced by fetch() is decorated by XmlResponse . This then exposes the xml() method that returns an instance of the XML interface. The same can be done with JSON through the Java JSON API ( JSR-353 ). None of the libraries that I'm aware of or worked with offer this feature. Immutable The last requirement, but certainly not the least important, is that I need all interfaces of the library to be annotated with @Immutable . This is important because I need to be able to encapsulate an instance of Request in other immutable classes. ps. A short summary of this article was published at JavaLobby "},{"title":"PDD by Roles","url":"/2014/04/12/puzzle-driven-development-by-roles.html","tags":["xdsd","pdd","mgmt"],"date":"2014-04-12 00:00:00 +0000","categories":[],"body":" In this post, I'll try to walk you through a project managed with the spirit of Puzzle Driven Development (PDD). As I do this, I will attempt to convey typical points of view of various project members. Basically, there are six key roles in any software team: Project Manager — assigns tasks and pays on completion System Analyst — documents the product owner's ideas Architect — defines how system components interact Designer — implements most complex components Programmer — implements all components Tester — finds and reports bugs Everybody, except the project manager, affects the project in two ways: they fix it and they break it at the same time. Let me explain this with a simple example. Fix and Break Let's assume, for the sake of simplicity, that a project is a simple software tool written by me for a close friend. I created the first draft version 0.0.1 and delivered it to him. For me, the project is done. I've completed the work, and hopefully will never have to return to it again. However, the reality of the project is very different. In just a few hours, I receive a call from my friend saying that a he's found a few bugs in the tool. He is asking me to fix them. Now, I can see that the project is not done. In fact, it's broken. It has a few bugs in it, which means a few tasks to complete. I'm going to fix the project, by removing the bugs. I implement a new version of the software, name it 0.0.2 and ship it to my friend. Again, I believe my project is finished. It is fixed and should be closed. This scenario repeats itself again and again until my friend stops calling me. In other words, until he stops breaking my project. It is obvious that the more my friend breaks my project, the higher the quality of the software delivered ultimately at the end. Version 0.0.1 was just a very preliminary version, although I considered it final at the time I released it. In a few months, after I learn of and fix hundreds of bugs, version 3.5.17 will be much more mature and stable. This is the result of this \"fix and break\" approach. The diagram shows the relation between time and mess in the project. The bugs my friend is reporting to me are breaking the project, increasing its instability (or simply its messiness). New versions I release resolve the bugs and are fixing the project. Your Github commit dynamics should resemble this graph, for example: When the project starts, its messiness is rather low, and then it starts to grow. The messiness then reaches its peak and starts to go down. Project Manager The job of a project manager is to do as much as possible to fix the project. He has to use the sponsor's time and money in order to remove all bugs and inconsistencies and return the project back to a \"fixed\" state. Pulp Fiction (1994) by Quentin Tarantino When I say \"bugs,\" I mean more than just software errors but also: unclear or ambiguous requirements features not yet implemented functional and non-functional bugs lack of test coverage unresolved @todo markers lack of risk analysis etc. The project manager gives me tasks that he wants done in order to fix and stabilize the project to return it back to a bug-free state. My job, as a member of a software team, is to help him perform the needed fixes and, at the same time, do my best to break the project! In the example with my friend, he was breaking the project constantly by reporting bugs to me. This is how he helped both of us increase the final quality of the product. I should do the same and always try to report new bugs when I'm working on some feature. I should fix and break at the same time. Now let's take a closer look at project roles. System Analyst A product owner submits an informal feature request, which usually starts with \"it would be nice to have...\" I'm a system analyst and my job is to translate owner's English into formal specifications in the SRS, understandable both by programmers and myself. It's not my responsibility to implement the feature. Arizona Dream (1992) by Emir Kusturica My task is complete when a new version of the SRS is signed by the Change Control Board. I'm an interpreter for the product owners, translating from their language to formal language needed in the SRS document. My only customer is the product owner. As soon as she closes the feature request, I'll be paid. Besides feature requests from product owners, I often receive complaints about the quality of the SRS. The document may not be clear enough for some team members. Therefore, it's my job to resolve clarity problems and fix the SRS. These team members are also my customers. When they close their bug reports, I'll be paid. In both cases (a feature request or a bug,) I can make changes to the SRS immediately - if I have enough time. However, it's not always possible. I can submit a bug and wait for its resolution; but, I don't want to keep my customers waiting. This is where puzzle driven development helps me. Instead of submitting bug reports, I add \" TBD \" puzzles in the SRS document. The puzzles are informal replacements of normally very strict formal requirements. They satisfy my customer, since they are in plain English, and are understandable by technical people. Thus, when I don't have time, I don't wait. I change the SRS using TBDs at points where I can't create a proper and formal description of the requirements or simply don't know what to write exactly. Architect Now, I'm the architect, and my task is to implement a requirement, which has been formally specified in the SRS. PM is expecting a working feature from me, which I can deliver only when the architecture is clear and classes have been designed and implemented. The Science of Sleep (2006) by Michel Gondry Being an architect, I'm responsible for assembling all of the components together and making sure they fit. In most cases, I'm not creating them myself, but I'm telling everybody how they should be created. My work flow of artifacts is the following: directed graph digraph G { SRS -> UML; UML -> \"Source code\"; } G SRS SRS UML UML SRS->UML Source code Source code UML->Source code I receive requirements from the SRS, produce UML diagrams and explain to designers how to create source code according to my diagrams. I don't really care how source code is implemented. I'm more concerned with the interaction of components and how well the entire architecture satisfies functional and non-functional (!) requirements. My task will be closed and paid when the system analyst changes its state to \"implemented\" in the SRS. The system analyst is my only customer. I have to sell my solution to him. Project manager will close my task and pay me when system analyst changes the status of the functional requirement from \"specified\" to \"implemented\". The task sounds big, and I have only half an hour. Obviously, puzzle driven development should help me. I will create many tickets and puzzles. For example: SRS doesn't explain requirements properly Non-functional requirements are not clear UML diagrams are not clear enough Components are not implemented Build is not automated Continuous integration is not configured Quality of code is not under control Performance testing is not automated When all of my puzzles are resolved, I can get back to my main task and finish feature implementation. Obviously, this may take a long time - days or even weeks. But, the time cost of the main task is less than an hour. What is the point of all this hard work? Well, it's simple; I'll earn my hours from all the bugs reported. From this small half-an-hour task, I will generate many tickets, and every one of them will give me extra cash. Designer and Programmer The only real differences between designer and programmer are the complexity of their respective tasks and the hourly rates they receive. Designers usually do more complex and higher level implementations, while programmers implement all low-level details. Pulp Fiction (1994) by Quentin Tarantino I'm a programmer and my task is to implement a class or method or to fix some functional bug. In most cases, I have only half an hour available. And, most tasks are bigger and require more time than that. Puzzle driven development helps me break my task into smaller sub-tasks. I always start with a unit test. In the unit test, I'm trying to reproduce a bug or model the feature. When my test fails, I commit it and determine the amount of time I have left. If I still have time to make it pass — I do it, commit the changes and report to the project manager. If I don't have time to implement the fix, I mark pieces of code that don't already have @todo markers, commit them and report to the project manager that I've finished. As you see, I'm fixing the code and breaking it at the same time. I'm fixing it with my new unit test, but breaking it with @todo puzzles. This is how I help to increase the overall quality of the project - by fixing and breaking at the same time. Tester I'm a tester and my primary motivation is to find bugs. This may be contradictory to what you've heard before; but in XDSD , we plan to find a certain amount of bugs at every stage of the project. Fear and Loathing in Las Vegas (1998) by Terry Gilliam As as a tester, I receive tasks from my project manager. These tasks usually resemble \"review feature X and find 10 bugs in it\". The project manager needs a certain number of bugs to be found in order to fix the project. From his point of view, the project is fixed when, say, 200 bugs have been found. That's why he asks me to find more. Thus, to respond to the request, i find bugs to do my part in regard to the \"fixing\" part of the bigger picture. At the same time, though, I can find defects on my own and report them. This is the \"breaking\" part of my mission. "},{"title":"Bugs Are Welcome","url":"/2014/04/13/bugs-are-welcome.html","tags":["testing","xdsd","mgmt"],"date":"2014-04-13 00:00:00 +0000","categories":[],"body":"The traditional understanding of a software defect (aka \"bug\") is that it is something negative and want to avoid in our projects. We want our projects to be \"bug-free.\" Our customers are asking us to develop software that doesn't have bugs. And, we, as users, expect software to work without bugs. Charlie and the Chocolate Factory (2005) by Tim Burton But, let's take a look at bugs from a different angle. In XDSD , we say that \"bugs are welcome.\" This means we encourage all interested parties to find bugs and report them. We want our team to see bugs as something that we need in our projects. Why? Because we understand that there are two categories of bugs: visible and hidden. The more bugs that become visible, the more of them we can fix. More fixed bugs means fewer to annoy our users. By discovering bugs we make them visible. This is the primary job of a software tester — to make bugs visible. Obviously, their visibility affects the quality of the product in a positive way. This is because we can fix them before our users start complaining. In order to motivate all team members to make more bugs visible, we pay for their discovery. In XDSD projects, we are pay 15 minutes for every bug found (no matter who finds them and where.) We Plan Bugs We go even further. At XDSD , we plan for a number of hidden bugs in every project. We do this by using our experience with previous projects and expert judgment. Let's say we're starting to develop a web system, which is similar to the one we worked on last year. We know that in the previous project our users and team together reported 500 bugs. It's logical to assume that the new project will have a similar number of bugs. Thus, our task is to make those 500 bugs visible before they hit the production platform and our users call us to complain about them. Therefore, we're making it one of the project goals: \"discover 500 bugs.\" Of course, our estimate may be wrong. Nevertheless, we have historical records for a few dozen projects, and in all of them the number is close to 500. So, finding 500 bugs in a project is usually a reality — we can use it as a target. What Is a Bug? Let us try to define a bug (or software defect) in a non-ambiguous manner. Something can be reported as a bug and subsequently paid for iff: it is reproducible it refers to functionality already implemented it can be fixed in a reasonable amount of time it doesn't duplicate a bug already reported Reproducibility of a bug is very important. Consequently, it is the responsibility of a bug reporter to make sure the bug is reproducible. Until it is proven that the bug can be reproduced — it's not a bug for which payment can be made. A bug is not a task; it has to refer to an existing functionality. Additionally, an explanation must exist for how and when the existing functionality doesn't work as expected. "},{"title":"No Obligations","url":"/2014/04/13/no-obligations-principle.html","tags":["xdsd","mgmt"],"date":"2014-04-13 00:00:00 +0000","categories":[],"body":" It is a very common problem in project management — how to make team members more responsible and avoid micro management ? We start with creating plans, drawing Gantt charts, announcing milestones, motivating everybody and promising big bonuses on success. Excuses Then everybody begins working and we start hearing excuses: \"The task is not yet ready. I was doing something else\" \"May I take a day off? Tomorrow is my birthday?\" \"May I skip the unit test because I don't know how to fix it?\" \"I don't know how to do it, can someone help me?\" \"I tried, but this doesn't work; what can I do?\" \"This installation requires all of my time. I can't finish the task\" With excuses, team members transfer responsibility back to the project manager. There was a very famous article \"Management Time: Who's Got the Monkey?\" published in the Harvard Business Review about this very subject. I recommend that you read it. Its authors present problems as monkeys sitting on our shoulders. When the project manager assigns a task to a programmer — he moves the monkey from his shoulders to the programmer's shoulders. The programmer usually presents the excuse \"I don't know what to do\". Now the monkey is back on the shoulders of the managers. The goal of the manager is to send the monkey back to make it the programmer's problem again. One of traditional way of transferring responsibility back to team members is to become an aggressive manager. For instance the manager may say, \"You have a birthday tomorrow? I don't care, you still have to meet your deadline\" or \"You don't know how to fix the unit test? Not my problem, it should be fixed by tomorrow,\" etc. We've all seen multiple examples of that type of aggressive management. Personally, I find this management style extremely annoying and destructive for the project. The project environment becomes very unhealthy and good people usually end up leaving. Another traditional management method is micro-management. This results when the project manager checks task statuses every few hours and tells people what to do and how to handle problems. Needless to say, this management style ruins the team and causes good people to leave even faster. However, in order to keep the project on track and meet all milestones, responsibility must be on the shoulders of the team members. They should be responsible for their own tasks and report back to the project manager when they are finished with their jobs. The Big Lebowski (1998) by Joel Coen Implementation problems should be solved by team members on their own. So, how do we accomplish this in XDSD ? I Owe You Nothing In XDSD , there is the first fundamental principle that says everybody should be paid for deliverables. Based on this idea, we can go even further and declare a \"No Obligations\" principle. In essence, for every team member, it says: if you don’t like the task assigned to you, don’t have time or you’re simply not in the mood — don't do it. You have no obligation to do anything. You're free to reject every second task that a project manager gives to you or even all of them. On the other hand, though, the project manager is not obliged to keep a task assigned to you for longer than 10 days (we think that this time frame is logical). If you get a task, and don't deliver within ten days, the project manager can take it away and pay you nothing — no matter how much time you invested in the task already or the reasons for your failure to complete it. Where Are The Monkeys Now? This principle helps us to separate responsibilities between project manager and team members. The manager is responsible for finding the right people and assigning them appropriate tasks. There is a problem with the project manager's management style if he receives too many rejections from the team. On the other hand, his team members are responsible for their tasks and should not provide excuses for non-completion. Well, team members can make excuses, but they won't change anything. No matter what their excuses are, the deliverables will be purchased only from members who manage to complete their tasks on time. How Does This Affect Me? When you're working with XDSD -inspired project, you should always keep the \"No Obligations\" principle in mind. You should start a task only if you're sure that you can finish it in a few days. You should pursue your tasks and control deadlines yourself. The project manager will not ask you for status updates, as usually happens with traditional projects. He will just take the task away from you after ten days if you don’t finish it. To avoid that, you should control your tasks and their deadlines. With every task, try to be as lazy as possible and cut every corner you can. The smaller the amount of work you perform on a task, the easier it will be to deliver it and pass all quality controls. Always remember that your efforts are not appreciated — only the deliverables matter. "},{"title":"Object-Oriented DynamoDB API","url":"/2014/04/14/jcabi-dynamo-java-api-of-aws-dynamodb.html","tags":["dynamodb","aws","java","jcabi"],"date":"2014-04-14 00:00:00 +0000","categories":[],"body":" I'm a big fan of cloud computing in general and of Amazon Web Services in particular. I honestly believe that in a few years big providers will host all, or almost all, computing and storage resources. When this is the case, we won't have to worry too much anymore about downtime, backups and system administrators. DynamoDB is one of the steps towards this future. This looks cool - jcabi-dynamo - a #Java Object layer atop the #DynamoDB SDK - http://t.co/khRFR2joKX #aws — Jeff Barr (@jeffbarr) September 19, 2013 DynamoDB is a NoSQL database accessible through RESTful JSON API. Its design is relatively simple. There are tables, which basically are collections of data structs, or in AWS terminology, \"items.\" Every item has a mandatory \"hash,\" an optional \"range\" and a number of other optional attributes. For instance, take the example table depts : +------+--------+---------------------------+ | dept | worker | Attributes | +------+--------+---------------------------+ | 205 | Jeff | job=\"manager\", sex=\"male\" | | 205 | Bob | age=43, city=\"Chicago\" | | 398 | Alice | age=27, job=\"architect\" | +------+--------+---------------------------+ For Java, Amazon provides an SDK , which mirrors all RESTful calls to Java methods. The SDK works fine, but is designed in a pure procedural style. Let's say we want to add a new item to the table above. RESTful call putItem looks like (in essence): putItem: tableName: depts item: dept: 435 worker: \"William\" job: \"programmer\" This is what the Amazon server needs to know in order to create a new item in the table. This is how you're supposed to make this call through the AWS Java SDK: 1 2 3 4 5 6 7 8 9 10 11 12 13 PutItemRequest request = new PutItemRequest (); request . setTableName ( \"depts\" ); Map < String , AttributeValue > attributes = new HashMap <>(); attributes . put ( \"dept\" , new AttributeValue ( 435 )); attributes . put ( \"worker\" , new AttributeValue ( \"William\" )); attributes . put ( \"job\" , new AttributeValue ( \" programmer )); request . setItem ( attributes ); AmazonDynamoDB aws = // instantiate it with credentials try { aws . putItem ( request ); } finally { aws . shutdown (); } The above script works fine, but there is one major drawback — it is not object oriented. It is a perfect example of an imperative procedural programming . To allow you to compare, let me show what I've done with jcabi-dynamo . Here is my code, which does exactly the same thing, but in an object-oriented way: 1 2 3 4 5 6 7 8 Region region = // instantiate it with credentials Table table = region . table ( \"depts\" ); Item item = table . put ( new Attributes () . with ( \"dept\" , 435 ) . with ( \"worker\" , \"William\" ) . with ( \"job\" , \"programmer\" ) ); My code is not only shorter, but it also employs encapsulation and separates responsibilities of classes. Table class (actually it is an interface internally implemented by a class) encapsulates information about the table, while Item encapsulates item details. We can pass an item as an argument to another method and all DynamoDB related implementation details will be hidden from it. For example, somewhere later in the code: void sayHello ( Item item ) { System . out . println ( \"Hello, \" + item . get ( \"worker\" )); } In this script, we don't know anything about DynamoDB or how to deal with its RESTful API. We interact solely with an instance of Item class. By the way, all public entities in jcabi-dynamo are Java interfaces. Thanks to that, you can test and mock the library completely. Let's consider a more complex example, which would take a page of code if we were to use a bare AWS SDK. Let's say that we want to remove all workers from our table who work as architects: Region region = // instantiate it with credentials Iterator < Item > workers = region . table ( \"depts\" ). frame () . where ( \"job\" , Condition . equalTo ( \"architect\" )); while ( workers . hasNext ()) { workers . remove (); } jcabi-dynamo has saved a lot of code lines in a few of my projects. You can see it in action at rultor-users . The library ships as a JAR dependency in Maven Central (get its latest versions from Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamo </artifactId> </dependency> "},{"title":"Definition Of Done","url":"/2014/04/15/definition-of-done.html","tags":["mgmt","xdsd"],"date":"2014-04-15 00:00:00 +0000","categories":[],"body":" Definition of Done (DoD) is a key definition used in Scrum and the one we also use in XDSD . DoD is an exit criteria of a simple atomic task and answers the question:\"am I done with this task?\" Moreover, DoD answers the question: \"will I be paid for the task?\" In XDSD , the definition of \"done\" is very simple — the task is done iff its author accepts the deliverables. At XDSD , our first and most important principle states that someone is paid only when he provides deliverables. Combining the definition of done and the principle of paying only for deliverables provides us a very important conclusion: we do not pay for un-finished tasks. Every task has its own time budget. Regardless of the number of people who worked on a task previously, only the last one — the one who managed to provide a working deliverable — receives payment. To better understand this principle, you should read: No Obligations Principle . Your goal as a developer working on a task should be to close it and receive payment as soon as possible. To that end, here are few things that can help you complete tasks and receive payments without too much frustration: Don't even start a task unless you're sure you can finish it; Ask any and all questions of the task author in advance (before beginning work); Don't assume anything — ask if you're not sure; Stay after the author to close tasks — be aggressive; Don't expect any help from anyone — you're on your own; Ask about payment if you don’t receive it automatically after an author closes your task(s) It is important to remember that, as a developer, it is your responsibility to ensure that tasks are closed and you receive payment. "},{"title":"Github Guidelines","url":"/2014/04/15/github-guidelines.html","tags":["github","xdsd"],"date":"2014-04-15 00:00:00 +0000","categories":[],"body":"This manual explains the workflow used when working with a XDSD project hosted on Github.com . You start when a Github issue is assigned to you. Next, you will receive a message from a project manager containing the issue number, title, description and its budget in hours (usually 30 minutes). If you don't agree with the budget allotment, don't hesitate to ask for an increase. As soon as you are comfortable with the budget and understand the scope of the work, say so in a reply to the ticket and start working. Be aware that you won't be paid for time spent above and beyond the allotted time budget. 1. Fork Even though you're part of the development team, you don't have write access to the repository in Github. Consequently, to contribute changes, you should fork the repository to your own Github account (create a private copy of it), make needed changes and then submit them for review using \"a pull request.\" After you submit a pull request review, the repository owner approves your changes by merging them into the main repository. This is how we protect the main development stream against accidental damage. This article explains how to fork a repository: fork-a-repo This one explains how to download and install Github on your computer: set-up-git Finally, don't forget to add your private SSH key to Github: generating-ssh-keys 2. Branch Once you have a forked our repository to your account, clone it to your computer, and then check out the master branch. For example: 1 2 git clone git@github.com:yegor256/jcabi.git git checkout master Now, it's time to branch ( 123 is the number of the Github issue you're going to work with, and the name of the branch): 1 git checkout -b 123 By convention, we use the same names for the branch and issue you're working with. 3. Changes All task-related questions should be discussed in the Github issue. For Github issues, we don't use emails, Skype, phone calls or meetings. All questions should be asked directly in the Github issues. Don't hesitate to submit new issues if something is not clear or you need help. It's a very common to receive a task that you may not be able to implement. Don't panic. This usually happens when you first just join a project and don't yet have enough information. If this happens, don't try to figure out a problem or issue by yourself. The rule of thumb for this type of situation is: \"if something is not clear, it is our fault, not yours.\" Therefore, if you don’t understand the project design, it is the fault of the project designer. Submit a bug report requesting an explanation of a design concept. You will be paid for this report, and the information you receive in the reply will be shared between all other developers. Read this article: Bugs Are Welcome . Don't expect anyone to help you. Your only source of help is the source code itself. If the code doesn't explain everything you need to know — it is a bug, which must be reported. 4. Commit and Push Make any needed changes using a text editor or IDE. It's a good practice to commit changes as soon as you make them. Don't accumulate large numbers of changes too long before committing them. 1 2 git commit -am '#123: the description of the changes' git push origin 123 If you have questions about the scope of work, post them in the Github issue and wait for an answer. If you think that the existing code needs improvements, don't hesitate to submit a new issue to Github. Don't try to fix all problems in one branch; let other programmers take care of them. 5. Pull Request Create a pull request in Github using the process in the following article: using-pull-requests Post its number in the original issue and wait for feedback. 6. Code Review After a while, your pull request will be reviewed by someone from the project team. In many cases, you may receive a few negative comments, and you will have to fix any and all issues associated with them. Your pull request won't be merged into master branch , until your changes satisfy the reviewer. Be patient with the reviewer, and listen to him carefully. However, don't think that your reviewer is always right. If you think that your changes are valid, insist that someone else review them. 7. Merge When everything looks good to the reviewer, he will inform our automated merge bot. The automated merge bot will then select your pull request and try to merge it into master branch. For various reasons, this operation fails often. If the merge fails, regardless of the reason, it is your responsibility to make sure that your branch is merged successfully. If you can't merge a branch because of failures in tests not associated with your task, don't try to fix them yourself. Instead, report a problem as a new bug and wait for its resolution. Remember, until your branch is merged, you are not paid. 8. Payment Once your changes are merged, return to the Github issue and ask the author to close it. Once the issue is closed by a project manager, you will receive your payment within a few hours, through oDesk or PayPal. "},{"title":"How XDSD Is Different","url":"/2014/04/17/how-xdsd-is-different.html","tags":["xdsd","mgmt"],"date":"2014-04-17 00:00:00 +0000","categories":[],"body":" eXtremely Distributed Software Development, or XDSD for short, is a methodology that differs significantly from working in traditional software development teams. Most XDSD methods are so different (yet critical) that many newcomers get confused. This article should help you bootstrap once you join a project managed with by XDSD principles — either as a developer or a project sponsor. We Pay Only For Closed Tasks Unlike with many other projects, in XDSD , we pay only for closed tasks and the agreed upon time budget. Let me explain by example. Let's say, you are a Ruby programmer and you a get a new task that requires you to fix a broken unit test. The task has a time budget of 30 minutes, as is the case most of the time. Sometimes, though, tasks may have time budgets of fifteen minutes or one hour. In our example, we agree upon a contract rate of $50 per hour. With the broken test, you will receive $25 for completing the task — 30 minute tasked billed at $50 per hour. It does not matter how long it actually takes you to fix the test. Your actual time spent on the project may be five minutes or five hours. Nevertheless, you will receive compensation for 30 minutes of work only. If you fix the broken test in 5 minutes, you receive $25. If the task takes you an hour, or even a month, to complete, you still receive only $25. Furthermore, if you fail to fix the unit test and close the task altogether, you will receive no pay at all for the assignment. You can view more details about this principle in the following articles: No Obligations Principle or Definition of Done . Revolver (2005) by Guy Ritchie As mentioned above, this is one of the most important differences between XDSD and other methods. Many people get confused when they see this principle in action, and some leave our projects because of it. They simply are used to being paid by the end of the month — no matter how much work they actually deliver. In XDSD, we consider this type of approach very unfair. We feel that people who deliver more results should receive more cash. Conversely, those who don't deliver should get less. We Deliver Unfinished Components Since most of our tasks are half an hour in size, we encourage developers to deliver unfinished components. Read more about this concept in the article below: Puzzle Driven Development . No Informal Communications Unlike many other projects or teams you may have worked with, XDSD uses no informal communication channels . To clarify, we never use emails, we never chat on Skype and we don't do any meetings or phone calls. Additionally, XDSD maintains no type mailing list. Our only method of communication is a ticket tracking system (which in most projects consists of Github Issues .) Moreover, we discourage horizontal communications between developers regarding the scope of individual tasks. When assigned a task, your single and only point of contact (and your only customer) is the task author. You communicate with the author in the ticket to clarify task requirements. When the requirements of a task are clear — and you understand them fully — deliver the result to the author and wait for him to close the task. After the author closes the task, the project manager pays you. Goodfellas (1990) by Martin Scorsese We're very strict about this principle — no informal communications. However, it doesn't mean that we are not interested in your opinions and constructive criticism. Rather, we encourage everyone to submit their suggestions and bugs. By the way, we pay for bugs (see the next section for further details about bug reporting and payments.) Since we have no formal communications, members of project teams are not required to work at specific times. Instead, team members work at times convenient for them in their time zones. This includes weekdays and weekends. We Pay For Bugs Unlike many other software teams, XDSD welcomes bug reports in all our projects. Therefore, we ask for bugs openly and expect team members to report them. Review the following article for complete details on XDSD bug reporting: Bugs are welcome We expect everyone involved with a project to report every bug found. Additionally, we encourage team members to make suggestions. In XDSD, we pay team members for every properly reported bug. XDSD makes payments for reported bugs because we believe that the more of them we can find, the higher the quality of the end product. Some new developers are surprised when they receive tasks such as \"you must find 10 bugs in class A.\" Often, the natural reaction is to ask \"what if there are no bugs?\" However, we believe that any software product may have an unlimited amount of bugs; it is just a matter of expending the time and effort needed to discover them. Only Pull Request We never grant team member access to the master branch — no matter how long you work on a project. Consequently, you must always submit your changes through pull requests (most of our projects are done in Github .) We enforce this policy not because we don't trust our developers, but simply because we don't trust anyone :) Read this article: Master Branch Must Be Read-Only . No Compromises About Code Quality Before merge any changes to the master branch, we check the entire code base with unit tests and static analyzers . Unit testing is a very common component in modern software development, and one by which you should not be surprised. However, the strictness of static analysis is something that often frustrates XDSD newcomers, and we understand that. We pay much more attention to the quality and uniformity of our source code than most of our competing software development teams. Even more important is that we never make compromises. If your pull request violates even one rule of the static analyzer, it won't be accepted. And, it doesn't matter how small or innocent that violation may look. This merging process is fully automated and can't be bypassed. "},{"title":"Mocking of HTTP Server in Java","url":"/2014/04/18/jcabi-http-server-mocking.html","tags":["jcabi","http","mocking"],"date":"2014-04-18 00:00:00 +0000","categories":[],"body":" Recently, I explained a fluent Java HTTP client created (mostly) to make HTTP interactions more object-oriented than with other available clients,including: Apache Client , Jersey Client and plain old HttpURLConnection . This client ships in the jcabi-http Maven artifact. However, the client part is not the only benefit of using jcabi-http . Jcabi also includes a server component that can help you in unit and integration testing of your HTTP clients. Let me show you an example first. In the example, I'm using hamcrest for assertions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 MkContainer container = new MkGrizzlyContainer () . next ( new MkAnswer . Simple ( \"hello, world!\" )) . start (); try { new JdkRequest ( container . home ()) . header ( \"User-agent\" , \"Myself\" ) . fetch () . assertBody ( Matchers . containsString ( \"hello\" )); } finally { container . stop (); } MkQuery query = container . take (); MatcherAssert . assertThat ( query . headers (). get ( \"User-agent\" ), Matchers . hasItem ( \"Myself\" ) ); Now, let's discover what happens here. In the first few lines, I create an instance of MkContainer , which literally has four methods: next(MkAnswer) , start() , stop() , and home() . It works as an HTTP server with a \"first-in-first-out\" queue for HTTP answers. We add answers, and the server returns them in response to HTTP requests. The server starts on start() call and stops on stop() . Its method home() returns a URL of its \"home page\". The server then binds itself to a randomly allocated TCP port. The container finds the first available and unoccupied port. In the example above, I added just one answer. This means that the container will reply only to the first HTTP request with that answer and that all consecutive requests will cause HTTP responses with status \"internal server error 500 .\" In lines 5 through 8, I make an HTTP request to the already started server. Also, I make an assertion that the body of the HTTP response contains the text \"hello\" . Obviously, this assertion will pass because the server will return \"hello, world!\" to my first request: new JdkRequest ( container . home ()) . header ( \"User-agent\" , \"Myself\" ) . fetch () . assertBody ( Matchers . containsString ( \"hello\" )); As you can see, I use container.home() in order to get the URL of the server. It is recommended that you allow the container to find the first unoccupied TCP port and bind itself to it. Nevertheless, if you need to specify your own port, you can do it with a one-argument method start(int) in MkContainer . I use try/finally to stop the container safely. In unit tests, this is not critical, as you can simplify your code and never stop the container. Besides, the container will be killed together with the JVM. However, for the sake of clarity, I would recommend you stop the container in the finally block. On line 12, I ask the stopped container to give me the first request it received. This mechanism is similar conceptually to the \"verify\" technology of mocking frameworks. For example, Mockito . MkQuery query = container . take (); MatcherAssert . assertThat ( query . headers (). get ( \"User-agent\" ), Matchers . hasItem ( \"Myself\" ) ); An instance of MkQuery exposes information about the query made. In this example, I get all headers of the HTTP request and making an assertion that the \"User-Agent\" header was there and had at least one value equal to \"Myself\" . This mocking technology is used actively in unit and integration tests of jcabi-github , which is a Java client to Github API. In its development, the technology is very important in checking which requests are being sent to the server and validating whether they comply with our requirements. Here, we are using jcabi-http mocking. As with the client, you need the jcabi-http.jar dependency (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-http </artifactId> </dependency> Besides the above, you need to add one more dependency, which is a Grizzly HTTP server. MkGrizzlyContainer is based on it. <dependency> <groupId> com.sun.grizzly </groupId> <artifactId> grizzly-servlet-webserver </artifactId> <scope> test </scope> </dependency> If you have any questions or suggestions, please submit them through Github issues . As always, bugs are welcome :) "},{"title":"How Hourly Rate Is Calculated","url":"/2014/04/20/how-hourly-rate-is-calculated.html","tags":["xdsd"],"date":"2014-04-20 00:00:00 +0000","categories":[],"body":" In XDSD , everyone — including project managers, analysts, programmers, and product owners — receives payments based on deliverables with agreed upon budgets. In the fhe first section of the article, How XDSD Is Different I explain exactly how this concept works. I don't explain in the article, though, how we decide which hourly rate is acceptable for each project participant. When new people come to us, usually they have some numbers in mind. They know how much they expect to make per week, per month or per day. We rarely negotiate the payment rates, but rather just accept reasonable offers (see How Much Do You Cost? ). Nonetheless, every few months, we review payments rates and change them accordingly (increasing or decreasing them as appropriate). Further along in the article, is a list of factors that influence our decision making process regarding payment rates. However, before we get to the factors that influence our rate-setting decisions, it is important to mention that — unlike most other companies or software teams — we don't pay attention to the following: Your geographic location; Skills and experience listed in your CV; Amount of time already spent on our projects; Age, sex, nationality, religious beliefs, etc. The factors listed below, though, are indeed very important to us. They affect your \"overall score\" significantly and play a major part in decisions to decrease or increase a payment rate. After changing a payment rate, we don't negotiate it with the project member. Keep in mind that besides decreasing your hourly rate, a low overall score may affect the number of tasks you receive from us. The best developers receive most of the new tasks. So, continue reading, follow our principles and learn how to earn and enjoy higher rates :) Fast Delivery The faster you deliver on a task, the better. We track all your completed tasks and can calculate easily how many days it takes you, on average, to close tasks. To increase this metric, you should try to close all tasks as soon as possible to reduce your overall completion-time average. If you see that a specific task is not suitable for you, don't hold on to it. Instead, inform your project manager as soon as possible that you do not want to work on the task. After you inform the project manager, he will try find you something else more suitable. By the way, the best developers usually close their tasks in five calendar days (or less) on average. Past Due Tasks Though we encourage everyone to reject tasks they don't like, we are strongly against overdue tasks. Once you have started to work on a task, we expect you to finish it on time. Our No Obligation Principle gives our project managers freedom to take any task away from you if don’t complete it in a reasonable amount of time (ten days). Removal of tasks by project managers affects your overall score negatively. Nevertheless, even the best developers sometimes have overdue tasks, and we understand that it happens from time to time. However, our best developers they keep their number of overdue tasks to a minimum. A good rule of thumb for acceptable numbers in this area is about one overdue task per twenty completed successfully and on time. Complexity Every XDSD task has a project role assigned to it. The article, Puzzle Driven Development by Roles , lists the key roles we use in XDSD projects. Generally speaking, the higher the role, the higher the complexity of tasks assigned to it. Therefore, closing a task in an \"architect\" role is much more important than closing one as an \"implementer\" (or \"developer.\") The more tasks you close in your current role, the faster you will receive promotions and receive pay-rate increases. Very often, our developers work in a few roles at the same time. Lengthy Discussions We discourage long conversations on one task. The longer the discussions about a task, the longer it takes to complete — which lowers your quality as a developer. Ideally, developers should receive a task, deliver the result and inform the task author after it's done. Afterwards, the task author closes the task and payment is made. We track the number of messages you post and receive in your tasks automatically. Consequently, too many messages may affect your overall score in a negative way. To avoid long conversations in tasks, submit new tickets with questions or bug reports. Again, the Puzzle Driven Development by Roles article explains the whole idea of helping us \"to break the project\" by submitting new bugs. Follow this concept and you'll be fine. Contribution via Bugs In XDSD Bugs Are Welcome . You are supposed to report bugs along the normal development activities. Besides receiving extra money for reporting bugs, you can also increase your overall rating. The best developers submit one bug for every 2 to 3 tasks they complete. "},{"title":"Basic HTTP Auth for S3 Buckets","url":"/2014/04/21/s3-http-basic-auth.html","tags":["aws","s3","http","s3auth"],"date":"2014-04-21 00:00:00 +0000","categories":[],"body":" Amazon S3 is a simple and very useful storage of binary objects (aka \"files\"). To use it, you create a \"bucket\" there with a unique name and upload your objects. Afterwards, AWS guarantees your object will be available for download through their RESTful API . A few years ago, AWS introduced a S3 feature called static website hosting . With static website hosting, you simply turn on the feature and all objects in your bucket become available through public HTTP. This is an awesome feature for hosting static content, such as images, JavaScript files, video and audio content. When using the hosting, you need to change the CNAME record in your DNS so that it points to www.example.com.aws.amazon.com . After changing the DNS entry, your static website is available at www.example.com just as it would be normally. When using Amazon S3, though, it is not possible to protect your website because the content is purely static. This means you can't have a login page on the front end. With the service, you can either make your objects either absolutely public — so that anyone can see them online — or assign access rights to them — but only for users connected through RESTful API. My use case with the service was a bit more complex, though. I wanted to host my static content as S3 objects. However, I wanted to do this while ensuring only a few people had access to the content using their Web browsers. HTTP Basic Authentication The HTTP protocol offers a nice \"basic access authentication\" feature that doesn't require any extra site pages. When an HTTP request arrives at the server, it doesn't deliver the content but replies with a 401 status response. This response means literally \"I don't know who you are, please authenticate yourself.\" The browser shows its native login screen and prompts for a user name and password. After entering the login credentials, they are concatenated, Base64 encoded, and added to the next request in Authorization HTTP header. Now, the browser tries to make another attempt to fetch the same webpage. But, this time, the HTTP request contains a header: Authorization: Basic am9lOnNlY3JldA== The above is just an example. In the example, the Base64 encoded part means joe:secret , where joe is the user name and secret the password entered by the user. This time the server has authentication information and can make a decision whether this user is authenticated (his password matches the server's records) and authorized (he has permission to access the request webpage). s3auth.com Since Amazon doesn't provide this feature, I decided to create a simple web service, s3auth.com , which stays in front of my Amazon S3 buckets and implements the HTTP-native authentication and authorization mechanism. Instead of making my objects public, though, I make them private and point my CNAME record to relay.s3auth.com . HTTP requests from Web browsers then arrive at my server, connect to Amazon S3, retrieve my objects and deliver them back in HTTP responses. The server implements authentication and authorization using a special file .htpasswd in the root of my bucket. The format of the \".htpasswd\" file is identical to the one used by Apache HTTP Server — one user per line. Every line has the name of a user and a hash version of his password. Implementation I made this software open source mostly to guarantee to my users that the server doesn't store their private data anywhere, but rather acts only as a pass-through service. As a result, the software is on Github . For the sake of privacy and convenience, I use only OAuth2 for user accounts. This means that I don't know who my users are. I don't possess their names or emails, but only their account numbers in Facebook, Google Plus or Github. Of course, I can find their names using these numbers, but this information is public anyway. The server is implemented in Java6. For its hosting, I'm using a single Amazon EC2 m1.small Ubuntu server. These days, the server seems to work properly and is stable. Extra Features Besides authentication and authorization, the s3auth.com server can render lists of pages — just like Apache HTTP Server. If you have a collection of objects in your bucket — but the index.html file is missing — Amazon S3 delivers a \"page not found\" result. Conversely, my server displays a list of objects in the bucket, when no \"index.html\" is present, and makes it possible to navigate up or down one folder. When your bucket has the versioning feature turned on, you are able to list all versions of any object in the browser. To do this, just add ?all-versions to the end of the URL to display the list. Next, click a version to have s3auth.com retrieve and render it. Traction I created this service mostly for myself, but apparently I'm not the only with the problems described above. At the moment, s3auth.com hosts over 300 domains and sends through more than 10Mb of data each hour. "},{"title":"Java XML Parsing Made Easy","url":"/2014/04/24/java-xml-parsing-and-traversing.html","tags":["xml","java","jcabi"],"date":"2014-04-24 00:00:00 +0000","categories":[],"body":"Unlike with many other modern languages, parsing XML in Java requires more than one line of code. XML traversing using XPath takes even more code, and I find this is unfair and annoying. I'm a big fan of XML and use it it in almost every Java application. Some time ago, I decided to put all of that XML-to-DOM parsing code into a small library — jcabi-xml . Put simply, the library is a convenient wrapper for JDK-native DOM manipulations. That's why it is small and dependency-free. With the following example, you can see just how simple XML parsing can be: import com.jcabi.xml.XML ; import com.jcabi.xml.XMLDocument ; XML xml = new XMLDocument ( \"<root><a>hello</a><b>world!</b></root>\" ); Now, we have an object of interface XML that can traverse the XML tree and convert it back to text. For example: // outputs \"hello\" System . out . println ( xml . xpath ( \"/root/a/text()\" ). get ( 0 )); // outputs the entire XML document System . out . println ( xml . toString ()); Method xpath() allows you to find a collection of text nodes or attributes in the document, and then convert them to a collection of strings, using XPath query : // outputs \"hello\" and \"world\" for ( String text : xml . xpath ( \"/root/*/text()\" )) { System . out . println ( text ); } Method nodes() enables the same XPath search operation, but instead returns a collection of instances of XML interface: // outputs \"<a>hello</a>\" and \"<b>world</b>\" for ( XML node : xml . nodes ( \"/root/*\" )) System . out . println ( node ); } Besides XML parsing, printing and XPath traversing, jcabi-xml also provides XSD validation and XSL transformations. I'll write about those features in the next post :) "},{"title":"Incremental Requirements With Requs","url":"/2014/04/26/incremental-requirements-with-requs.html","tags":["requs","xdsd","requirements"],"date":"2014-04-26 00:00:00 +0000","categories":[],"body":"Requirements engineering is one of the most important disciplines in software development. Perhaps, even more important than architecture, design or coding itself. Joy Beatty and Karl Wiegers in Software Requirements argue that the cost of mistakes made in a requirements specification is significantly higher than a bug in source code. I totally agree. In XDSD projects we specify requirements using Requs , a controlled natural language that sounds like English, while at the same time is parseable by computers. A simple requirements document in Requs may look similar to: 1 2 3 Department has employee-s. Employee has name and salary. UC1 where Employee gets raise: \"TBD\". This Software Requirements Specification (SRS) defines two types ( Department and Employee ) and one method UC (aka \"use case\"). Requs syntax is explained here . The main and only goal of requirements engineering in any XDSD project is to create a complete and non-ambiguous SRS document. The person who performs this task is called the \"system analyst\". This article explains his or her main tasks and discusses possible pitfalls. Tasks We modify SRS incrementally, and our increments are very small. For instance, say we have the sample document I mentioned above, and I'm a system analyst on the project. All my tasks will be similar to \"there is a bug in SRS, let's fix it\". Even if it is a suggestion, it will still start with a complaint about the incompleteness of the SRS. For example: UC1 doesn't explain how exactly an employee receives a raise. Does the salary of an employee have limits? Can it be negative? How many employees can a department have? Can it be zero? Can an employee receive a decrease in salary? All of these bugs are addressed to me. I need to fix them by improving the SRS. My workflow is the same in every task: Understand what is required Change the SRS Close the task Let's try this step by step. Requirements Providers As a system analyst, my job is to understand what product owners (aka \"requirements providers\") want and document their wishes. In most cases, their wants and wishes are very vague and chaotic. My job is to make them complete and unambiguous. That's why the first step is to understand what is required. First of all, I must determine who the product owner is before I can begin. The product owner signs the SRS, so I should pay complete attention to his opinions. However, my job is not only to listen, but also to suggest. A good system analyst can provoke creative thinking in a product owner by asking the right questions. OK, now I that know the identity of the product owner, I need to talk to him. In XDSD, we don't do any meetings, phone calls, or any other type of informal communications. Therefore, my only mechanism for receiving the information I need is with is — tickets. I will submit new tickets, addressing them to the product owner. As there can be many product owners in a project, I must submit tickets that clearly state in the first sentence that the ticket pertains to questions for a particular owners. The person receiving the ticket will then determine the best person to answer it. Thus, while working with a single task, I will submit many questions and receive many interesting answers. I'll do all this in order to improve my understanding of the product the owners are developing. When I understand how the SRS should be fixed, it is time to make changes in the Requs files. Requs Files The SRS document is generated automatically on every continuous integration build cycle. It is compiled from pieces called .req files, which are usually located in the src/main/requs directory in a project repository. My job, as a system analyst, is to make changes to some of these files and submit a pull request for review. Github Guidelines explains [how to work with Github. However, in short, I need to: Clone the repository; Check out its copy to my computer; Make changes; Commit my changes; Push them to my remote fork; Submit a pull request It doesn't really matter which files I edit because Requs automatically composes together all files with the req extension. I can even add new files to the directory — they will be picked up. Likewise, I can also add sub- directories with files. Local Build Before submitting a pull request, I will try to validate that my changes are syntactically and grammatically valid. I will compile Requs files into the SRS document using the same method our continuous integration server uses to compile them. Before I can compile, though, I need to install JDK7 and Maven . Afterwards, I make the following command line call in the project directory: 1 mvn clean requs:compile After entering the commands, I expect to see the BUILD SUCCESS message. If not, there are some errors and I should fix them. My pull request won't be merged and I won't be able to close the task if Requs can't compile the files. Once compiled, I can open the SRS in Firefox. It is in target/requs/index.xml . Even though it is an XML file, Firefox can open it as a webpage. Other browsers won't work. Well, Google Chrome will work, but only with this small trick . Pull Request Review Once all changes are finished, I will submit a pull request. A project manager will the assign someone to review my pull request and I will receive feedback. In most cases, there will be at least a few corrections requested by the reviewer. Generally speaking, my requests are reviewed by other system analysts. Therefore, I must address all comments and make sure my changes satisfy the reviewer. I will make extra changes to the same branch locally, and push them to Github. The pull request will be updated automatically, so I don't need to create a new one. Once the pull request is clean enough for the reviewer, he will merge it into the master branch. Close and Get Paid Finally, my pull request is merged and I get back to the task owner. I tell him that the SRS was fixed and request that he review it. His original problem should be fixed by now — the SRS should provide the information required. He then closes the task and the project manager pays me within a few hours. "},{"title":"Typical Mistakes in Java Code","url":"/2014/04/27/typical-mistakes-in-java-code.html","tags":["anti-pattern","java","oop"],"date":"2014-04-27 00:00:00 +0000","categories":["jcg"],"body":"This page contains most typical mistakes I see in the Java code of people working with me. Static analysis (we're using qulice can't catch all of the mistakes for obvious reasons, and that's why I decided to list them all here. Let me know if you want to see something else added here, and I'll be happy to oblige. All of the listed mistakes are related to object-oriented programming in general and to Java in particular. Class Names Your class should be an abstraction of a real life entity with no \"validators\", \"controllers\", \"managers\", etc. If your class name ends with an \"-er\" — it's a bad design . BTW, here are my seven virtues of a good object. Also, this post explains this idea in more details: Don't Create Objects That End With -ER . And, of course, utility classes are anti-patterns, like StringUtils , FileUtils , and IOUtils from Apache. The above are perfect examples of terrible designs. Read this follow up post: OOP Alternative to Utility Classes Of course, never add suffixes or prefixes to distinguish between interfaces and classes . For example, all of these names are terribly wrong: IRecord , IfaceEmployee , or RecordInterface . Usually, interface name is the name of a real-life entity, while class name should explain its implementation details. If there is nothing specific to say about an implementation, name it Default, Simple , or something similar. For example: class SimpleUser implements User {}; class DefaultRecord implements Record {}; class Suffixed implements Name {}; class Validated implements Content {}; Method Names Methods can either return something or return void . If a method returns something, then its name should explain what it returns , for example (don't use the get prefix ever ): boolean isValid ( String name ); String content (); int ageOf ( File file ); If it returns void, then its name should explain what it does . For example: void save ( File file ); void process ( Work work ); void append ( File file , String line ); There is only one exception to the rule just mentioned — test methods for JUnit. They are explained below. Test Method Names Method names in JUnit tests should be created as English sentences without spaces. It's easier to explain by example: /** * HttpRequest can return its content in Unicode. * @throws Exception If test fails */ public void returnsItsContentInUnicode () throws Exception { } It's important to start the first sentence of your JavaDoc with the name of the class you're testing followed by can . So, your first sentence should always be similar to \"somebody can do something\". The method name will state exactly the same, but without the subject. If I add a subject at the beginning of the method name, I should get a complete English sentence, as in above example: \"HttpRequest returns its content in unicode\". Pay attention that the test method doesn't start with can .Only JavaDoc comments start with 'can.' Additionally, method names shouldn’t start with a verb. It's a good practice to always declare test methods as throwing Exception . Variable Names Avoid composite names of variables, like timeOfDay , firstItem , or httpRequest . I mean with both — class variables and in-method ones. A variable name should be long enough to avoid ambiguity in its scope of visibility, but not too long if possible. A name should be a noun in singular or plural form, or an appropriate abbreviation. More about it in this post: A Compound Name Is a Code Smell . For example: List < String > names ; void sendThroughProxy ( File file , Protocol proto ); private File content ; public HttpRequest request ; Sometimes, you may have collisions between constructor parameters and in-class properties if the constructor saves incoming data in an instantiated object. In this case, I recommend to create abbreviations by removing vowels (see how USPS abbreviates street names ). Another example: public class Message { private String recipient ; public Message ( String rcpt ) { this . recipient = rcpt ; } } In many cases, the best hint for a name of a variable can ascertained by reading its class name. Just write it with a small letter, and you should be good: File file ; User user ; Branch branch ; However, never do the same for primitive types, like Integer number or String string . You can also use an adjective, when there are multiple variables with different characteristics. For instance: String contact(String left, String right); Constructors Without exceptions, there should be only one constructor that stores data in object variables. All other constructors should call this one with different arguments. For example: public class Server { private String address ; public Server ( String uri ) { this . address = uri ; } public Server ( URI uri ) { this ( uri . toString ()); } } One-time Variables Avoid one-time variables at all costs. By \"one-time\" I mean variables that are used only once. Like in this example: String name = \"data.txt\" ; return new File ( name ); This above variable is used only once and the code should be refactored to: return new File ( \"data.txt\" ); Sometimes, in very rare cases — mostly because of better formatting — one-time variables may be used. Nevertheless, try to avoid such situations at all costs. Exceptions Needless to say, you should never swallow exceptions, but rather let them bubble up as high as possible. Private methods should always let checked exceptions go out. Never use exceptions for flow control. For example this code is wrong: int size ; try { size = this . fileSize (); } catch ( IOException ex ) { size = 0 ; } Seriously, what if that IOException says \"disk is full\"? Will you still assume that the size of the file is zero and move on? Indentation For indentation, the main rule is that a bracket should either end a line or be closed on the same line (reverse rule applies to a closing bracket). For example, the following is not correct because the first bracket is not closed on the same line and there are symbols after it. The second bracket is also in trouble because there are symbols in front of it and it is not opened on the same line: final File file = new File(directory, \"file.txt\"); Correct indentation should look like: StringUtils.join( Arrays.asList( \"first line\", \"second line\", StringUtils.join( Arrays.asList(\"a\", \"b\") ) ), \"separator\" ); The second important rule of indentation says that you should put as much as possible on one line - within the limit of 80 characters. The example above is not valid since it can be compacted: StringUtils.join( Arrays.asList( \"first line\", \"second line\", StringUtils.join(Arrays.asList(\"a\", \"b\")) ), \"separator\" ); Redundant Constants Class constants should be used when you want to share information between class methods, and this information is a characteristic (!) of your class. Don't use constants as a replacement of string or numeric literals — very bad practice that leads to code pollution. Constants (as with any object in OOP) should have a meaning in a real world. What meaning do these constants have in the real world: class Document { private static final String D_LETTER = \"D\" ; // bad practice private static final String EXTENSION = \".doc\" ; // good practice } Another typical mistake is to use constants in unit tests to avoid duplicate string/numeric literals in test methods. Don't do this! Every test method should work with its own set of input values. Use new texts and numbers in every new test method. They are independent. So, why do they have to share the same input constants? Test Data Coupling This is an example of data coupling in a test method: User user = new User ( \"Jeff\" ); // maybe some other code here MatcherAssert . assertThat ( user . name (), Matchers . equalTo ( \"Jeff\" )); On the last line, we couple \"Jeff\" with the same string literal from the first line. If, a few months later, someone wants to change the value on the third line, he/she has to spend extra time finding where else \"Jeff\" is used in the same method. To avoid this data coupling, you should introduce a variable. "},{"title":"XML/XPath Matchers for Hamcrest","url":"/2014/04/28/xml-xpath-hamcrest-matchers.html","tags":["xml","hamcrest","xpath","testing"],"date":"2014-04-28 00:00:00 +0000","categories":[],"body":" Hamcrest is my favorite instrument in unit testing. It replaces the JUnit procedural assertions of org.junit.Assert with an object-oriented mechanism. However, I will discuss that subject in more detail sometime later. Now, though, I want to demonstrate a new library published today on Github and Maven Central: jcabi-matchers . jcabi-matchers is a collection of Hamcrest matchers to make XPath assertions in XML and XHTML documents. Let's say, for instance, a class that is undergoing testing produces an XML that needs to contain a single <message> element with the content \"hello, world!\" This is how that code would look in a unit test: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import com.jcabi.matchers.XhtmlMatchers ; import org.hamcrest.MatcherAssert ; import org.junit.Test ; public class FooTest { @Test public void hasWelcomeMessage () { MatcherAssert . assertThat ( new Foo (). createXml (), XhtmlMatchers . hasXPaths ( \"/document[count(message)=1]\" , \"/document/message[.='hello, world!']\" ) ); } } There are two alternatives to the above that I'm aware of, which are do almost the same thing: xml-matchers by David Ehringer and hasXPath() method in Hamcrest itself. I have tried them both, but faced a number of problems. First, Hamcrest hasXPath() works only with an instance of Node . With this method, converting a String into Node becomes a repetitive and routine task in every unit test. The above is a very strange limitation of Hamcrest in contrast to jcabi-matchers , which works with almost anything, from a String to a Reader and even an InputStream . Second, `XmlMatchers from xml-matchers provides a very inconvenient way for working with namespaces. Before you can use an XPath query with a non-default namespace, you should create an instance of NamespaceContext. The library provides a simple implementation of this interface, but, still, it is requires extra code in every unit test. jcabi-matchers simplifies namespace handling problems even further, as it pre-defines most popular namespaces, including xtml , xs , xsl , etc. The following example works right out-of-the-box — without any extra configuration: 1 2 3 4 MatcherAssert . assertThat ( new URL ( \"http://www.google.com\" ). getContent (), XhtmlMatchers . hasXPath ( \"//xhtml:body\" ) ); To summarize, my primary objective with the library was its simplicity of usage. "},{"title":"W3C Java Validators","url":"/2014/04/29/w3c-java-validators.html","tags":["w3c","java","jcabi"],"date":"2014-04-29 00:00:00 +0000","categories":[],"body":" A few years ago, I created two Java wrappers for W3C validators: ( HTML and CSS ). Both wrappers seemed to be working fine and were even listed by W3C on their website in the API section. Until recently, these wrappers have always been part of ReXSL library. A few days ago, though, I took the wrappers out of ReXSL and published them as a standalone library — jcabi-w3c . Consequently, now seems to be a good time to write a few words about them. Below is an example that demonstrates how you can validate an HTML document against W3C compliancy rules: 1 2 3 4 import com.jcabi.w3c.ValidatorBuilder ; assert ValidatorBuilder . html () . validate ( \"<html>hello, world!</html>\" ) . valid (); The valid() method is a black or white indicator that returns false when the document is not valid. Additionally, you can obtain more information through a list of \"defects\" returned by the W3C server: 1 2 3 Collection < Defect > defects = ValidatorBuilder . html () . validate ( \"<html>hello, world!</html>\" ) . errors (); The same can be done with CSS: 1 2 3 Collection < Defect > defects = ValidatorBuilder . css () . validate ( \"body { font-family: Arial; }\" ) . errors (); Personally, I think it is a good practice to validate all of HTML pages produced by your application against W3C during integration testing. It's not a matter of seeking perfection, but rather of preventing bigger problems later. These dependencies are mandatory when using jcabi-w3c (get their latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-w3c </artifactId> </dependency> <dependency> <groupId> org.glassfish </groupId> <artifactId> javax.json </artifactId> </dependency> <dependency> <groupId> com.sun.jersey </groupId> <artifactId> jersey-client </artifactId> </dependency> <dependency> <groupId> org.hamcrest </groupId> <artifactId> hamcrest-core </artifactId> </dependency> "},{"title":"DynamoDB Local Maven Plugin","url":"/2014/05/01/dynamodb-local-maven-plugin.html","tags":["dynamodb","maven","aws","java"],"date":"2014-05-01 00:00:00 +0000","categories":[],"body":" DynamoDB Local is a locally running copy of Amazon DynamoDB server. Amazon developed the tool and based it on SQLite. It acts as a real DynamoDB service through the RESTful API. I guess, DynamoDB Local is meant to be used in integration testing and this is how we're going to use it below. I use Maven to run all of my Java integration testing using maven-failsafe-plugin . The philosophy of integration testing with Maven is that you start all your supplementary test stubs during the pre-integration-test phase, run your tests in the integration-test phase and then shutdown all stubs during the post-integration-test . It would be great if it were possible to use DynamoDB Local that way. I didn't find any Maven plugins for that purpose, so I decided to create my own — jcabi-dynamodb-maven-plugin . Full usage details for the plugin are explained on its website . However, here is a simple example (get its latest versions in Maven Central ): <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamodb-maven-plugin </artifactId> <executions> <execution> <goals> <goal> start </goal> <goal> stop </goal> </goals> <configuration> <port> 10500 </port> <dist> ${project.build.directory}/dynamodb-dist </dist> </configuration> </execution> </executions> </plugin> The above configuration will start DynamoDB Local right before running integration tests, and then stop it immediately afterwards. The server will listen at TCP port 10500. While the number is used in the example, you're supposed to use a randomly allocated port instead. When the DynamoDB Local server is up and running, we can create an integration test for it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import com.amazonaws.auth.BasicAWSCredentials ; import com.amazonaws.services.dynamodbv2.AmazonDynamoDB ; import com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient ; import com.amazonaws.services.dynamodbv2.model.ListTablesResult ; public class FooITCase { @Test public void worksWithAwsDynamoDb () { AmazonDynamoDB aws = new AmazonDynamoDBClient ( new BasicAWSCredentials ( \"\" , \"\" ) ); aws . setEndpoint ( \"http://localhost:10500\" ); ListTablesResult list = aws . listTables (); for ( String name : list . getTableNames ()) { System . out . println ( \"table found: \" + name ); } } } Of course, there won't be any output because the server starts without any tables. Since the server is empty, you should create tables before every integration test, using createTable() from DynamoDB SDK . To avoid this type of extra hassle, in the latest version 0.6 of jcabi-dynamodb-maven-plugin we introduced a new goal create-tables : <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamodb-maven-plugin </artifactId> <executions> <execution> <goals> <goal> create-tables </goal> </goals> <configuration> <tables> <table> ${basedir}/src/test/dynamodb/foo.json </table> </tables> </configuration> </execution> </executions> </plugin> The foo.json file used above should contain a JSON request that is sent to DynamoDB Local right after it is up and running. The request should comply with the specification of CreateTable request. For example: { \"AttributeDefinitions\" : [ { \"AttributeName\" : \"id\" , \"AttributeType\" : \"N\" } ], \"KeySchema\" : [ { \"AttributeName\" : \"id\" , \"KeyType\" : \"HASH\" } ], \"ProvisionedThroughput\" : { \"ReadCapacityUnits\" : \"1\" , \"WriteCapacityUnits\" : \"1\" }, \"TableName\" : \"foo\" } The table will be created during the pre-integration-test phase and dropped at the post-integration-test phase. Now, we can make our integration test much more meaningful with the help of jcabi-dynamo : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import com.jcabi.dynamo.Attributes ; import com.jcabi.dynamo.Conditions ; import com.jcabi.dynamo.Credentials ; import com.jcabi.dynamo.Region ; import com.jcabi.dynamo.Table ; import org.hamcrest.MatcherAssert ; import org.hamcrest.Matchers ; public class FooITCase { @Test public void worksWithAwsDynamoDb () { Region region = new Region . Simple ( new Credentials . Simple ( \"\" , \"\" )); Table table = region . table ( \"foo\" ); table . put ( new Attributes () . with ( \"id\" , 123 ) . with ( \"name\" , \"Robert DeNiro\" ) ); MatcherAssert . assertThat ( table . frame (). where ( \"id\" , Conditions . equalTo ( 123 )), Matchers . notEmpty () ); } } The above test will put a new item into the table and then assert that the item is there. The plugin was tested with three operating systems, and proved to work without problems: Mac OS X 10.8.5, Windows 7 SP1 and Ubuntu Linux 12.04 Desktop. "},{"title":"OOP Alternative to Utility Classes","url":"/2014/05/05/oop-alternative-to-utility-classes.html","tags":["oop","anti-pattern"],"date":"2014-05-05 00:00:00 +0000","categories":["best","jcg"],"body":"A utility class (aka helper class) is a \"structure\" that has only static methods and encapsulates no state. StringUtils , IOUtils , FileUtils from Apache Commons ; Iterables and Iterators from Guava , and Files from JDK7 are perfect examples of utility classes. This design idea is very popular in the Java world (as well as C#, Ruby, etc.) because utility classes provide common functionality used everywhere. Here, we want to follow the DRY principle and avoid duplication. Therefore, we place common code blocks into utility classes and reuse them when necessary: // This is a terrible design, don't reuse public class NumberUtils { public static int max ( int a , int b ) { return a > b ? a : b ; } } Indeed, this a very convenient technique!? Utility Classes Are Evil However, in an object-oriented world, utility classes are considered a very bad (some even may say \"terrible\") practice. There have been many discussions of this subject; to name a few: Are Helper Classes Evil? by Nick Malik, Why helper, singletons and utility classes are mostly bad by Simon Hart, Avoiding Utility Classes by Marshal Ward, Kill That Util Class! by Dhaval Dalal, Helper Classes Are A Code Smell by Rob Bagby. Additionally, there are a few questions on StackExchange about utility classes: If a “Utilities” class is evil, where do I put my generic code? , Utility Classes are Evil . A dry summary of all their arguments is that utility classes are not proper objects; therefore, they don't fit into object-oriented world. They were inherited from procedural programming, mostly because most were used to a functional decomposition paradigm back then. Assuming you agree with the arguments and want to stop using utility classes, I'll show by example how these creatures can be replaced with proper objects. Procedural Example Say, for instance, you want to read a text file, split it into lines, trim every line and then save the results in another file. This is can be done with FileUtils from Apache Commons: 1 2 3 4 5 6 7 8 void transform ( File in , File out ) { Collection < String > src = FileUtils . readLines ( in , \"UTF-8\" ); Collection < String > dest = new ArrayList <>( src . size ()); for ( String line : src ) { dest . add ( line . trim ()); } FileUtils . writeLines ( out , dest , \"UTF-8\" ); } The above code may look clean; however, this is procedural programming, not object-oriented. We are manipulating data (bytes and bits) and explicitly instructing the computer from where to retrieve them and then where to put them on every single line of code. We're defining a procedure of execution . Object-Oriented Alternative In an object-oriented paradigm, we should instantiate and compose objects, thus letting them manage data when and how they desire. Instead of calling supplementary static functions, we should create objects that are capable of exposing the behaviour we are seeking: public class Max implements Number { private final int a ; private final int b ; public Max ( int x , int y ) { this . a = x ; this . b = y ; } @Override public int intValue () { return this . a > this . b ? this . a : this . b ; } } This procedural call: int max = NumberUtils . max ( 10 , 5 ); Will become object-oriented: int max = new Max ( 10 , 5 ). intValue (); Potato, potato? Not really; just read on... Objects Instead of Data Structures This is how I would design the same file-transforming functionality as above but in an object-oriented manner: 1 2 3 4 5 6 7 8 9 void transform ( File in , File out ) { Collection < String > src = new Trimmed ( new FileLines ( new UnicodeFile ( in )) ); Collection < String > dest = new FileLines ( new UnicodeFile ( out ) ); dest . addAll ( src ); } FileLines implements Collection<String> and encapsulates all file reading and writing operations. An instance of FileLines behaves exactly as a collection of strings and hides all I/O operations. When we iterate it — a file is being read. When we addAll() to it — a file is being written. Trimmed also implements Collection<String> and encapsulates a collection of strings ( Decorator pattern ). Every time the next line is retrieved, it gets trimmed. All classes taking participation in the snippet are rather small: Trimmed , FileLines , and UnicodeFile . Each of them is responsible for its own single feature, thus following perfectly the single responsibility principle . On our side, as users of the library, this may be not so important, but for their developers it is an imperative. It is much easier to develop, maintain and unit-test class FileLines rather than using a readLines() method in a 80+ methods and 3000 lines utility class FileUtils . Seriously, look at its source code . An object-oriented approach enables lazy execution. The in file is not read until its data is required. If we fail to open out due to some I/O error, the first file won't even be touched. The whole show starts only after we call addAll() . All lines in the second snippet, except the last one, instantiate and compose smaller objects into bigger ones. This object composition is rather cheap for the CPU since it doesn't cause any data transformations. Besides that, it is obvious that the second script runs in O(1) space, while the first one executes in O(n). This is the consequence of our procedural approach to data in the first script. In an object-oriented world, there is no data; there are only objects and their behavior! "},{"title":"Why NULL is Bad?","url":"/2014/05/13/why-null-is-bad.html","tags":["oop","anti-pattern"],"date":"2014-05-13 00:00:00 +0000","categories":["best","jcg"],"body":"A simple example of NULL usage in Java: 1 2 3 4 5 6 7 public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { return null ; } return new Employee ( id ); } What is wrong with this method? It may return NULL instead of an object — that's what is wrong. NULL is a terrible practice in an object-oriented paradigm and should be avoided at all costs. There have been a number of opinions about this published already, including Null References, The Billion Dollar Mistake presentation by Tony Hoare and the entire Object Thinking book by David West. Here, I'll try to summarize all the arguments and show examples of how NULL usage can be avoided and replaced with proper object-oriented constructs. Basically, there are two possible alternatives to NULL . The first one is Null Object design pattern (the best way is to make it a constant): public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { return Employee . NOBODY ; } return Employee ( id ); } The second possible alternative is to fail fast by throwing an Exception when you can't return an object: public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { throw new EmployeeNotFoundException ( name ); } return Employee ( id ); } Now, let's see the arguments against NULL . Besides Tony Hoare's presentation and David West's book mentioned above, I read these publications before writing this post: Clean Code by Robert Martin, Code Complete by Steve McConnell, Say \"No\" to \"Null\" by John Sonmez, Is returning null bad design? discussion at StackOverflow. Ad-hoc Error Handling Every time you get an object as an input you must check whether it is NULL or a valid object reference. If you forget to check, a NullPointerException (NPE) may break execution in runtime. Thus, your logic becomes polluted with multiple checks and if/then/else forks: // this is a terrible design, don't reuse Employee employee = dept . getByName ( \"Jeffrey\" ); if ( employee == null ) { System . out . println ( \"can't find an employee\" ); System . exit (- 1 ); } else { employee . transferTo ( dept2 ); } This is how exceptional situations are supposed to be handled in C and other imperative procedural languages. OOP introduced exception handling primarily to get rid of these ad-hoc error handling blocks. In OOP, we let exceptions bubble up until they reach an application-wide error handler and our code becomes much cleaner and shorter: dept . getByName ( \"Jeffrey\" ). transferTo ( dept2 ); Consider NULL references an inheritance of procedural programming, and use 1) Null Objects or 2) Exceptions instead. Ambiguous Semantic In order to explicitly convey its meaning, the function getByName() has to be named getByNameOrNullIfNotFound() . The same should happen with every function that returns an object or NULL . Otherwise, ambiguity is inevitable for a code reader. Thus, to keep semantic unambiguous, you should give longer names to functions. To get rid of this ambiguity, always return a real object, a null object or throw an exception. Some may argue that we sometimes have to return NULL , for the sake of performance. For example, method get() of interface Map in Java returns NULL when there is no such item in the map: Employee employee = employees . get ( \"Jeffrey\" ); if ( employee == null ) { throw new EmployeeNotFoundException (); } return employee ; This code searches the map only once due to the usage of NULL in Map . If we would refactor Map so that its method get() will throw an exception if nothing is found, our code will look like this: if (! employees . containsKey ( \"Jeffrey\" )) { // first search throw new EmployeeNotFoundException (); } return employees . get ( \"Jeffrey\" ); // second search Obviously, this is method is twice as slow as the first one. What to do? The Map interface (no offense to its authors) has a design flaw. Its method get() should have been returning an Iterator so that our code would look like: Iterator found = Map . search ( \"Jeffrey\" ); if (! found . hasNext ()) { throw new EmployeeNotFoundException (); } return found . next (); BTW, that is exactly how C++ STL map::find() method is designed. Computer Thinking vs. Object Thinking Statement if (employee == null) is understood by someone who knows that an object in Java is a pointer to a data structure and that NULL is a pointer to nothing ( 0x00000000 , in Intel x86 processors). However, if you start thinking as an object, this statement makes much less sense. This is how our code looks from an object point of view: - Hello, is it a software department? - Yes. - Let me talk to your employee \"Jeffrey\" please. - Hold the line please... - Hello. - Are you NULL? The last question in this conversation sounds weird, doesn’t it? Instead, if they hang up the phone after our request to speak to Jeffrey, that causes a problem for us (Exception). At that point, we try to call again or inform our supervisor that we can't reach Jeffrey and complete a bigger transaction. Alternatively, they may let us speak to another person, who is not Jeffrey, but who can help with most of our questions or refuse to help if we need something \"Jeffrey specific\" (Null Object). Slow Failing Instead of failing fast , the code above attempts to die slowly, killing others on its way. Instead of letting everyone know that something went wrong and that an exception handling should start immediately, it is hiding this failure from its client. This argument is close to the \"ad-hoc error handling\" discussed above. It is a good practice to make your code as fragile as possible, letting it break when necessary. Make your methods extremely demanding as to the data they manipulate. Let them complain by throwing exceptions, if the provided data provided is not sufficient or simply doesn’t fit with the main usage scenario of the method. Otherwise, return a Null Object, that exposes some common behavior and throws exceptions on all other calls: public Employee getByName ( String name ) { int id = database . find ( name ); Employee employee ; if ( id == 0 ) { employee = new Employee () { @Override public String name () { return \"anonymous\" ; } @Override public void transferTo ( Department dept ) { throw new AnonymousEmployeeException ( \"I can't be transferred, I'm anonymous\" ); } }; } else { employee = Employee ( id ); } return employee ; } Mutable and Incomplete Objects In general, it is highly recommended to design objects with immutability in mind. This means that an object gets all necessary knowledge during its instantiating and never changes its state during the entire lifecycle. Very often, NULL values are used in lazy loading , to make objects incomplete and mutable. For example: public class Department { private Employee found = null ; public synchronized Employee manager () { if ( this . found == null ) { this . found = new Employee ( \"Jeffrey\" ); } return this . found ; } } This technology, although widely used, is an anti-pattern in OOP. Mostly because it makes an object responsible for performance problems of the computational platform, which is something an Employee object should not be aware of. Instead of managing a state and exposing its business-relevant behavior, an object has to take care of the caching of its own results — this is what lazy loading is about. Caching is not something an employee does in the office, does he? The solution? Don't use lazy loading in such a primitive way, as in the example above. Instead, move this caching problem to another layer of your application. For example, in Java, you can use aspect-oriented programming aspects. For example, jcabi-aspects has @Cacheable annotation that caches the value returned by a method: import com.jcabi.aspects.Cacheable ; public class Department { @Cacheable ( forever = true ) public Employee manager () { return new Employee ( \"Jacky Brown\" ); } } I hope this analysis was convincing enough that you will stop NULL -ing your code :) "},{"title":"Object-Oriented Github API","url":"/2014/05/14/object-oriented-github-java-sdk.html","tags":["github","jcabi"],"date":"2014-05-14 00:00:00 +0000","categories":[],"body":" Github is an awesome platform for maintaining Git sources and tracking project issues. I moved all my projects (both private and public) to Github about three years ago and have no regrets. Moreover, Github gives access to almost all of its features through RESTful JSON API. There are a few Java SDKs that wrap and expose the API. I tried to use them, but faced a number of issues: They are not really object-oriented (even though one of them has a description that says it is) They are not based on JSR-353 (JSON Java API) They provide no mocking instruments They don't cover the entire API and can't be extended Keeping in mind all those drawbacks, I created my own library — jcabi-github . Let's look at its most important advantages. Object Oriented for Real Github server is an object. A collection of issues is an object, an individual issue is an object, its author is an author, etc. For example, to retrieve the name of the author we use: Github github = new RtGithub ( /* credentials */ ); Repos repos = github . repos (); Repo repo = repos . get ( new Coordinates . Simple ( \"jcabi/jcabi-github\" )); Issues issues = github . issues (); Issue issue = issues . get ( 123 ); User author = new Issue . Smart ( issue ). author (); System . out . println ( author . name ()); Needless to say, Github , Repos , Repo , Issues , Issue , and User are interfaces. Classes that implement them are not visible in the library. Mock Engine MkGithub class is a mock version of a Github server. It behaves almost exactly the same as a real server and is the perfect instrument for unit testing. For example, say that you're testing a method that is supposed to post a new issue to Github and add a message into it. Here is how the unit test would look: public class FooTest { @Test public void createsIssueAndPostsMessage () { Github github = new MkGithub ( \"jeff\" ); github . repos (). create ( Json . createObjectBuilder (). add ( \"name\" , owner ). build () ); new Foo (). doTheThing ( github ); MatcherAssert . assertThat ( github . issues (). get ( 1 ). comments (). iterate (), Matchers . not ( Matchers . emptyIterable ()) ); } } This is much more convenient and compact than traditional mocking via Mockito or a similar framework. Extendable It is based on JSR-353 and uses jcabi-http for HTTP request processing. This combination makes it highly customizable and extendable, when some Github feature is not covered by the library (and there are many of them). For example, you want to get the value of hireable attribute of a User . Class User.Smart doesn't have a method for it. So, here is how you would get it: User user = // get it somewhere // name() method exists in User.Smart, let's use it System . out . println ( new User . Smart ( user ). name ()); // there is no hireable() method there System . out . println ( user . json (). getString ( \"hireable\" )); We're using method json() that returns an instance of JsonObject from JSR-353 (part of Java7). No other library allows such direct access to JSON objects returned by the Github server. Let's see another example. Say, you want to use some feature from Github that is not covered by the API. You get a Request object from Github interface and directly access the HTTP entry point of the server: Github github = new RtGithub ( oauthKey ); int found = github . entry () . uri (). path ( \"/search/repositories\" ). back () . method ( Request . GET ) . as ( JsonResponse . class ) . getJsonObject () . getNumber ( \"total_count\" ) . intValue (); jcabi-http HTTP client is used by jcabi-github . Immutable All classes are truly immutable and annotated with @Immutable . This may sound like a minor benefit, but it was very important for me. I'm using this annotation in all my projects to ensure my classes are truly immutable. Version 0.8 A few days ago we released the latest version 0.8 . It is a major release, that included over 1200 commits. It covers the entire Github API and is supposed to be very stable. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-github </artifactId> </dependency> "},{"title":"Atomic Counters at Stateful.co","url":"/2014/05/18/cloud-autoincrement-counters.html","tags":["stateful","aws","dynamodb"],"date":"2014-05-18 00:00:00 +0000","categories":[],"body":" Amazon DynamoDB is a great NoSQL cloud database. It is cheap, highly reliable and rather powerful. I'm using it in many web systems. There is one feature that it lacks, though — auto-increment attributes. Say that you have a table with a list of messages: +------+----------------------------+ | id | Attributes | +------+----------------------------+ | 205 | author=\"jeff\", text=\"...\" | | 206 | author=\"bob\", text=\"...\" | | 207 | author=\"alice\", text=\"...\" | +------+----------------------------+ Every time you add a new item to the table, a new value of id has to be set. And this has to be done with concurrency in mind. SQL databases like PostgreSQL, Oracle, MySQL and others support auto-increment features. When you add a new record to the table, the value of the primary key is omitted and the server retrieves the next one automatically. If a number of INSERT requests arrive at the same time the server guarantees that the numbers won't be duplicated. However, DynamoDB doesn't have this feature. Instead, DynamoDB has Atomic Counters and Conditional Updates , which are very similar features. Still, they're not exactly the same. In case of an atomic counter, you should create a supplementary table and keep the latest value of id in it. In case of conditional updates, you should retry a few times in case of collisions. To make life easier in a few of my applications, I created a simple web service — stateful.co . It provides a simple atomic counter feature through its RESTful API. First, you create a counter with a unique name. Then, you set its initial value (it is zero by default). And, that's it. Every time you need to obtain a new value for id column in DynamoDB table, you make an HTTP request to stateful.co asking to increment your counter by one and return its next value. stateful.co guarantees that values returned will never duplicate each other — no matter how many clients are using a counter or how fast they request increments simultaneously. Moreover, I designed a small Java SDK for stateful.co . All you need to do is add this java-sdk.jar Maven dependency to your project: <dependency> <groupId> co.stateful </groupId> <artifactId> java-sdk </artifactId> <version> 0.6 </version> </dependency> And, you can use stateful.co counters from Java code: Sttc sttc = new RtSttc ( new URN ( \"urn:github:526301\" ), \"9FF3-41E0-73FB-F900\" ); Counters counters = sttc . counters (); Counter counter = counters . get ( \"foo\" ); long value = counter . incrementAndGet ( 1L ); System . out . println ( \"new value: \" + value ); You can review authentication parameters for RtSttc constructor at stateful.co . The service is absolutely free of charge. "},{"title":"MySQL Maven Plugin","url":"/2014/05/21/mysql-maven-plugin.html","tags":["mysql","maven","java"],"date":"2014-05-21 00:00:00 +0000","categories":[],"body":"I was using MySQL in a few Java web projects and found out there was no Maven plugin that would help me to test my DAO classes against a real MySQL server. There are plenty of mechanisms to mock a database persistence layer both in memory and on disc. However, it is always good to make sure that your classes are tested against a database identical to the one you have in production environment. I've created my own Maven plugin, jcabi-mysql-maven-plugin , that does exactly two things: starts a MySQL server on pre-integration-test phase and shuts it down on post-integration-test . This is how you configure it in pom.xml (see also its full usage instructions ): <project> <build> <plugins> <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> build-helper-maven-plugin </artifactId> <executions> <execution> <goals> <goal> reserve-network-port </goal> </goals> <configuration> <portNames> <portName> mysql.port </portName> </portNames> </configuration> </execution> </executions> </plugin> <plugin> <artifactId> maven-dependency-plugin </artifactId> <executions> <execution> <goals> <goal> unpack </goal> </goals> <configuration> <artifactItems> <artifactItem> <groupId> com.jcabi </groupId> <artifactId> mysql-dist </artifactId> <version> 5.6.14 </version> <classifier> ${mysql.classifier} </classifier> <type> zip </type> <overWrite> false </overWrite> <outputDirectory> ${project.build.directory}/mysql-dist </outputDirectory> </artifactItem> </artifactItems> </configuration> </execution> </executions> </plugin> <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-mysql-maven-plugin </artifactId> <executions> <execution> <id> mysql-test </id> <goals> <goal> classify </goal> <goal> start </goal> <goal> stop </goal> </goals> <configuration> <port> ${mysql.port} </port> <data> ${project.build.directory}/mysql-data </data> </configuration> </execution> </executions> </plugin> <plugin> <artifactId> maven-failsafe-plugin </artifactId> <configuration> <systemPropertyVariables> <mysql.port> ${mysql.port} </mysql.port> </systemPropertyVariables> </configuration> <executions> <execution> <goals> <goal> integration-test </goal> <goal> verify </goal> </goals> </execution> </executions> </plugin> </plugins> </build> [...] </project> There are two plugins configured above. Let's take a look at what each does. build-helper-maven-plugin is reserving a temporary random TCP port, which will be used by MySQL server. We don't want to start a server on its default 3306 port, because there could be another server already running there. Besides that, if we use a hard-coded TCP port, we won't be able to run multiple builds in parallel. Maybe not a big deal when you're developing locally, but in continuous integration environment this can be a problem. That's why we're reserving a TCP port first. maven-dependency-plugin is downloading a MySQL distribution in a zip archive (rather big file, over 300Mb for Linux), and unpacks it. This archive contains exactly the same files as you would use for a traditional MySQL installation. When the archive is unpacked, it is ready to start serving SQL requests as a normal MySQL server. jcabi-mysql-maven-plugin starts a server, binding it to a TCP port reserved randomly. The main responsibility of my Maven plugin is to make sure that MySQL server starts correctly on every platform (Mac OS, Linux, Windows) and stops when it's not needed any more. All the rest is done by the MySQL distribution itself. maven-failsafe-plugin is running unit tests on integration-test phase. Its main difference from maven-surefire-plugin is that it doesn't fail a build when some tests fail. Instead, it saves all failures into supplementary files in target directory and allows the build continue. Later, when we call its verify goal, it will fail a build if there were any errors during its integration-test goal execution. To be precise, this is the order in which Maven will execute configured goals: jcabi-mysql-maven-plugin:classify maven-dependency-plugin:unpack build-helper-maven-plugin:reserve-network-port jcabi-mysql-maven-plugin:start maven-failsafe-plugin:integration-test jcabi-mysql-maven-plugin:stop maven-failsafe-plugin:verify Run mvn clean install and see how it works. If it doesn't work for some reason, don't hesitate to report an issue to Github . Now it's time to create an integration test, which will connect to the temporary MySQL server, create a table there and insert some data into it. This is just an example to show that MySQL server is running and is capable of serving transactions (I'm using jcabi-jdbc ): public class FooITCase { private static final String PORT = System . getProperty ( \"mysql.port\" ); @Test public void worksWithMysqlServer () { Connection conn = DriverManager . getConnection ( String . format ( \"jdbc:mysql://localhost:%s/root?user=root&password=root\" , FooITCase . PORT ) ); new JdbcSession ( conn ) . sql ( \"CREATE TABLE foo (id INT PRIMARY KEY)\" ) . execute (); } } If you're using Hibernate, just create a db.properties file in src/test/resources directory. In that file you would do something like: hibernate.connection.url = jdbc:mysql://localhost:${mysql.port}/root hibernate.connection.username = root hibernate.connection.password = root Maven will replace that ${mysql.port} with the number of reserved TCP port, during resources copying. This operation is called \"resources filtering\", and you can read about it here . That's pretty much it. I'm using jcabi-mysql-maven-plugin in a few projects, and it helps me to stay confident that my code works with a real MySQL server. I'm also using the Liquibase Maven plugin in order to populate an empty server with tables required for the application. Nevertheless, that is a story for the next post :) "},{"title":"Get Rid of Java Static Loggers","url":"/2014/05/23/avoid-java-static-logger.html","tags":["logging","java","slf4j"],"date":"2014-05-23 00:00:00 +0000","categories":[],"body":"This is a very common practice in Java (using LoggerFactory from slf4j ): import org.slf4j.LoggerFactory ; public class Foo { private static final Logger LOGGER = LoggerFactory . getLogger ( Foo . class ); public void save ( String file ) { // save the file if ( Foo . LOGGER . isInfoEnabled ()) { Foo . LOGGER . info ( \"file {} saved successfuly\" , file ); } } } What's wrong with it? Code duplication. This static LOGGER property has to be declared in every class where logging is required. Just a few lines of code, but this is pure noise, as I see it. To make life easier, I created a library about two years ago, jcabi-log , which has a convenient utility class Logger (yes, I know that utility classes are evil ). import com.jcabi.log.Logger ; public class Foo { public void save ( String file ) { // save the file Logger . info ( this , \"file %s saved successfuly\" , file ); } } This looks much cleaner to me and does exactly the same — sends a single log line to the SLF4J logging facility. Besides, it check automatically whether a given logging level is enabled (for performance optimization) and formats the given string using Formatter (same as String.format() ). For convenience, there are also a number of \"decors\" implemented in the library. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-log </artifactId> </dependency> "},{"title":"Object-Oriented Java Adapter of Amazon S3 SDK","url":"/2014/05/26/amazon-s3-java-oop-adapter.html","tags":["aws","s3","java"],"date":"2014-05-26 00:00:00 +0000","categories":[],"body":" I'm a big fan of Amazon Web Services (AWS). I'm using them in almost all of my projects. One of their most popular services is Simple Storage Service (S3) . It is a storage for binary objects (files) with unique names, accessible through HTTP or RESTful API. Using S3 is very simple. You create a \"bucket\" with a unique name, upload your \"object\" into the bucket through their web interface or through RESTful API, and then download it again (either through HTTP or the API.) Amazon ships the Java SDK that wraps their RESTful API. However, this SDK is not object-oriented at all. It is purely imperative and procedural — it just mirrors the API. For example, in order to download an existing object doc.txt from bucket test-1 , you have to do something like this: AWSCredentials creds = new BasicAWSCredentials ( key , secret ); AmazonS3 aws = new AmazonS3Client ( creds ); S3Object obj = aws . getObject ( new GetObjectRequest ( \"test-1\" , \"doc.txt\" ) ); InputStream input = obj . getObjectContent (); String content = IOUtils . toString ( input , \"UTF-8\" ); input . close (); As always, procedural programming has its inevitable disadvantages. To overcome them all, I designed jcabi-s3 , which is a small object-oriented adapter for Amazon SDK. This is how the same object-reading task can be accomplished with jcabi-s3 : Region region = new Region . Simple ( key , secret ); Bucket bucket = region . bucket ( \"test-1\" ); Ocket ocket = bucket . ocket ( \"doc.txt\" ); String content = new Ocket . Text ( ocket ). read (); Why is this approach better? Well, there are a number of obvious advantages. S3 Object is an Object in Java S3 object get its representative in Java. It is not a collection of procedures to be called in order to get its properties (as with AWS SDK). Rather, it is a Java object with certain behaviors. I called them \"ockets\" (similar to \"buckets\"), in order to avoid clashes with java.lang.Object . Ocket is an interface, that exposes the behavior of a real AWS S3 object: read, write, check existence. There is also a convenient decorator Ocket.Text that simplifies working with binary objects: Ocket . Text ocket = new Ocket . Text ( ocket_from_s3 ); if ( ocket . exists ()) { System . out . print ( ocket . read ()); } else { ocket . write ( \"Hello, world!\" ); } Now, you can pass an object to another class, instead of giving it your AWS credentials, bucket name, and object name. You simply pass a Java object, which encapsulates all AWS interaction details. Extendability Through Decoration Since jcabi-s3 exponses all entities as interfaces, they can easily be extended through encapsulation ( Decorator Pattern ). For example, you want your code to retry S3 object read operations a few times before giving up and throwing an IOException (by the way, this is a very good practice when working with web services). So, you want all your S3 reading operations to be redone a few times if first attempts fail. You define a new decorator class, say, RetryingOcket , which encapsulates an original Ocket : public RetryingOcket implements Ocket { private final Ocket origin ; public RetryingOcket ( Ocket ocket ) { this . origin = ocket ; } @Override public void read ( OutputStream stream ) throws IOException { int attempt = 0 ; while ( true ) { try { this . origin . read ( stream ); } catch ( IOException ex ) { if ( attempt ++ > 3 ) { throw ex ; } } } } // same for other methods } Now, everywhere where Ocket is expected you send an instance of RetryingOcket that wraps your original object: foo . process ( new RetryingOcket ( ocket )); Method foo.process() won't see a difference, since it is the same Ocket interface it is expecting. By the way, this retry functionality is implemented out-of-the-box in jcabi-s3 , in com.jcabi.s3.retry package. Easy Mocking Again, due to the fact that all entities in jcabi-s3 are interfaces, they are very easy to mock. For example, your class expects an S3 object, reads its data and calculates the MD5 hash (I'm using DigestUtils from commons-codec ): import com.jcabi.s3.Ocket ; import org.apache.commons.codec.digest.DigestUtils ; public class S3Md5Hash { private final Ocket ocket ; public S3Md5Hash ( Ocket okt ) { this . ocket = okt ; } public hash () throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream (); this . ocket . read ( baos ); return DigestUtils . md5hex ( baos . toByteArray ()); } } Here is how simple a unit test will look (try to create a unit test for a class using AWS SDK and you will see the difference): import com.jcabi.s3.Ocket ; import org.junit.Test ; public class S3Md5HashTest { @Test public void generatesHash () { Ocket ocket = Mockito . mock ( Ocket . class ); Mockito . doAnswer ( new Answer < Void >() { public Void answer ( final InvocationOnMock inv ) throws IOException { OutputStream . class . cast ( inv . getArguments ()[ 0 ]). write ( ' ' ); } } ). when ( ocket ). read ( Mockito . any ( OutputStream . class )); String hash = new S5Md5Hash ( ocket ); Assert . assertEquals ( hash , \"7215ee9c7d9dc229d2921a40e899ec5f\" ); } } I'm using JUnit and Mockito in this test. Immutability All classes in jcabi-s3 are annotated with @Immutable and are truly immutable. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-s3 </artifactId> </dependency> As always, your comments and criticism are welcome as Github issues . "},{"title":"Java Method Logging with AOP and Annotations","url":"/2014/06/01/aop-aspectj-java-method-logging.html","tags":["aop","java","logging","jcabi"],"date":"2014-06-01 00:00:00 +0000","categories":["jcg"],"body":"Sometimes, I want to log (through slf4j and log4j ) every execution of a method, seeing what arguments it receives, what it returns and how much time every execution takes. This is how I'm doing it, with help of AspectJ , jcabi-aspects and Java 6 annotations: public class Foo { @Loggable public int power ( int x , int p ) { return Math . pow ( x , p ); } } This is what I see in log4j output: [INFO] com.example.Foo #power(2, 10): 1024 in 12μs [INFO] com.example.Foo #power(3, 3): 27 in 4μs Nice, isn't it? Now, let's see how it works. Annotation with Runtime Retention Annotations is a technique introduced in Java 6. It is a meta-programming instrument that doesn't change the way code works, but gives marks to certain elements (methods, classes or variables). In other words, annotations are just markers attached to the code that can be seen and read. Some annotations are designed to be seen at compile time only — they don't exist in .class files after compilation. Others remain visible after compilation and can be accessed in runtime. For example, @Override is of the first type (its retention type is SOURCE ), while @Test from JUnit is of the second type (retention type is RUNTIME ). @Loggable — the one I'm using in the script above — is an annotation of the second type, from jcabi-aspects . It stays with the bytecode in the .class file after compilation. Again, it is important to understand that even though method power() is annotated and compiled, it doesn't send anything to slf4j so far. It just contains a marker saying \"please, log my execution\". Aspect Oriented Programming (AOP) AOP is a useful technique that enables adding executable blocks to the source code without explicitly changing it. In our example, we don't want to log method execution inside the class. Instead, we want some other class to intercept every call to method power() , measure its execution time and send this information to slf4j. We want that interceptor to understand our @Loggable annotation and log every call to that specific method power() . And, of course, the same interceptor should be used for other methods where we'll place the same annotation in the future. This case perfectly fits the original intent of AOP — to avoid re-implementation of some common behavior in multiple classes. Logging is a supplementary feature to our main functionality, and we don't want to pollute our code with multiple logging instructions. Instead, we want logging to happen behind the scenes. In terms of AOP, our solution can be explained as creating an aspect that cross-cuts the code at certain join points and applies an around advice that implements the desired functionality. AspectJ Let's see what these magic words mean. But, first, let's see how jcabi-aspects implements them using AspectJ (it's a simplified example, full code you can find in MethodLogger.java ): @Aspect public class MethodLogger { @Around ( \"execution(* *(..)) && @annotation(Loggable)\" ) public Object around ( ProceedingJoinPoint point ) { long start = System . currentTimeMillis (); Object result = point . proceed (); Logger . info ( \"#%s(%s): %s in %[msec]s\" , MethodSignature . class . cast ( point . getSignature ()). getMethod (). getName (), point . getArgs (), result , System . currentTimeMillis () - start ); return result ; } } This is an aspect with a single around advice around() inside. The aspect is annotated with @Aspect and advice is annotated with @Around . As discussed above, these annotations are just markers in .class files. They don't do anything except provide some meta-information to those w ho are interested in runtime. Annotation @Around has one parameter, which — in this case — says that the advice should be applied to a method if: its visibility modifier is * ( public , protected or private ); its name is name * (any name); its arguments are .. (any arguments); and it is annotated with @Loggable When a call to an annotated method is to be intercepted, method around() executes before executing the actual method. When a call to method power() is to be intercepted, method around() receives an instance of class ProceedingJoinPoint and must return an object, which will be used as a result of method power() . In order to call the original method, power() , the advice has to call proceed() of the join point object. We compile this aspect and make it available in classpath together with our main file Foo.class . So far so good, but we need to take one last step in order to put our aspect into action — we should apply our advice. Binary Aspect Weaving Aspect weaving is the name of the advice applying process. Aspect weaver modifies original code by injecting calls to aspects. AspectJ does exactly that. We give it two binary Java classes Foo.class and MethodLogger.class ; it gives back three — modified Foo.class , Foo$AjcClosure1.class and unmodified MethodLogger.class . In order to understand which advices should be applied to which methods, AspectJ weaver is using annotations from .class files. Also, it uses reflection to browse all classes on classpath. It analyzes which methods satisfy the conditions from the @Around annotation. Of course, it finds our method power() . So, there are two steps. First, we compile our .java files using javac and get two files. Then, AspectJ weaves/modifies them and creates its own extra class. Our Foo class looks something like this after weaving: public class Foo { private final MethodLogger logger ; @Loggable public int power ( int x , int p ) { return this . logger . around ( point ); } private int power_aroundBody ( int x , int p ) { return Math . pow ( x , p ); } } AspectJ weaver moves our original functionality to a new method, power_aroundBody() , and redirects all power() calls to the aspect class MethodLogger . Instead of one method power() in class Foo now we have four classes working together. From now on, this is what happens behind the scenes on every call to power() : Original functionality of method power() is indicated by the small green lifeline on the diagram. As you see, the aspect weaving process connects together classes and aspects, transferring calls between them through join points. Without weaving, both classes and aspects are just compiled Java binaries with attached annotations. jcabi-aspects jcabi-aspects is a JAR library that contains Loggable annotation and MethodLogger aspect (btw, there are many more aspects and annotations). You don't need to write your own aspect for method logging. Just add a few dependencies to your classpath and configure jcabi-maven-plugin for aspect weaving (get their latest versions in Maven Central ): <project> <dependencies> <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-aspects </artifactId> </dependency> <dependency> <groupId> org.aspectj </groupId> <artifactId> aspectjrt </artifactId> </dependency> </dependencies> <build> <plugins> <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-maven-plugin </artifactId> <executions> <execution> <goals> <goal> ajc </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> Since this weaving procedure takes a lot of configuration effort, I created a convenient Maven plugin with an ajc goal, which does the entire aspect weaving job. You can use AspectJ directly, but I recommend that you use jcabi-maven-plugin . That's it. Now you can use @com.jcabi.aspects.Loggable annotation and your methods will be logged through slf4j. If something doesn't work as explained, don't hesitate to submit a Github issue . "},{"title":"Objects Should Be Immutable","url":"/2014/06/09/objects-should-be-immutable.html","tags":["oop","anti-pattern"],"date":"2014-06-09 00:00:00 +0000","categories":["best","jcg"],"body":"In object-oriented programming, an object is immutable if its state can't be modified after it is created. In Java, a good example of an immutable object is String . Once created, we can't modify its state. We can request that it creates new strings, but its own state will never change. However, there are not so many immutable classes in JDK. Take, for example, class Date . It is possible to modify its state using setTime() . I don't know why the JDK designers decided to make these two very similar classes differently. However, I believe that the design of a mutable Date has many flaws, while the immutable String is much more in the spirit of the object-oriented paradigm. Moreover, I think that all classes should be immutable in a perfect object-oriented world . Unfortunately, sometimes, it is technically not possible due to limitations in JVM. Nevertheless, we should always aim for the best. This is an incomplete list of arguments in favor of immutability: immutable objects are simpler to construct, test, and use truly immutable objects are always thread-safe they help to avoid temporal coupling their usage is side-effect free (no defensive copies) identity mutability problem is avoided they always have failure atomicity they are much easier to cache they prevent NULL references, which are bad Let's discuss the most important arguments one by one. Thread Safety The first and the most obvious argument is that immutable objects are thread-safe. This means that multiple threads can access the same object at the same time, without clashing with another thread. If no object methods can modify its state, no matter how many of them and how often are being called parallel — they will work in their own memory space in stack. Goetz et al. explained the advantages of immutable objects in more details in their very famous book Java Concurrency in Practice (highly recommended). Avoiding Temporal Coupling Here is an example of temporal coupling (the code makes two consecutive HTTP POST requests, where the second one contains HTTP body): 1 2 3 4 5 Request request = new Request ( \"http://example.com\" ); request . method ( \"POST\" ); String first = request . fetch (); request . body ( \"text=hello\" ); String second = request . fetch (); This code works. However, you must remember that the first request should be configured before the second one may happen. If we decide to remove the first request from the script, we will remove the second and the third line, and won't get any errors from the compiler: Request request = new Request ( \"http://example.com\" ); // request.method(\"POST\"); // String first = request.fetch(); request . body ( \"text=hello\" ); String second = request . fetch (); Now, the script is broken although it compiled without errors. This is what temporal coupling is about — there is always some hidden information in the code that a programmer has to remember. In this example, we have to remember that the configuration for the first request is also used for the second one. We have to remember that the second request should always stay together and be executed after the first one. If Request class were immutable, the first snippet wouldn't work in the first place, and would have been rewritten like: final Request request = new Request ( \"\" ); String first = request . method ( \"POST\" ). fetch (); String second = request . method ( \"POST\" ). body ( \"text=hello\" ). fetch (); Now, these two requests are not coupled. We can safely remove the first one, and the second one will still work correctly. You may point out that there is a code duplication. Yes, we should get rid of it and re-write the code: final Request request = new Request ( \"\" ); final Request post = request . method ( \"POST\" ); String first = post . fetch (); String second = post . body ( \"text=hello\" ). fetch (); See, refactoring didn't break anything and we still don't have temporal coupling. The first request can be removed safely from the code without affecting the second one. I hope this example demonstrates that the code manipulating immutable objects is more readable and maintainable, because it doesn't have temporal coupling. Avoiding Side Effects Let's try to use our Request class in a new method (now it is mutable): public String post ( Request request ) { request . method ( \"POST\" ); return request . fetch (); } Let's try to make two requests — the first with GET method and the second with POST: Request request = new Request ( \"http://example.com\" ); request . method ( \"GET\" ); String first = this . post ( request ); String second = request . fetch (); Method post() has a \"side effect\" — it makes changes to the mutable object request . These changes are not really expected in this case. We expect it to make a POST request and return its body. We don't want to read its documentation just to find out that behind the scene it also modifies the request we're passing to it as an argument. Needless to say, such side effects lead to bugs and maintainability issues. It would be much better to work with an immutable Request : public String post ( Request request ) { return request . method ( \"POST\" ). fetch (); } In this case, we may not have any side effects. Nobody can modify our request object, no matter where it is used and how deep through the call stack it is passed by method calls: Request request = new Request ( \"http://example.com\" ). method ( \"GET\" ); String first = this . post ( request ); String second = request . fetch (); This code is perfectly safe and side effect free. Avoiding Identity Mutability Very often, we want objects to be identical if their internal states are the same. Date class is a good example: Date first = new Date ( 1L ); Date second = new Date ( 1L ); assert first . equals ( second ); // true There are two different objects; however, they are equal to each other because their encapsulated states are the same. This is made possible through their custom overloaded implementation of equals() and hashCode() methods. The consequence of this convenient approach being used with mutable objects is that every time we modify object's state it changes its identity: Date first = new Date ( 1L ); Date second = new Date ( 1L ); first . setTime ( 2L ); assert first . equals ( second ); // false This may look natural, until you start using your mutable objects as keys in maps: Map < Date , String > map = new HashMap <>(); Date date = new Date (); map . put ( date , \"hello, world!\" ); date . setTime ( 12345L ); assert map . containsKey ( date ); // false When modifying the state of date object, we're not expecting it to change its identity. We're not expecting to lose an entry in the map just because the state of its key is changed. However, this is exactly what is happening in the example above. When we add an object to the map, its hashCode() returns one value. This value is used by HashMap to place the entry into the internal hash table. When we call containsKey() hash code of the object is different (because it is based on its internal state) and HashMap can't find it in the internal hash table. It is a very annoying and difficult to debug side effects of mutable objects. Immutable objects avoid it completely. Failure Atomicity Here is a simple example: public class Stack { private int size ; private String [] items ; public void push ( String item ) { size ++; if ( size > items . length ) { throw new RuntimeException ( \"stack overflow\" ); } items [ size ] = item ; } } It is obvious that an object of class Stack will be left in a broken state if it throws a runtime exception on overflow. Its size property will be incremented, while items won't get a new element. Immutability prevents this problem. An object will never be left in a broken state because its state is modified only in its constructor. The constructor will either fail, rejecting object instantiation, or succeed, making a valid solid object, which never changes its encapsulated state. For more on this subject, read Effective Java, 2nd Edition by Joshua Bloch. Arguments Against Immutability There are a number of arguments against immutability. “Immutability is not for enterprise systems”. Very often, I hear people say that immutability is a fancy feature, while absolutely impractical in real enterprise systems. As a counter-argument, I can only show some examples of real-life applications that contain only immutable Java objects: jcabi-http , jcabi-xml , jcabi-github , jcabi-s3 , jcabi-dynamo , jcabi-simpledb The above are all Java libraries that work solely with immutable classes/objects. netbout.com and stateful.co are web applications that work solely with immutable objects. “It's cheaper to update an existing object than create a new one”. Oracle thinks that “The impact of object creation is often overestimated and can be offset by some of the efficiencies associated with immutable objects. These include decreased overhead due to garbage collection, and the elimination of code needed to protect mutable objects from corruption.” I agree. If you have some other arguments, please post them below and I'll try to comment. P.S. Check takes.org , a Java web framework that consists entirely of immutable objects. "},{"title":"Avoid String Concatenation","url":"/2014/06/19/avoid-string-concatenation.html","tags":["java","oop","anti-pattern"],"date":"2014-06-19 00:00:00 +0000","categories":[],"body":"This is \"string concatentation\", and it is a bad practice: // bad practice, don't reuse! String text = \"Hello, \" + name + \"!\" ; Why? Some may say that it is slow, mostly because parts of the resulting string are copied multiple times. Indeed, on every + operator, String class allocates a new block in memory and copies everything it has into it; plus a suffix being concatenated. This is true, but this is not the point here. Actually, I don't think performance in this case is a big issue. Moreover, there were multiple experiments showing that concatenation is not that slow when compared to other string building methods and sometimes is even faster. Some say that concatenated strings are not localizable because in different languages text blocks in a phrase may be positioned in a different order. The example above can't be translated to, say, Russian, where we would want to put a name in front of \"привет\". We will need to localize the entire block of code, instead of just translating a phrase. However, my point here is different. I strongly recommend avoiding string concatenation because it is less readable than other methods of joining texts together. Let's see these alternative methods. I'd recommend three of them (in order of preference): String.format() , Apache StringUtils and Guava Joiner . There is also a StringBuilder , but I don't find it as attractive as StringUtils . It is a useful builder of strings, but not a proper replacer or string concatenation tool when readability is important. String.format() String.format() is my favorite option. It makes text phrases easy to understand and modify. It is a static utility method that mirrors sprintf() from C. It allows you to build a string using a pattern and substitutors: String text = String . format ( \"Hello, %s!\" , name ); When the text is longer, the advantages of the formatter become much more obvious. Look at this ugly code: String msg = \"Dear \" + customer . name () + \", your order #\" + order . number () + \" has been shipped at \" + shipment . date () + \"!\" ; This one looks much more beautiful doesn’t it: String msg = String . format ( \"Dear %1$s, your order #%2$d has been shipped at %3$tR!\" , customer . name (), order . number (), shipment . date () ); Please note that I'm using argument indexes in order to make the pattern even more localizable. Let's say, I want to translate it to Greek. This is how will it look: Αγαπητέ %1$s, στις %3$tR στείλαμε την παραγγελία σου με αριθμό #%2$d! I'm changing the order of substitutions in the pattern, but not in the actual list of methods arguments. Apache StringUtils.join() When the text is rather long (longer than your screen width), I would recommend that you use the utility class StringUtils from Apache commons-lang3 : import org.apache.commons.lang3.StringUtils ; String xml = StringUtils . join ( \"<?xml version='1.0'?>\" , \"<html><body>\" , \"<p>This is a test XHTML document,\" , \" which would look ugly,\" , \" if we would use a single line,\" \" or string concatenation or String format().</p>\" \"</body></html>\" ); The need to include an additional JAR dependency to your classpath may be considered a downside with this method (get its latest versions in Maven Central ): <dependency> <groupId> org.apache.commons </groupId> <artifactId> commons-lang3 </artifactId> </dependency> Guava Joiner Similar functionality is provided by Joiner from Google Guava : import com.google.common.base.Joiner ; String text = Joiner . on ( '' ). join ( \"WE HAVE BUNNY.\\n\" , \"GATHER ONE MILLION DOLLARS IN UNMARKED \" , \"NON-CONSECUTIVE TWENTIES.\\n\" , \"AWAIT INSTRUCTIONS.\\n\" , \"NO FUNNY STUFF\" ); It is a bit less convenient than StringUtils since you always have to provide a joiner (character or a string placed between text blocks). Again, a dependency is required in this case: <dependency> <groupId> com.google.guava </groupId> <artifactId> guava </artifactId> </dependency> Yes, in most cases, all of these methods work slower than a plain simple concatenation. However, I strongly believe that computers are cheaper than people . What I mean is that the time spent by programmers understanding and modifying ugly code is much more expensive than a cost of an additional server that will make beautifully written code work faster. If you know any other methods of avoiding string concatenation, please comment below. "},{"title":"Limit Java Method Execution Time","url":"/2014/06/20/limit-method-execution-time.html","tags":["java","aop"],"date":"2014-06-20 00:00:00 +0000","categories":[],"body":" Say, you want to allow a Java method to work for a maximum of five seconds and want an exception to be thrown if the timeframe is exceeded. Here is how you can do it with jcabi-aspects and AspectJ : public class Resource { @Timeable ( limit = 5 , unit = TimeUnit . SECONDS ) public String load ( URL url ) { return url . openConnection (). getContent (); } } Keep in mind that you should weave your classes after compilation, as explained here . Let's discuss how this actually works, but first, I recommend you read this post , which explains how AOP aspects work together with Java annotations. Due to @Timeable annotation and class weaving, every call to a method load() is intercepted by an aspect from jcabi-aspects . That aspect starts a new thread that monitors the execution of a method every second, checking whether it is still running. If the method runs for over five seconds, the thread calls interrupt() on the method's thread. Despite a very common expectation that a thread should be terminated immediately on that call, it is not happening at all. This article explains the mechanism in more detail. Let's discuss it briefly: interrupt() sets a marker in a thread; The thread checks interrupted() as often as it can; If the marker is set, the thread stops and throws InterruptedException This method will not react to interrupt() call and will work until JVM is killed (very bad design): public void work () { while ( true ) { // do something } } This is how we should refactor it in order to make sensitive to interruption requests: public void work () { while ( true ) { if ( Thread . interruped ()) { throw new InterruptedException (); } // do something } } In other words, your method can only stop itself. Nothing else can do it. The thread it is running in can't be terminated by another thread. The best thing that the other thread can do is to send your thread a \"message\" (through interrupt() method) that it's time to stop. If your thread ignores the message, nobody can do anything. Most I/O operations in JDK are designed this way. They check the interruption status of their threads while waiting for I/O resources. Thus, use @Timeable annotation, but keep in mind that there could be situations when a thread can't be interrupted. "},{"title":"CasperJS Tests in Maven Build","url":"/2014/06/21/casperjs-with-maven.html","tags":["maven","casperjs","phantomjs","testing"],"date":"2014-06-21 00:00:00 +0000","categories":[],"body":"I'm a big fan of automated testing in general and integration testing in particular. I strongly believe that effort spent on writing tests are direct investments into quality and stability of the product under development. CasperJS is a testing framework on top of PhantomJS , which is a headless browser. Using CasperJS, we can ensure that our application responds correctly to requests sent by a regular web browser. This is a sample CasperJS test, which makes an HTTP request to a home page of a running WAR application and asserts that the response has 200 HTTP status code: casper . test . begin ( 'home page can be rendered' , function ( test ) { casper . start ( casper . cli . get ( 'home' ), // URL of home page function () { test . assertHttpStatus ( 200 ); } ); casper . run ( function () { test . done (); } ); } ); I keep this test in the src/test/casperjs/home-page.js file. Let's see how CasperJS can be executed automatically on every Maven build. Here is the test scenario, implemented with a combination of Maven plugins: Install PhantomJS Install CasperJS Reserve a random TCP port Start Tomcat on that TCP port (with WAR inside) Run CasperJS tests and point them to the running Tomcat Shutdown Tomcat I'm using a combination of plugins. Let's go through the steps one by one. BTW, I'm not showing plugin versions in the examples below, primarily because most of them are in active development. Check their versions at Maven Central (yes, all of them are available there). 1. Install PhantomJS First of all, we have to download the PhantomJS executable. It is a platform-specific binary. Thanks to Kyle Lieber , we have an off-the-shelf Maven plugin: phantomjs-maven-plugin that understands what the current platform is and downloads the appropriate binary automatically, placing it into the target directory. <plugin> <groupId> com.github.klieber </groupId> <artifactId> phantomjs-maven-plugin </artifactId> <executions> <execution> <goals> <goal> install </goal> </goals> </execution> </executions> <configuration> <version> 1.9.2 </version> </configuration> </plugin> The exact name of the downloaded binary is stored in the ${phantomjs.binary} Maven property. 2. Install CasperJS Unfortunately, there is no similar plugin for the CasperJS installation (at least I haven't found any as of yet). That's why I'm using plain old git (you should have it installed on your build machine). <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> exec-maven-plugin </artifactId> <executions> <execution> <id> casperjs-install </id> <phase> pre-integration-test </phase> <goals> <goal> exec </goal> </goals> <configuration> <executable> git </executable> <arguments> <argument> clone </argument> <argument> --depth=1 </argument> <argument> https://github.com/n1k0/casperjs.git </argument> <argument> ${project.build.directory}/casperjs </argument> </arguments> </configuration> </execution> </executions> </plugin> 3. Reserve TCP Port I need to obtain a random TCP port where Tomcat will be started. The port has to be available on the build machine. I want to be able to run multiple Maven builds in parallel, so that's why I get a random port on every build. In other examples, you may see people using fixed port numbers, like 5555 or something similar. This is a very bad practice. Always reserve a new random port when you need it. <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> build-helper-maven-plugin </artifactId> <executions> <execution> <id> tomcat-port </id> <goals> <goal> reserve-network-port </goal> </goals> <configuration> <portNames> <portName> tomcat.port </portName> </portNames> </configuration> </execution> </executions> </plugin> The plugin reserves a port and sets it value to the ${tomcat.port} Maven property. 4. Start Tomcat Now, it's time to start Tomcat with the WAR package inside. I'm using tomcat7-maven-plugin that starts a real Tomcat7 server and configures it to serve on the port reserved above. <plugin> <groupId> org.apache.tomcat.maven </groupId> <artifactId> tomcat7-maven-plugin </artifactId> <configuration> <path> / </path> </configuration> <executions> <execution> <id> start-tomcat </id> <phase> pre-integration-test </phase> <goals> <goal> run-war-only </goal> </goals> <configuration> <port> ${tomcat.port} </port> <fork> true </fork> </configuration> </execution> </executions> </plugin> Due to the option fork being set to true , Tomcat7 continues to run when the plugin execution finishes. That's exactly what I need. 5. Run CasperJS Now, it's time to run CasperJS. Even though there are some plugins exist for this, I'm using plain old exec-maven-plugin , mostly because it is more configurable. <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> exec-maven-plugin </artifactId> <executions> <execution> <id> casperjs-test </id> <phase> integration-test </phase> <goals> <goal> exec </goal> </goals> <configuration> <executable> ${project.build.directory}/casperjs/bin/casperjs </executable> <workingDirectory> ${basedir} </workingDirectory> <arguments> <argument> test </argument> <argument> --verbose </argument> <argument> --no-colors </argument> <argument> --concise </argument> <argument> --home=http://localhost:${tomcat.port} </argument> <argument> ${basedir}/src/test/casperjs </argument> </arguments> <environmentVariables> <PHANTOMJS_EXECUTABLE> ${phantomjs.binary} </PHANTOMJS_EXECUTABLE> </environmentVariables> </configuration> </execution> </executions> </plugin> The environment variable PHANTOMJS_EXECUTABLE is the undocumented feature that makes this whole scenario possible. It configures the location of the PhantomJS executable, which was downloaded a few steps above. 6. Shutdown Tomcat In the last step, I shut down the Tomcat server. <plugin> <groupId> org.apache.tomcat.maven </groupId> <artifactId> tomcat7-maven-plugin </artifactId> <executions> <execution> <id> stop-tomcat </id> <phase> post-integration-test </phase> <goals> <goal> shutdown </goal> </goals> </execution> </executions> </plugin> Real Example If you want to see how this all works in action, take a look at stateful.co . It is a Java Web application hosted at CloudBees . Its source code is open and available in Github . Its pom.xml contains exactly the same configurations explained above, but joined together. If you have any questions, please don't hesitate to ask below. "},{"title":"Deploy Jekyll to Github Pages","url":"/2014/06/24/jekyll-github-deploy.html","tags":["jekyll","github","ruby"],"date":"2014-06-24 00:00:00 +0000","categories":[],"body":"This blog is written in Jekyll and is hosted at Github Pages . It uses half a dozen custom plugins, which are not allowed there . Here is how I deploy it: $ jgd That's it. jgd is my Ruby gem (stands for \"Jekyll Github Deploy\"), which does the trick. Here is what it does : It clones your existing repository from the current directory to a temporary one (guessing the URL of the repo from .git/config file). Runs jekyll build in that temporary directory, which saves the output in another temporary directory. Checks out gh-pages branch or creates one if it doesn't exist. Copies the content of the site built by jekyll build into the branch, thus overwriting existing files, commits and pushes to Github. Cleans up all temporary directories. Using this gem is very easy. Just install it with gem install jgd and then run in the root directory of your Jekyll blog. What is important is that your Jekyll site files be located in the root directory of the repository. Just as they do on this blog; see its sources in Github . You can easily integrate jgd with Travis. See .travis.yml of this blog. Full documentation about the gem is located here . "},{"title":"XML+XSLT in a Browser","url":"/2014/06/25/xml-and-xslt-in-browser.html","tags":["xslt","java","restful"],"date":"2014-06-25 00:00:00 +0000","categories":[],"body":"Separating data and their presentation is a great concept. Take HTML and CSS for example. HTML is supposed to have pure data and CSS is supposed to format that data in order to make it readable by a human. Years ago, that was probably the intention of HTML/CSS, but in reality it doesn't work like that. Mostly because CSS is not powerful enough. We still have to format our data using HTML tags, while CSS can help slightly with positioning and decorating. On the other hand, XML with XSLT implements perfectly the idea of separating data and presentation. XML documents, like HTML, are supposed to contain data only without any information about positioning or formatting. XSL stylesheets position and decorate the data. XSL is a much more powerful language. That's why it's possible to avoid any formatting inside XML. The latest versions of Chrome, Safari, FireFox and IE all support this mechanism. When a browser retrieves an XML document from a server, and the document has an XSL stylesheet associated with it — the browser transforms XML into HTML on-fly. Working Example Let's review a simple Java web application that works this way. It is using ReXSL framework that makes this mechanism possible. In the next post, I'll explain how ReXSL works. For now, though, let's focus on the idea of delivering bare data in XML and formatting it with an XSL stylesheet. Open http://www.stateful.co — it is a collection of stateful web primitives, explained in the Atomic Counters at Stateful.co article. Open it in Chrome or Safari. When you do, you should see a normal web page with a logo, some text, some links, a footer, etc. Now check its sources (I assume you know how to do this). This is approximately what you will see (I assume you understand XML, if not, start learning it immediately): <?xml-stylesheet type='text/xsl' href='/xsl/index.xsl'?> <page date= \"2014-06-15T15:30:49.521Z\" ip= \"10.168.29.135\" > <menu> home </menu> <documentation> .. some text here .. </documentation> <version> <name> 1.4 </name> <revision> 5c7b5af </revision> <date> 2014-05-29 07:58 </date> </version> <links> <link href= \"...\" rel= \"rexsl:google\" type= \"text/xml\" /> <link href= \"...\" rel= \"rexsl:github\" type= \"text/xml\" /> <link href= \"...\" rel= \"rexsl:facebook\" type= \"text/xml\" /> </links> <millis> 70 </millis> </page> As you see, it is a proper XML document with attributes, elements and data. It contains absolutely no information about how its elements have to be presented to an end-user. Actually, this document is more suitable for machine parsing instead of reading by a human. The document contains data, which is important for its requestor. It's up to the requestor on how to render the data or to not render it at all. Its second line associates the document with the XSL stylesheet /xsl/index.xsl that is loaded by the browser separately: <?xml-stylesheet type='text/xsl' href='/xsl/index.xsl'?> Open developer tools in Chrome and you will see that right after the page is loaded, the browser loads the XSL stylesheet and then all other resources including a few CSS stylesheets, jQuery and an SVG logo: index.xsl includes layout.xsl , that's why it is loaded right after. Let's consider an example of index.xsl (in reality it is much more complex, check layout.xsl . For example: <xsl:stylesheet version= \"2.0\" xmlns:xsl= \"http://www.w3.org/1999/XSL/Transform\" xmlns= \"http://www.w3.org/1999/xhtml\" > <xsl:template match= \"page\" > <html> <body> <p> Current version of the application is <xsl:value-of select= \"version/name\" /> </p> </body> </html> </xsl:template> </xsl:stylesheet> I think it's obvious how the HTML page will look like after applying this XSL stylesheet to our XML document. For me, this XSL looks clean and easy to understand. However, I often hear people say that XSLT is a hard-to-understand programming language. I don't find it hard to understand at all. Of course, I'm not using all of its features. But, for simple page rendering, all I need to know are a few simple commands and the principle of XML transformation. Why Not a Templating Engine? Now, why is this approach better than all that widely use Java templating engines, including JSP , JSF , Velocity , FreeMarker , Tiles , etc? Well, I see a number of reasons. But, the most important are: Web UI and API are same pages . There is no need to develop separate pages for RESTful API — Web user interface, being accessed by a computer, is an API. In my experience, this leads to massive avoidance of code duplication. XSL is testable by itself without a server . In order to test how our web site will look with certain data, we just create a new XML document with necessary test data, associate it with an XSL and open it in a browser. We can also modify XML and refresh the page in browser. This makes the work of HTML/CSS designer much easier and independent of programmers. XSL is a powerful functional language . Compared with all other templating engines, which look mostly like workarounds, XSL is a complete and well-designed environment. Writing XSL (after you get used to its syntax and programming concepts) is a pleasure in itself. You're not injecting instructions into a HTML document (like in JSP and all others). Instead, you are programming transformation of data into presentation — a different mindset and much better feeling. XML output is perfectly testable . A controller in MVC that generates an XML document with all data required for the XSL stylesheet can easily be tested in a single unit test using simple XPath expressions. Testing of a controller that injects data into a templating engine is a much more complex operation — even impossible sometimes. I'm also writing in PHP and Ruby. They have exactly the same problems — even though their templating engines are much more powerful due to the interpretation nature of the languages. Is It Fully Supported? Everything would be great if all browsers would support XML+XSL rendering. However, this is far from being true. Only the latest versions of modern browsers support XSL. Check this comparison done by Julian Reschke. Besides that, XSLT 2.0 is not supported at all. There is a workaround, though. We can understand which browser is making a request (via its User-Agent HTTP header) and transform XML into HTML on the server side. Thus, for modern browsers that support XSL, we will deliver XML and for all others — HTML. This is exactly how ReXSL framework works. Open http://www.stateful.co in Internet Explorer and you will see an HTML document, not an XML document as is the case with Chrome. In one of the next posts, I'll explain ReXSL framework . Read this one, it continues the discussion of this subject: RESTful API and a Web Site in the Same URL "},{"title":"SASS in Java Webapp","url":"/2014/06/26/sass-in-java-webapp.html","tags":["java","sass"],"date":"2014-06-26 00:00:00 +0000","categories":[],"body":"SASS is a powerful and very popular language for writing CSS style sheets. This is how I'm using SASS in my Maven projects. First, I change the extensions of .css files to .scss and move them from src/main/webapp/css to src/main/scss . Then, I configure the sass-maven-plugin (get its latest versions in Maven Central ): <plugin> <groupId> nl.geodienstencentrum.maven </groupId> <artifactId> sass-maven-plugin </artifactId> <executions> <execution> <id> generate-css </id> <phase> generate-resources </phase> <goals> <goal> update-stylesheets </goal> </goals> <configuration> <sassSourceDirectory> ${basedir}/src/main/scss </sassSourceDirectory> <destination> ${project.build.directory}/css </destination> </configuration> </execution> </executions> </plugin> The SASS compiler will compile .scss files from src/main/scss and place .css files into target/css . Then, I configure the minify-maven-plugin to compress/minify the style sheets produced by the SASS compiler: <plugin> <groupId> com.samaxes.maven </groupId> <artifactId> minify-maven-plugin </artifactId> <configuration> <charset> UTF-8 </charset> <nosuffix> true </nosuffix> <webappTargetDir> ${project.build.directory}/css-min </webappTargetDir> </configuration> <executions> <execution> <id> minify-css </id> <goals> <goal> minify </goal> </goals> <configuration> <webappSourceDir> ${project.build.directory} </webappSourceDir> <cssSourceDir> css </cssSourceDir> <cssSourceIncludes> <include> *.css </include> </cssSourceIncludes> <skipMerge> true </skipMerge> </configuration> </execution> </executions> </plugin> Minified .css files will be placed into target/css-min . The final step is to configure the maven-war-plugin to pick up .css files and package them into the final WAR archive: <plugin> <artifactId> maven-war-plugin </artifactId> <configuration> [..other configuration options..] <webResources combine.children= \"append\" > <resource> <directory> ${project.build.directory}/css-min </directory> </resource> </webResources> </configuration> </plugin> That's it. "},{"title":"Custom Pygments Lexer in Jekyll","url":"/2014/06/29/custom-lexer-in-jekyll.html","tags":["jekyll","pygments"],"date":"2014-06-29 00:00:00 +0000","categories":[],"body":"I needed to create a custom syntax highlighting for requs.org on which I'm using Jekyll for site rendering. This is how my code blocks look in markdown pages: { % highlight requs %} User is a \"human being\". { % endhighlight %} I created a custom Pygments lexer : from pygments.lexer import RegexLexer from pygments.token import Punctuation , Text , Keyword , Name , String from pygments.util import shebang_matches class RequsLexer ( RegexLexer ): name = 'requs' aliases = [ 'requs' ] tokens = { 'root' : [ ( r'\"[^\"]+\"' , String ), ( r'\"\"\".+\"\"\"' , Text ), ( r'\\b(needs|includes|requires|when|fail|since|must|is|a|the)\\s*\\b' , Keyword ), ( r'([A-Z][a-z]+)+' , Name ), ( r'[,;:]' , Punctuation ), ], } def analyse_text ( text ): return shebang_matches ( text , r'requs' ) Then, I packaged it for easy_install and installed locally: $ easy_install src/requs_pygment Processing requs_pygment Running setup.py -q bdist_egg --dist-dir /Volumes/ssd2/code/requs/src/requs_pygment/egg-dist-tmp-ISj8Nx zip_safe flag not set ; analyzing archive contents... Adding requs-pygment 0.1 to easy-install.pth file Installed /Library/Python/2.7/site-packages/requs_pygment-0.1-py2.7.egg Processing dependencies for requs-pygment == 0.1 Finished processing dependencies for requs-pygment == 0.1 It's done. Now I run jekyll build and my syntax is highlighted according to the custom rules I specified in the lexer. "},{"title":"How to Read MANIFEST.MF Files","url":"/2014/07/03/how-to-read-manifest-mf.html","tags":["java","jcabi"],"date":"2014-07-03 00:00:00 +0000","categories":[],"body":" Every Java package (JAR, WAR, EAR, etc.) has a MANIFEST.MF file in the META-INF directory. The file contains a list of attributes, which describe this particular package. For example: Manifest-Version: 1.0 Created-By: 1.7.0_06 (Oracle Corporation) Main-Class: MyPackage.MyClass When your application has multiple JAR dependencies, you have multiple MANIFEST.MF files in your class path. All of them have the same location: META-INF/MANIFEST.MF . Very often it is necessary to go through all of them in runtime and find the attribute by its name. jcabi-manifests makes it possible with a one-liner: import com.jcabi.manifests.Manifests ; String created = Manifests . read ( \"Created-By\" ); Let's see why you would want to read attributes from manifest files, and how it works on a low level. Package Versioning When you package a library or even a web application, it is a good practice to add an attribute to its MANIFEST.MF with the package version name and build number. In Maven, maven-jar-plugin can help you (almost the same configuration for maven-war-plugin ): <plugin> <artifactId> maven-jar-plugin </artifactId> <configuration> <archive> <manifestEntries> <Foo-Version> ${project.version} </Foo-Version> <Foo-Hash> ${buildNumber} </Foo-Hash> </manifestEntries> </archive> </configuration> </plugin> buildnumber-maven-plugin will help you to get ${buildNumber} from Git, SVN or Mercurial: <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> buildnumber-maven-plugin </artifactId> <executions> <execution> <goals> <goal> create </goal> </goals> </execution> </executions> </plugin> After all these manipulations, MANIFEST.MF , in your JAR will contain these two extra lines (on top of all others added there by Maven by default): Foo-Version: 1.0-SNAPSHOT Foo-Hash: 7ef4ac3 In runtime, you can show these values to the user to help him understand which version of the product he is working with at any given moment. Look at stateful.co , for example. At the bottom of its front page, you see the version number and Git hash. They are retrieved from MANIFEST.MF of the deployed WAR package, on every page click. Credentials Although this may be considered as a bad practice (see Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation by Jez Humble and David Farley), sometimes it is convenient to package production credentials right into the JAR/WAR archive during the continuous integration/delivery cycle. For example, you can encode your PostgreSQL connection details right into MANIFEST.MF : <plugin> <artifactId> maven-war-plugin </artifactId> <configuration> <archive> <manifestEntries> <Pgsql> jdbc:postgresql://${pg.host}:${pg.port}/${pg.db} </Pgsql> </manifestEntries> </archive> </configuration> </plugin> Afterwards, you can retrieve them in runtime using jcabi-manifests : String url = Manifests . read ( \"Pgsql\" ); If you know of any other useful purposes for MANIFEST.MF , let me know :) "},{"title":"Liquibase with Maven","url":"/2014/07/20/liquibase-in-maven.html","tags":["liquibase","maven","java"],"date":"2014-07-20 00:00:00 +0000","categories":[],"body":"Liquibase is a migration management tool for relational databases. It versionalizes schema and data changes in a database; similar to the way Git or SVN works for source code. Thanks to their Maven plugin , Liquibase can be used as a part of a build automation scenario. Maven Plugin Let's assume you're using MySQL (PostgreSQL or any other database configuration will be very similar.) Add liquibase-maven-plugin to your pom.xml (get its latest version in Maven Central ): <project> [...] <build> [...] <plugins> <plugin> <groupId> org.liquibase </groupId> <artifactId> liquibase-maven-plugin </artifactId> <configuration> <changeLogFile> ${basedir}/src/main/liquibase/master.xml </changeLogFile> <driver> com.mysql.jdbc.Driver </driver> <url> jdbc:mysql://${mysql.host}:${mysql.port}/${mysql.db} </url> <username> ${mysql.login} </username> <password> ${mysql.password} </password> </configuration> </plugin> </plugins> </build> </project> To check that it works, run mvn liquibase:help . I would recommend you keep database credentials in settings.xml and in their respective profiles. For example: <settings> <profiles> <profile> <id> production </id> <properties> <mysql.host> db.example.com </mysql.host> <mysql.port> 3306 </mysql.port> <mysql.db> example </mysql.db> </properties> </profile> <profile> <id> test </id> <properties> <mysql.host> test-db.example.com </mysql.host> <mysql.port> 3306 </mysql.port> <mysql.db> example-db </mysql.db> </properties> </profile> </profiles> </settings> When you run Maven, don't forget to turn on one of the profiles. For example: mvn -Pproduction . Initial Schema I assume you already have a database with a schema (tables, triggers, views, etc.) and some data. You should \"reverse engineer\" it and create an initial schema file for Liquibase. In other words, we should inform Liquibase where we are at the moment, so that it starts to apply changes from this point. Maven plugin doesn't support it, so you will have to run Liquibase directly. But, it's not that difficult. First, run mvn liquibase:help in order to download all artifacts. Then, replace placeholders with your actual credentials: $ java -jar ~/.m2/repository/org/liquibase/liquibase-core/3.1.1/liquibase-core-3.1.1.jar \\ --driver = com.mysql.jdbc.Driver \\ --url = jdbc:mysql://db.example.com:3306/example \\ --username = example --password = example \\ generateChangeLog > src/main/liquibase/2014/000-initial-schema.xml Liquibase will analyze your current database schema and copy its own schema into src/main/liquibase/2014/000-initial-schema.xml . Master Changeset Now, create XML master changeset and save it to src/main/liquibase/master.xml : <databaseChangeLog xmlns= \"http://www.liquibase.org/xml/ns/dbchangelog\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-2.0.xsd\" > <includeAll path= \"src/main/liquibase/2014\" /> </databaseChangeLog> It is an entry point for Liquibase. It starts from this file and loads all other changesets available in src/main/liquibase/2014 . They should be either .xml or .sql . I recommend that you use XML mostly because it is easier to maintain and works faster. Incremental Changesets Let's create a simple changeset, which adds a new column to an existing table: <databaseChangeLog xmlns= 'http://www.liquibase.org/xml/ns/dbchangelog' xmlns:xsi= 'http://www.w3.org/2001/XMLSchema-instance' xsi:schemaLocation= 'http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-2.0.xsd' > <changeSet id= \"002\" author= \"Yegor\" > <sql> ALTER TABLE user ADD COLUMN address VARCHAR(1024); </sql> </changeSet> </databaseChangeLog> We save this file we in src/main/liquibase/2014/002-add-user-address.xml . In big projects, you can name your files by the names of the tickets they are produced in. For example, 045-3432.xml , which means changeset number 45 coming from ticket #3432. The important thing is to have this numeric prefix in front of file names, in order to sort them correctly. We want changes to be applied in their correct chronological order. That's it. We're ready to run mvn liquibase:update -Pproduction and our production database will be updated — a new column will be added to the user table. Also, see how MySQL Maven Plugin can help you to automate integration testing of database-connected classes. "},{"title":"Master Branch Must Be Read-Only","url":"/2014/07/21/read-only-master-branch.html","tags":["rultor","devops","mgmt"],"date":"2014-07-21 00:00:00 +0000","categories":["best"],"body":"Continuous integration is easy. Download Jenkins, install, create a job, click the button, and get a nice email saying that your build is broken (I assume your build is automated). Then, fix broken tests (I assume you have tests), and get a much better looking email saying that your build is clean. Then, tweet about it, claiming that your team is using continuous integration. Then, in a few weeks, start filtering out Jenkins alerts, into their own folder, so that they don't bother you anymore. Anyway, your team doesn't have the time or desire to fix all unit tests every time someone breaks them. After all, we all know that unit testing is not for a team working with deadlines, right? Wrong. Continuous integration can and must work. What is Continuous Integration? Nowadays, software development is done in teams. We develop in feature branches and isolate changes while they are in development. Then, we merge branches into master . After every merge, we test the entire product, executing all available unit and integration tests. This is called continuous integration (aka \"CI\"). Sometimes, some tests fail. When this happens, we say that our \"build is broken\". Such a failure is a positive side effect of quality control because it raises a red flag immediately after an error gets into master . It is a well-known practice, when fixing that error becomes a top priority for its author and the entire team. The error should be fixed right after a red flag is raised by the continuous integration server. Continuous Delivery by Jez Humble et. al. explains this approach perfectly in Chapter 7, pages 169–186. There are a few good tools on the market, which automate DevOps procedures. Some of them are open source, you can download and install them on your own servers. For example: Jenkins , Go , and CruiseControl . Some of them are available as a service in cloud, such as: Travis , Drone , Wercker , and many others. Why Continuous Integration Doesn't Work? CI is great, but the bigger the team (and the code base), the more often builds get broken. And, the longer it takes to fix them. I've seen many examples where a hard working team starts to ignore red flags, raised by Jenkins, after a few weeks or trying to keep up. The team simply becomes incapable of fixing all errors in time. Mostly because the business has other priorities. Product owners do not understand the importance of a \"clean build\" and technical leaders can't buy time for fixing unit tests. Moreover, the code that broke them was already in master and, in most cases, has been already deployed to production and delivered to end-users. What's the urgency of fixing some tests if business value was already delivered? In the end, most development teams don't take continuous integration alerts seriously. Jenkins or Travis are just fancy tools for them that play no role in the entire development and delivery pipeline. No matter what continuous integration server says, we still deliver new features to our end-users. We'll fix our build later. And it's only logical . What Is a Solution? Four years ago, in 2010, I published an article in php|Architect called \"Prevent Conflicts in Distributed Agile PHP Projects\". In the article, a solution was proposed (full article in PDF ) for Subversion and PHP. Since that time, I used experimentally that approach in multiple open source projects and a few commercial ones with PHP, Java, Ruby and JavaScript, Git and Subversion. In all cases, my experience was only positive, and that's why rultor.com was born (later about that though). So, the solution is simple — prohibit anyone from merging anything into master and create a script that anyone can call. The script will merge, test, and commit. The script will not make any exceptions. If any branch is breaking at even one unit test, the entire branch will be rejected. In other words, we should raise that red flag before the code gets into master . We should put the blame for broken tests on the shoulders of its author. Say, I'm developing a feature in my own branch. I finished the development and broke a few tests, accidentally. It happens, we all make mistakes. I can't merge my changes into master . Git simply rejects my push , because I don't have the appropriate permissions. All I can do is call a magic script, asking it to merge my branch. The script will try to merge, but before pushing into master , it will run all tests. And if any of them break, my branch will be rejected. My changes won't be merged. Now it's my responsibility — to fix them and call the script again. In the beginning, this approach slows down the development, because everybody has to start writing cleaner code. At the end, though, this method pays off big time. Pre-flight Builds Some CI servers offer pre-flight builds feature, which means testing branches before they get merged into master . Travis, for example, has this feature and it is very helpful. When you make a new commit to a branch, Travis immediately tries to build it, and reports in Github pull request, if there are problems. Pay attention, pre-flight builds don't merge. They just check whether your individual branch is clean. After merge, it can easily break master . And, of course, this mechanism doesn't guarantee that no collaborators can commit directly to master , breaking it accidentally. Pre-flight builds are a preventive measure, but do not solve the problem entirely. Rultor.com In order to start working as explained above, all you have to do is to revoke write permissions to master branch (or /trunk , in Subversion). Unfortunately, this is not possible in Github. The only solution is to work through forks and pull requests only. Simply remove everybody from the list of \"collaborators\" and they will have to submit changes through pull requests. Then, start using Rultor.com , which will help you to test, merge and push every pull request. Basically, Rultor is the script we were talking about above. It is available as a free cloud service. ps. A short version of this article is also published at devops.com "},{"title":"Rultor.com, a Merging Bot","url":"/2014/07/24/rultor-automated-merging.html","tags":["rultor","devops"],"date":"2014-07-24 00:00:00 +0000","categories":[],"body":" You get a Github pull request. You review it. It looks correct — it's time to merge it into master . You post a comment in it, asking @rultor to test and merge. Rultor starts a new Docker container, merges the pull request into master , runs all tests and, if everything looks clean — merges, pushes, and closes the request. Then, you ask @rultor to deploy the current version to production environment. It checks out your repository, starts a new Docker container, executes your deployment scripts and reports to you right there in the Github issue. Why not Jenkins or Travis? There are many tools on the market, which automate continuous integration and continuous delivery (let's call them DevOps). For example, downloadable open-source Jenkins and hosted Travis both perform these tasks. So, why do we need one more? Well, there are three very important features that we need for our projects, but we can't find all of them in any of the DevOps tools currently available on the market: Merging . We make master branch read-only in our projects, as this article recommends. All changes into master we pass through a script that validates them and merges. Docker . Every build should work in its own Docker container, in order to simplify configuration, isolate resources and make errors easily reproduceable. Tell vs. Trigger . We need to communicate with DevOps tool through commands, right from our issue tracking system (Github issues, in most projects). All existing DevOps systems trigger builds on certain conditions. We need our developers to be able to talk to the tool, through human-like commands in the tickets they are working with. A combination of these three features is what differs Rultor from all other existing systems. How Rultor Merges Once Rultor finds a merge command in one of your Github pull requests, it does exactly this: Reads the .rultor.yml YAML config file from the root directory of your repository. Gets automated build execution command from it, for example bundle test . Checks out your repository into a temporary directory on one of its servers. Merges pull request into master branch. Starts a new Docker container and runs bundle test in it. If everything is fine, pushes modified master branch to Github. Reports back to you, in the Github pull request. You can see it in action, for example, in this pull request: jcabi/jcabi-github#878 . "},{"title":"Every Build in Its Own Docker Container","url":"/2014/07/29/docker-in-rultor.html","tags":["docker","rultor","devops"],"date":"2014-07-29 00:00:00 +0000","categories":[],"body":" Docker is a command line tool that can run a shell command in a virtual Linux, inside an isolated file system. Every time we build our projects, we want them to run in their own Docker containers. Take this Maven project for example: $ sudo docker run -i -t ubuntu mvn clean test This command will start a new Ubuntu system and execute mvn clean test inside it. Rultor.com , our virtual assistant, does exactly that with our builds, when we deploy, package, test and merge them. Why Docker? What benefits does it give us? And why Docker, when there are many other virtualization technologies , like LXC, for example? Well, there are a few very important benefits: Image repository (hub.docker.com) Versioning Application-centric Let's discuss them in details. Image Repository Docker enables image sharing through its public repository at hub.docker.com . This means that after I prepare a working environment for my application, I make an image out of it and push it to the hub. Let's say, I want my Maven build to be executed in a container with a pre-installed graphviz package (in order to enable dot command line tool). First, I would start a plain vanilla Ubuntu container, and install graphviz inside it: $ sudo docker run -i -t ubuntu /bin/bash root@215d2696e8ad:/# sudo apt-get install -y graphviz Reading package lists... Done Building dependency tree Reading state information... Done The following extra packages will be installed: ... root@215d2696e8ad:/# exit $ sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 215d2696e8ad ubuntu:14.04 /bin/bash About a minute ago Exited ( 0 ) 3 seconds ago high_mccarthy I have a container that stopped a few seconds ago. Container's ID is 215d2696e8ad . Now, I want to make it reusable for all further tests in Rultor.com. I have to create an image from it: $ sudo docker commit 215d2696e8ad yegor256/beta c5ad7718fc0e20fe4bf2c8a9bfade4db8617a25366ca5b64be2e1e8aa0de6e52 I just made my new commit to a new image yegor256/beta . This image can be reused right now. I can create a new container from this image and it will have graphviz installed inside! Now it's time to share my image at Docker hub, in order to make it available for Rultor: $ sudo docker push yegor256/beta The push refers to a repository [ yegor256/beta ] ( len: 1 ) Sending image list Pushing repository yegor256/beta ( 1 tags ) 511136ea3c5a: Image already pushed, skipping d7ac5e4f1812: Image already pushed, skipping 2f4b4d6a4a06: Image already pushed, skipping 83ff768040a0: Image already pushed, skipping 6c37f792ddac: Image already pushed, skipping e54ca5efa2e9: Image already pushed, skipping c5ad7718fc0e: Image successfully pushed Pushing tag for rev [ c5ad7718fc0e ] on { https://registry-1.docker.io/v1/repositories/yegor256/beta/tags/latest } The last step is to configure Rultor to use this image in all builds. To do this, I will edit .rultor.yml in the root directory of my Github repository: docker : image : yegor256/beta That's it. From now on, Rultor will use my custom Docker image with pre-installed graphviz, in every build (merge, release, deploy, etc.) Moreover, if and when I want to add something else to the image, it's easy to do. Say, I want to install Ruby into my build image. I start a container from the image and install it (pay attention, I'm starting a container not from ubuntu image, as I did before, but from yegor256/beta ): $ sudo docker run -i -t yegor256/beta /bin/bash root@7e0fbd9806c9:/# sudo apt-get install -y ruby Reading package lists... Done Building dependency tree Reading state information... Done The following extra packages will be installed: ... root@7e0fbd9806c9:/# exit $ sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e0fbd9806c9 yegor256/beta:latest /bin/bash 28 seconds ago Exited ( 0 ) 2 seconds ago pensive_pare 215d2696e8ad ubuntu:14.04 /bin/bash 10 minutes ago Exited ( 0 ) 8 minutes ago high_mccarthy You can now see that I have two containers. The first one is the one I am using right now; it contains Ruby. The second one is the one I was using before and it contains graphviz. Now I have to commit again and push: $ sudo docker commit 7e0fbd9806c9 yegor256/beta 6cbfb7a6b18a2182f42171f6bb5aef67c4819b5c2795edffa6a63ba78aaada2d $ sudo docker push yegor256/beta ... Thus, this Docker hub is a very convenient feature for Rultor and similar systems. Versioning As you saw in the example above, every change to a Docker image has its own version (hash) and it's possible to track changes. It is also possible to roll back to any particular change. Rultor is not using this functionality itself, but Rultor users are able to control their build configurations with much better precision. Application-Centric Docker, unlike LXC or Vagrant, for example, is application-centric. This means that when we start a container — we start an application. With other virtualization technologies, when you get a virtual machine — you get a fully functional Unix environment, where you can login through SSH and do whatever you want. Docker makes things simpler. It doesn't give you SSH access to container, but runs an application inside and shows you its output. This is exactly what we need in Rultor. We need to run an automated build (for example Maven or Bundler), see its output and get its exit code. If the code is not zero, we fail the build and report to the user. This is how we run Maven build: $ sudo docker run --rm -i -t yegor256/rultor mvn clean test [ INFO ] ------------------------------------------------------------------------ [ INFO ] Building jcabi-github 0.13 [ INFO ] ------------------------------------------------------------------------ [ INFO ] [ INFO ] --- maven-clean-plugin:2.5:clean ( default-clean ) @ jcabi-github --- [ INFO ] ... As you can see, Maven starts immediately. We don't worry about the internals of the container. We just start an application inside it. Furthermore, thanks to the --rm option, the container gets destroyed immediately after Maven execution is finished. This is what application-centric is about. Our overall impression of Docker is highly positive. ps. A compact version of this article was published at devops.com "},{"title":"Rultor + Travis","url":"/2014/07/31/travis-and-rultor.html","tags":["docker","rultor","devops"],"date":"2014-07-31 00:00:00 +0000","categories":[],"body":" Rultor is a coding team assistant. Travis is a hosted continuous integration system. In this article I'll show how our open source projects are using them in tandem to achieve seamless continuous delivery. I'll show a few practical scenarios. Scenario #1: Merge Pull Request jcabi-mysql-maven-plugin is a Maven plugin for MySQL integration testing . @ChristianRedl submitted pull request #35 with a new feature. I reviewed the request and asked Rultor to merge it into master : As you can see, an actual merge operation was made by Rultor. I gave him access to the project by adding his Github account to the list of project collaborators. Before giving a \"go ahead\" to Rultor I checked the status of the pre-build reported by Travis: Travis found a new commit in the pull request and immediately (without any interaction from my side) triggered a build in that branch. The build didn't fail, that's why Travis gave me a green sign. I looked at that sign and at the code. Since all problems in the code were corrected by the pull request author and Travis didn't complain — I gave a \"go\" to Rultor. Scenario #2: Continuous Integration Even though the previous step guarantees that master branch is always clean and stable, we're using Travis to continuously integrate it. Every commit made to master triggers a new build in Travis. The result of the build changes the status of the project in Travis: either \"failing\" or \"passing\". jcabi-aspects is a collection of AOP AspectJ aspects . We configured Travis to build it continuously. This is the badge it produces (the left one): Again, let me stress that even through read-only master is a strong protection against broken builds, it doesn't guarantee that at any moment master is stable. For example, sometimes unit tests fail sporadically due to changes in calendar, in environment, in dependencies, in network connection qualities, etc. Well, ideally, unit tests should either fail or pass because they are environment independent. However, in reality, unit tests are far from being ideal. That's why a combination of read-only master with Rultor and continuous integration with Travis gives us higher stability. Scenario #3: Release to RubyGems jekyll-github-deploy is a Ruby gem that automates deployment of Jekyll sites to Github Pages . @leucos submitted a pull request #4 with a new feature. The request was merged successfully into master branch. Then, Rultor was instructed by myself that master branch should be released to RubyGems and a new version to set is 1.5: Rultor executed a simple script, pre-configured in its .rultor.yml : release : script : | ./test.sh rm -rf *.gem sed -i \"s/2.0-SNAPSHOT/${tag}/g\" jgd.gemspec gem build jgd.gemspec chmod 0600 ../rubygems.yml gem push *.gem --config-file ../rubygems.yml The script is parameterized, as you see. There is one parameter that is passed by Rultor into the script: ${tag} . This parameter was provided by myself in the Github issue, when I submitted a command to Rultor. The script tests that the gem works (integration testing) and clean up afterwords: $ ./test.sh $ rm -rf *.gem Then, it changes the version of itself in jgd.gemspec to the one provided in the ${tag} (it is an environment variable), and builds a new .gem : $ sed -i \"s/2.0-SNAPSHOT/${tag}/g\" jgd.gemspec $ gem build jgd.gemspec Finally, it pushes a newly built .gem to RubyGems, using login credentials from ../rubygems.yml . This file is created by Rultor right before starting the script (this mechanism is discussed below): $ chmod 0600 ../rubygems.yml $ gem push *.gem --config-file ../rubygems.yml If everything works fine and RubyGems confirms successful deployment, Rultor reports to Github. This is exactly what happened in pull request #4 . Scenario #4: Deploy to CloudBees s3auth.com is a Basic HTTP authentication gateway for Amazon S3 Buckets . It is a Java web app. In its pull request #195 , a resource leakage problem was fixed by @carlosmiranda and the pull request was merged by Rultor. Then, @davvd instructed Rultor to deploy master branch to production environment. Rultor created a new Docker container and ran mvn clean deploy in it. Maven deployed the application to CloudBees : The overall procedure took 21 minutes, as you see the in the report generated by Rultor. There is one important trick worth mentioning. Deployment to production always means using secure credentials, like login, password, SSH keys, etc. In this particular example, Maven CloudBees Plugin needed API key, secret and web application name. These three parameters are kept secure and can't be revealed in an \"open source\" way. So, there is a mechanism that configures Rultor accordingly through its .rultor.yml file (pay attention to the first few lines): assets : settings.xml : \"yegor256/home#assets/s3auth/settings.xml\" pubring.gpg : \"yegor256/home#assets/pubring.gpg\" secring.gpg : \"yegor256/home#assets/secring.gpg\" These YAML entries inform Rultor that it has to get assets/s3auth/settings.xml file from yegor256/home private (!) Github repository and put it into the working directory of Docker container, right before starting the Maven build. This settings.xml file contains that secret data CloudBees plugin needs in order to deploy the application. How to Deploy to CloudBees, in One Click explains this process even better. You Can Do The Same Both Rultor and Travis are free hosted products, provided your projects are open source and hosted at Github. Other good examples of Rultor+Travis usage can be seen in these Github issues: jcabi/jcabi-http#47 , jcabi/jcabi-http#48 "},{"title":"Cache Java Method Results","url":"/2014/08/03/cacheable-java-annotation.html","tags":["java","jcabi","aop"],"date":"2014-08-03 00:00:00 +0000","categories":[],"body":" Say, you have a method that takes time to execute and you want its result to be cached. There are many solutions , including Apache Commons JCS , Ehcache , JSR 107 , Guava Caching and many others. jcabi-aspects offers a very simple one, based on AOP aspects and Java6 annotations: import com.jcabi.aspects.Cacheable ; public class Page { @Cacheable ( lifetime = 5 , unit = TimeUnit . MINUTES ) String load () { return new URL ( \"http://google.com\" ). getContent (). toString (); } } The result of load() method will be cached in memory for five minutes. How It Works? This post about AOP, AspectJ and method loging explains how \"aspect weaving\" works (I highly recommend that you read it first). Here I'll explain how caching works. The approach is very straight forward. There is a static hash map with keys as \"method coordinates\" and values as their results. Method coordinates consist of the object, an owner of the method and a method name with parameter types. In the example above, right after the method load() finishes, the map gets a new entry (simplified example, of course): key: [page, \"load()\"] value: \"<html>...</html>\" Every consecutive call to load() will be intercepted by the aspect from jcabi-aspects and resolved immediately with a value from the cache map. The method will not get any control until the end of its lifetime, which is five minutes in the example above. What About Cache Flushing? Sometimes it's necessary to have the ability to flush cache before the end of its lifetime. Here is a practical example: import com.jcabi.aspects.Cacheable ; public class Employees { @Cacheable ( lifetime = 1 , unit = TimeUnit . HOURS ) int size () { // calculate their amount in MySQL } @Cacheable.FlushBefore void add ( Employee employee ) { // add a new one to MySQL } } It's obvious that the number of employees in the database will be different after add() method execution and the result of size() should be invalidated in cache. This invalidation operation is called \"flushing\" and @Cacheable.FlushBefore triggers it. Actually, every call to add() invalidates all cached methods in this class, not only size() . There is also @Cacheable.FlushAfter . The difference is that FlushBefore guarantees that cache is already invalidated when the method add() starts. FlushAfter invalidates cache after method add() finishes. This small difference makes a big one, sometimes. This article explains how to add jcabi-aspects to your project . "},{"title":"Strict Control of Java Code Quality","url":"/2014/08/13/strict-code-quality-control.html","tags":["java","qulice","static-analysis"],"date":"2014-08-13 00:00:00 +0000","categories":["best"],"body":"There are many tools that control the quality of Java code, including Checkstyle , PMD , FindBugs , Cobertura , etc. All of them are usually used to analyze quality and build some fancy reports. Very often, those reports are published by continuous integration servers, like Jenkins. Qulice takes things one step further. It aggregates a few quality checkers, configures them to a maximum strict mode, and breaks your build if any of them fail. Seriously. There are over 130 checks in Checkstyle, over 400 rules in PMD, and over 400 bugs in FindBugs. All of them should say: \"Yes, we like your code\". Otherwise, your build shouldn't pass. What do you think? Would it be convenient for you — to have your code rejected every time it breaks just one of 900 checks? Would it be productive for the team — to force developers to focus so much on code quality? First Reaction If you join one of our teams as a Java developer, you will develop your code in branches and, then, Rultor will merge your changes into master . Before actually merging, though, Rultor will run an automated build script to make sure that your branch doesn't break it. As a static analysis tool, Qulice is just one of the steps in the automated build script. It is actually a Maven plugin and we automate Java builds with Maven 3x. Thus, if your changes break any of Qulice's rules, your entire branch gets rejected. Your first reaction - I've seen it hundreds of times - will be negative. You may actually become frustrated enough to leave the project immediately. You may say something like this (I'm quoting real life stories): \"These quality rules entirely ruin my creativity!\" \"Instead of wasting time on these misplaced commas and braces, we'd be better off developing new features!\" \"I've done many successful projects in my life, never heard about this ridiculous quality checking...\" This first reaction is only logical. I've seen many people say things like this, in both open source and commercial projects. Not only in Java, but also in PHP (with phpcs and phpmd ) and Ruby (with rubocop and simplecov ). How do I answer? Read on. On Second Thought My experience tells me that the sooner someone can get used to the strict quality control of Qulice, the faster he/she can learn and grow; the better programmer he/she is; and the further he/she can go with us and our projects. Having this experience in mind, I recommend that all new project members be patient and try to get used to this new approach to quality. In a few weeks, those who stick with it start to understand why this approach is good for the project and for them, as Java engineers. Ratatouille (2007) by Brad Bird and Jan Pinkava Why is it good? Read on. What Do Projects Get From Qulice? Let's take one simple rule as an example. Here is a piece of Java code that Qulice would complain about (due to the DesignForExtension rule from Checkstyle): public class Employee { public String name () { return \"Jeff\" ; } } What is wrong with this code? Method name() is not final and can be overridden by a class that extends Employee . Design-wise this is wrong, since a child class is allowed to break a super class, overriding its method. What is the right design? This one: public class Employee { public final String name () { return \"Jeff\" ; } } Now, the method is final and can't be overriden by child classes. It is a much safer design (according to Checkstyle, and I agree). So, let's say we make this rule mandatory for all classes in the project. What does the project gain from this? It can promise its members (programmers) a higher quality of work, compared to other projects that don't have this restriction, mostly because of: Predictability of Design — I don't have to scroll through the entire class to make sure it doesn't have methods that can be accidentally overriden. I know for sure that this can't happen in this project. In other words, I know what to expect. Less Hidden Tricks — Higher predictability of design leads to better visibility of mistakes and tricks. Standardization of source code makes it uniform. This means that it's easier to read and spot problems. Industry Standards — The decision to use this design is made by Checkstyle, not by a project architect. For me, as a project developer, this means that I'm following industry standards. That makes the project (and its leaders) more respectable. Learning — I'll bet that most of you who read this post didn't know about the design rule explained above. Just by reading this article, you learned something new. Imagine how much you could learn after making your code compliant to all 900 rules of Qulice (Checkstyle + PMD + FindBugs). The point about learning brings us to the last, and the most important, thought to discuss. What Do I Get from Qulice? As a programmer, I hope you already realize what you get from working in a project that raises its quality bar as high as Qulice asks. Yes, you'll learn a lot of new things about writing quality Java code. On top of that though, I would actually say that you are getting free lessons with every new line of code you write. And the teacher is a software, written by hundreds of Java developers, for the last ten years. Qulice just integrates those software tools together. Truthfully, it is the developers who are the real authors of quality checks and rules. So, what do I tell those who complain about quality rules being too strict? I say this: \"Do you want to learn and improve, or do you just want to get paid and get away with it?\" ps. You can use my settings.jar for IntelliJ, they are rather strict and will help you clean your code even before Qulice starts to complain. "},{"title":"How to Retry Java Method Call on Exception","url":"/2014/08/15/retry-java-method-on-exception.html","tags":["jcabi","java","aop"],"date":"2014-08-15 00:00:00 +0000","categories":[],"body":" If you have a method that fails occasionally and you want to retry it a few times before throwing an exception. @RetryOnFailure from jcabi-aspects can help. For example, if you're downloading the following web page: @RetryOnFailure ( attempts = 3 , delay = 10 , unit = TimeUnit . SECONDS ) public String load ( URL url ) { return url . openConnection (). getContent (); } This method call will throw an exception only after three failed executions with a ten seconds interval between them. This post explains how jcabi-aspects works with binary weaving. This mechanism integrates AspectJ with your code. When method load() from the example above is called, this is what is happening behind the scene (pseudo-code): while ( attempts ++ < 3 ) { try { return original_load ( url ); } catch ( Throwable ex ) { log ( \"we failed, will try again in 10 seconds\" ); sleep ( 10 ); } } This approach may be very useful in the following situations (based on my experience): Executing JDBC SELECT statements Loading data from HTTP, S3, FTP, etc resources Uploading data over the network Fetching data through RESTful stateless APIs The project is in Github . "},{"title":"Fluent JDBC Decorator","url":"/2014/08/18/fluent-jdbc-decorator.html","tags":["jcabi","java","jdbc"],"date":"2014-08-18 00:00:00 +0000","categories":[],"body":" This is how you fetch text from a SQL table with jcabi-jdbc : String name = new JdbcSession ( source ) . sql ( \"SELECT name FROM employee WHERE id = ?\" ) . set ( 1234 ) . select ( new SingleOutcome < String >( String . class )); Simple and straight forward, isn't it? The library simplifies interaction with relational databases via JDBC, avoiding the need to use ORMs. jcabi-jdbc is a lightweight wrapper of JDBC . It is very convenient to use when you don't need a full-scale ORM (like Hibernate), but want just to select, insert, or update a few rows in a relational database. Every instance of JdbcSession is a \"transaction\" in a database. You start it by instantiating the class with a single parameter — data source. You can obtain the data source from your connection pool. There are many implementations of connection pools. I would recommend that you use BoneCP . Below is an example of how you would connect to PostgreSQL: @Cacheable ( forever = true ) private static DataSource source () { BoneCPDataSource src = new BoneCPDataSource (); src . setDriverClass ( \"org.postgresql.Driver\" ); src . setJdbcUrl ( \"jdbc:postgresql://localhost/db_name\" ); src . setUser ( \"jeff\" ); src . setPassword ( \"secret\" ); return src ; } Be sure to pay attention to the @Cacheable annotation. This post explains how it can help you to cache Java method results for some time. Setting the forever attribute to true means that we don't want this method to be called more than once. Instead, we want the connection pool to be created just once, and every second call should return its existing instance (kind of like a Singleton pattern). jcabi-jdbc website explains how you can insert , update , or delete a row. You can also execute any SQL statement . By default, JdbcSession closes the JDBC connection right after the first select/update/insert operation. Simply put, it is designed to be used mainly for single atomic transactions. However, it is possible to leave the connection open and continue, for example: new JdbcSession ( source ) . autocommit ( false ) . sql ( \"START TRANSACTION\" ) . update () . sql ( \"DELETE FROM employee WHERE name = ?\" ) . set ( \"Jeff Lebowski\" ) . update () . sql ( \"INSERT INTO employee VALUES (?)\" ) . set ( \"Walter Sobchak\" ) . insert ( Outcome . VOID ) . commit (); In this example we're executing three SQL statements one by one, leaving connection (and transaction) open unti commit() is called. "},{"title":"How to Release to Maven Central, in One Click","url":"/2014/08/19/how-to-release-to-maven-central.html","tags":["java","rultor","devops","maven"],"date":"2014-08-19 00:00:00 +0000","categories":["jcg"],"body":"When I release a new version of jcabi-aspects , a Java open source library, to Maven Central, it takes 30 seconds of my time. Maybe even less. Recently, I released version 0.17.2. You can see how it all happened, in Github issue #80 : As you see, I gave a command to Rultor , and it released a new version to Maven central. I didn't do anything else. Now let's see how you can do the same. How you can configure your project so that the release of its new version to Maven Central takes just a few seconds of your time. By the way, I assume that you're hosting your project in Github. If not, this entire tutorial won't work. If you are still not in Github, I would strongly recommend moving there. Prepare Your POM Make sure your pom.xml contains all elements required by Sonatype, explained in Central Sync Requirements . We will deploy to Sonatype, and they will syncronize all JAR (and not only) artifacts to Maven Central. Register a Project With Sonatype Create an account in Sonatype JIRA and raise a ticket, asking to approve your groupId. This OSSRH Guide explains this step in more detail. Create and Distribute a GPG Key Create a GPG key and distribute it, as explained in this Working with PGP Signatures article. When this step is done, you should have two files: pubring.gpg and secring.gpg . Create settings.xml Create settings.xml , next to the two .gpg files created in the previous step: <settings> <profiles> <profile> <id> foo </id> <!-- give it the name of your project --> <properties> <gpg.homedir> /home/r </gpg.homedir> <gpg.keyname> 9A105525 </gpg.keyname> <gpg.passphrase> my-secret </gpg.passphrase> </properties> </profile> </profiles> <servers> <server> <id> sonatype </id> <username> <!-- Sonatype JIRA user name --> </username> <password> <!-- Sonatype JIRA pwd --> </password> </server> </servers> </settings> In this example, 9A105525 is the ID of your public key, and my-secret is the pass phrase you have used while generating the keys. Encrypt Security Assets Now, encrypt these three files with a rultor remote : $ gem install rultor $ rultor encrypt -p me/test pubring.gpg $ rultor encrypt -p me/test secring.gpg $ rultor encrypt -p me/test settings.xml Instead of me/test you should use the name of your Github project. You will get three new files: pubring.gpg.asc , secring.gpg.asc and settings.xml.asc . Add them to the root directory of your project, commit and push. The files contain your secret information, but only the Rultor server can decrypt them. Add Sonatype Repositories I would recommend using jcabi-parent , as a parent pom for your project. This will make many further steps unnecessary. If you're using jcabi-parent, skip this step. However, if you don't use jcabi-parent, you should add these two repositories to your pom.xml : <project> [...] <distributionManagement> <repository> <id> oss.sonatype.org </id> <url> https://oss.sonatype.org/service/local/staging/deploy/maven2/ </url> </repository> <snapshotRepository> <id> oss.sonatype.org </id> <url> https://oss.sonatype.org/content/repositories/snapshots </url> </snapshotRepository> </distributionManagement> </project> Configure GPG Plugin Again, I'd recommend using http://parent.jcabi.com , which configures this plugin automatically. If you're using it, skip this step. Otherwise, add this plugin to your pom.xml : <project> [..] <build> [..] <plugins> [..] <plugin> <artifactId> maven-gpg-plugin </artifactId> <version> 1.5 </version> <executions> <execution> <id> sign-artifacts </id> <phase> verify </phase> <goals> <goal> sign </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> Configure Versions Plugin Once again, I recommend using http://parent.jcabi.com . It configures all required plugins out-of-the-box. If you're using it, skip this step. Otherwise, add this plugin to your pom.xml : <project> [..] <build> [..] <plugins> [..] <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> versions-maven-plugin </artifactId> <version> 2.1 </version> <configuration> <generateBackupPoms> false </generateBackupPoms> </configuration> </plugin> </plugins> </build> </project> Configure Sonatype Plugin Yes, you're right, http://parent.jcabi.com will help you here as well. If you're using it, skip this step too. Otherwise, add these four plugins to your pom.xml : <project> [..] <build> [..] <plugins> [..] <plugin> <artifactId> maven-deploy-plugin </artifactId> <configuration> <skip> true </skip> </configuration> </plugin> <plugin> <artifactId> maven-source-plugin </artifactId> <executions> <execution> <id> package-sources </id> <goals> <goal> jar </goal> </goals> </execution> </executions> </plugin> <plugin> <artifactId> maven-javadoc-plugin </artifactId> <executions> <execution> <id> package-javadoc </id> <phase> package </phase> <goals> <goal> jar </goal> </goals> </execution> </executions> </plugin> <plugin> <groupId> org.sonatype.plugins </groupId> <artifactId> nexus-staging-maven-plugin </artifactId> <version> 1.6 </version> <extensions> true </extensions> <configuration> <serverId> oss.sonatype.org </serverId> <nexusUrl> https://oss.sonatype.org/ </nexusUrl> <description> ${project.version} </description> </configuration> <executions> <execution> <id> deploy-to-sonatype </id> <phase> deploy </phase> <goals> <goal> deploy </goal> <goal> release </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> Create Rultor Config Create a .rultor.yml file in the root directory of your project ( reference page explains this format in details): decrypt : settings.xml : \"repo/settings.xml.asc\" pubring.gpg : \"repo/pubring.gpg.asc\" secring.gpg : \"repo/secring.gpg.asc\" release : script : | mvn versions:set \"-DnewVersion=${tag}\" git commit -am \"${tag}\" mvn clean deploy --settings /home/r/settings.xml You can compare your file with live Rultor configuration of jcabi-aspects . Run It! Now it's time to see how it all works. Create a new ticket in the Github issue tracker, and post something like that into it (read more about Rultor commands ): @rultor release, tag is `0.1` You will get a response in a few seconds. The rest will be done by Rultor. Enjoy :) BTW, if something doesn't work as I've explained, don't hesitate to submit a ticket to Rultor issue tracker . I will try to help you. Yeah, forgot to mention, Rultor is also doing two important things. First, it creates a Github release with a proper description. Second, it posts a tweet about the release, which you can retweet, to make an announcement to your followers. Both features are very convenient for me. For example: DynamoDB Local Maven Plugin, 0.7.1 released https://t.co/C3KULouuKS — rultor.com (@rultors) August 19, 2014 "},{"title":"The Art of Software Testing by Glenford Myers","url":"/2014/08/22/art-of-software-testing.html","tags":["book-review","testing"],"date":"2014-08-22 00:00:00 +0000","categories":[],"body":" \"The Art of Software Testing\" by Glenford J. Myers, Tom Badgett and Corey Sandler is one of my favorite books concerning testing and software engineering in general. In this article, I will provide an overview of the book, as well as highlight the ideas and quotes that I found to be the most interesting. There were three editions of the book. The first one was published in 1979, when I was just too young to appreciate it. The second one was published in 2004 — I read it first in 2007. The third one was published just two years ago, in 2012. I bought this edition also, and read it like it was my first time. This book is still one of the top books in the software testing domain, despite its age and some content that is rather out-dated. Out-dated Content First, let's filter out what is not worth reading (in my opinion). There are eleven chapters, but you can easily skim through nine of them. This is because those chapter discuss concepts that are discussed elsewhere in the book with a more robust level of detail or on a much higher level of abstraction. For example, Chapter 3 contains an eleven-page checklist to be used by a code reviewer in order to find programming mistakes. This list is definitely not comprehensive and it can't compete with, say, \"Code Complete\" by Steve McConnell. I believe, this checklist had significant value twenty years ago, but now it is out of date. Chapter 5 discusses basic principles and strategies of unit testing. However, the discussion is not abstract enough for a short 25-page summary, and is not specific enough for a detailed discussion. Again, twenty years ago this information may have had some value. Nowadays, \"Growing Object-Oriented Softare, Guided by Tests\" by Steven Freeman and Nat Pryce is a much better source for this subject. There are also articles about usability testing, debugging, web application testing, and mobile testing. Here we have the same issue — they are not abstract enough and they are much too outdated to be relevant to the current issues in software testing. I would recommend readers to briefly skim those subjects for background information, but to not read too much into it. Psychology of Testing The most important and valuable part of the book is Chapter 2. It is full of priceless quotes that can also be very practical. For example, on page 6: Testing is a destructive, even sadistic, process, which explains why most people find it difficult In Chapter 2, Dr. Myers discusses the psychology of testing and a very common and crucial misunderstanding of testing objectives. He claims that it is commonly accepted that the goal of software testing is \"to show that a program performs its intended functions correctly\" (p.5). Testers are hired to check whether the software functions as expected. They then report back to management whether all tests have successfully passed and whether the program can be delivered to end users. Despite the plethora of software testing tomes available on the market today, many developers seem to have an attitude that is counter to extensive testing This is what Dr. Myers says on the second page, and I can humbly confirm that in all software groups I've been worked in thus far, almost everyone, including testers, project managers, and programmers, share this philosophy. They all believe that \"testing is the process of demonstrating that errors are not present\" (p.5) However, \"these definitions are upside down\" (p.6). The psychology of testing should be viewed as the opposite. There are two quotes that support this theory and I feel that they make the entire book. The first one, on page 6, defines the goal of software testing: Testing is the process of executing a program with the intent of finding errors The second one, on the following page, further refines the first goal: An unsuccessful test case is one that causes a program to produce the correct results without finding any errors Dr. Myers comes back to these two thoughts in every chapter. He reiterates over and over again that we should change the underlying psychology of how we view testing, in order to change our testing results. We should focus on breaking the software instead of confirming that it works. Because testing is a \"sadistic process\" (p.6) of breaking things. It is a \"destructive process\" (p.8). If you read Chapter2 very carefully and truly understand its underlying ideas, it may change your entire life :) This chapter should be a New Testament of every tester. Test Completion Criteria In Chapter 2, Dr. Myers also mentions that a program, no matter how simple, contains an unlimited number of errors. He says that \"you cannot test a program to guarantee that it is error free\" (p.10) and that \"it is impractical, often impossible, to find all the errros in a program\" (p.8). Furthermore, at the end of Chapter 6, he makes an important observation (p.135): One of the most difficult questions to answer when testing a program is determining when to stop, since there is no way of knowing if the error just detected is the last remaining error The problem is obvious. Since any program contains an unlimited number of errors, it doesn't matter how long we test, we won't find all of them. So when do we stop? What goals do we set for our testers? And even more importantly, when do we pay them and how much (this question is important to me since I only work with contractors and am required to define measurable and achievable goals)? The answer Dr. Myers gives is brilliant (p.136): Since the goal of testing is to find errors, why not make the completion criterion the detection of some predefined number of errors? He then goes on to discuss exactly how this \"predefined number\" can be estimated. I find this idea very interesting. I have even applied it to a few projects I've had in the last few years. It works. However it can also cause serious psychological problems for the team. Most people simply resent the goal of \"testing until you find a required number of bugs.\" The most common response is \"what if there are no bugs any more?\". However, after a few fights, the team eventually begins to appreciate the concept and get used to it. So, I can humbly confirm that Dr. Myers is right in his suggestion. You can successful plan testing based on a predefined number of errors. Summary I consider this book a fundamental writing in the area of software testing. This is mostly due to Chapter 2 of the book. In fact, there are just three pages of text that build the foundation of the entire book. They are the skeleton of the other two hundred pages. Unfortunately, since 1979, this skeleton hasn't become the backbone of the software testing industry. Most of us are still working against these principles. "},{"title":"How to Deploy to CloudBees, in One Click","url":"/2014/08/25/deploy-to-cloudbees.html","tags":["java","rultor","devops"],"date":"2014-08-25 00:00:00 +0000","categories":[],"body":"When I deploy a new version of stateful.co , a Java web application, to CloudBees, it takes 30 seconds of my time. Maybe even less. Recently, I deployed version 1.6.5. You can see how it all happened, in Github issue #6 : As you see, I gave a command to Rultor , and it packaged, tested and deployed a new version to CloudBees. I didn't do anything else. Now let's see how you can do the same. How you can configure your project so that the deployment of its new version to CloudBees takes just a few seconds of your time. Since CloudBees is [shutting down its PaaS service](http://www.cloudbees.com/press/cloudbees-becomes-enterprise-jenkins-company) by the end of December, 2014, this article will have no sense after that. Configure the CloudBees Maven Plugin Add this profile to your pom.xml : <project> [..] <profiles> <profile> <id> cloudbees </id> <activation> <property><name> bees.appId </name></property> </activation> <pluginRepositories> <pluginRepository> <id> cloudbees-public-release </id> <url> http://repository-cloudbees.forge.cloudbees.com/public-release </url> </pluginRepository> </pluginRepositories> <build> <pluginManagement> <plugins> <plugin> <artifactId> maven-deploy-plugin </artifactId> <configuration> <skip> true </skip> </configuration> </plugin> </plugins> </pluginManagement> <plugins> <plugin> <groupId> com.cloudbees </groupId> <artifactId> bees-maven-plugin </artifactId> <version> 1.3.2 </version> <configuration> <appid> ${bees.id} </appid> <apikey> ${bees.key} </apikey> <secret> ${bees.secret} </secret> </configuration> <executions> <execution> <id> deploy-to-production </id> <phase> deploy </phase> <goals> <goal> deploy </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> This plugin is not in Maven Central (unfortunately). That's why we have to specify that <pluginRepository> . Pay attention to the fact that we're also disabling maven-deploy-plugin , since it would try to deploy your WAR package to the repository from the <distributionManagement> section. We want to avoid this. The profile gets activated only when the bees.id property is defined. This won't happen during your normal development and testing, but it will occur during the deployment cycle initiated by Rultor, because we will define this property in settings.xml (discussed below). Secure Access to CloudBees Create an account in CloudBees and register your web application there. CloudBees is free, as long as you don't need too much computing power. I believe that web applications should be light-weight by definition, so CloudBees' free layer is an ideal choice. Create a settings.xml file (but don't commit it to your repo!): <settings> <profiles> <profile> <id> cloudbees </id> <properties> <bees.id> stateful/web </bees.id> <bees.key> <!-- your key --> </bees.key> <bees.secret> <!-- your secret --> </bees.secret> </properties> </profile> </profiles> </settings> Encrypt this file using rultor remote : $ gem install rultor $ rultor encrypt -p me/test settings.xml Instead of me/test use the name of your Github project. You should get a settings.xml.asc file; add it to the root directory of your project, commit and push. This file contains your CloudBees credentials, but in an encrypted format. Nobody can read it, except the Rultor server. Configure Versions Plugin I recommend using http://parent.jcabi.com . It configures the required plugin out-of-the-box. If you're using it, skip this step. Otherwise, add this plugin to your pom.xml : <project> [..] <build> [..] <plugins> [..] <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> versions-maven-plugin </artifactId> <version> 2.1 </version> <configuration> <generateBackupPoms> false </generateBackupPoms> </configuration> </plugin> </plugins> </build> </project> Configure Rultor Create a .rultor.yml file in the root directory of your project (this reference page explains this format in detail): decrypt : settings.xml : \"repo/settings.xml.asc\" release : script : | mvn versions:set \"-DnewVersion=${tag}\" git commit -am \"${tag}\" mvn clean deploy --settings /home/r/settings.xml You can compare your file with live Rultor configuration of stateful.co . Run It! Now it's time to see how it all works. Create a new ticket in the Github issue tracker, and post something like that into it (read more about Rultor commands ): @rultor release, tag is `0.1` You will get a response in a few seconds. The rest will be done by Rultor. Enjoy :) BTW, if something doesn't work as I've explained, don't hesitate to submit a ticket to the Rultor issue tracker . I will try to help you. Also, a similar configuration can be performed for Heroku (using jcabi-heroku-maven-plugin ) and for AWS Elastic Beanstalk (using jcabi-beanstalk-maven-plugin ). I'll probably dedicate individual posts to them, as well. "},{"title":"How to Publish to Rubygems, in One Click","url":"/2014/08/26/publish-to-rubygems.html","tags":["rubygems","rultor","devops","ruby"],"date":"2014-08-26 00:00:00 +0000","categories":[],"body":"When I release a new version of jgd , a Ruby gem, to Rubygems.org, it takes 30 seconds of my time. Here is how I released a bug fix for version 1.5.1, in Github issue #6 : As you see, I gave a command to Rultor , and it released a new version to Rubygems. I didn't do anything else. Now let's see how you can do the same. How you can configure your project so that the release of its new version to Rubygems.org takes just a few seconds of your time. By the way, I assume that you're hosting your project in Github. If not, this entire tutorial won't work. If you are still not in Github, I would strongly recommend moving there. Create Rubygems Account Create an account in Rubygems.org . Create rubygems.yml Create a rubygems.yml file (you may already have it as ~/.gem/credentials ): :rubygems_api_key : d355d8940bb031bfe9acf03ed3da4c0d You should get this API key from Rubygems. To find your API key, click on your username when logged in to RubyGems.org and then click on \"Edit Profile\". Encrypt rubygems.yml Now, encrypt rubygems.yml with a rultor remote : $ gem install rultor $ rultor encrypt -p me/test rubygems.yml Instead of me/test use the name of your Github project. You will get a new file rubygems.yml.asc . Add this file to the root directory of your project, commit and push. The file contains your secret information, but only the Rultor server can decrypt it. Prepare Gemspec In your gemspec file, make sure you use 1.0.snapshot as a version number: # coding: utf-8 Gem :: Specification . new do | s | # ... s . version = '1.0.snapshot' # ... end This version name will be replaced by Rultor during deployment. Configure Rultor Create a .rultor.yml file in the root directory of your project: decrypt : rubygems.yml : \"repo/rubygems.yml.asc\" release : script : | rm -rf *.gem sed -i \"s/1.0.snapshot/${tag}/g\" foo.gemspec gem build foo.gemspec chmod 0600 /home/r/rubygems.yml gem push *.gem --config-file /home/r/rubygems.yml In this example, replace foo with the name of your gem. Run It! Now it's time to see how it all works. Create a new ticket in the Github issue tracker, and post something like that into it (read more about Rultor commands ): @rultor release, tag is `0.1` You will get a response in a few seconds. The rest will be done by Rultor. Enjoy :) BTW, if something doesn't work as I've explained, don't hesitate to submit a ticket to Rultor issue tracker . I will try to help you. "},{"title":"How We Run as a Non-Root Inside Docker Container","url":"/2014/08/29/docker-non-root.html","tags":["docker","rultor"],"date":"2014-08-29 00:00:00 +0000","categories":[],"body":"Docker starts a process inside its container as a \"root\" user. In some cases, this is not convenient though. For example, initdb from PostgreSQL doesn't like to be started as root and will fail. In rultor.com , a DevOps team assistant, we're using Docker as a virtualization technology for every build we run. Here is how we change the user inside a running container, right after it is started. First, this is how we start a new Docker container: $ sudo docker run -i -t --rm -v \"$(pwd):/main\" yegor256/rultor /main/entry.sh There are two files in the current directory: entry.sh and script.sh . entry.sh is the file being executed by Docker on start, and it contains the following: #!/bin/bash adduser --disabled-password --gecos '' r adduser r sudo echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers su -m r -c /home/r/script.sh script.sh will be executed as a user r inside the container. And this r user will have sudo permissions. This is exactly what all projects, managing their DevOps procedures with rultor.com , need. "},{"title":"Simple Java SSH Client","url":"/2014/09/02/java-ssh-client.html","tags":["java","jcabi","ssh"],"date":"2014-09-02 00:00:00 +0000","categories":["jcg"],"body":"An execution of a shell command via SSH can be done in Java, in just a few lines, using jcabi-ssh : String hello = new Shell . Plain ( new SSH ( \"ssh.example.com\" , 22 , \"yegor\" , \"-----BEGIN RSA PRIVATE KEY-----...\" ) ). exec ( \"echo 'Hello, world!'\" ); jcabi-ssh is a convenient wrapper of JSch , a well-known pure Java implementation of SSH2. Here is a more complex scenario, where I upload a file via SSH and then read back its grepped content: Shell shell = new SSH( \"ssh.example.com\", 22, \"yegor\", \"-----BEGIN RSA PRIVATE KEY-----...\" ); File file = new File(\"/tmp/data.txt\"); new Shell.Safe(shell).exec( \"cat > d.txt && grep 'some text' d.txt\", new FileInputStream(file), Logger.stream(Level.INFO, this), Logger.stream(Level.WARNING, this) ); Class SSH , which implements interface Shell , has only one method, exec . This method accepts four arguments: interface Shell { int exec( String cmd, InputStream stdin, OutputStream stdout, OutputStream stderr ); } I think it's obvious what these arguments are about. There are also a few convenient decorators that make it easier to operate with simple commands. Shell.Safe Shell.Safe decorates an instance of Shell and throws an exception if the exec exit code is not equal to zero. This may be very useful when you want to make sure that your command executed successfully, but don't want to duplicate if/throw in many places of your code. Shell ssh = new Shell.Safe( new SSH( \"ssh.example.com\", 22, \"yegor\", \"-----BEGIN RSA PRIVATE KEY-----...\" ) ); Shell.Verbose Shell.Verbose decorates an instance of Shell and copies stdout and stderr to the slf4j logging facility (using jcabi-log ). Of course, you can combine decorators, for example: Shell ssh = new Shell.Verbose( new Shell.Safe( new SSH( \"ssh.example.com\", 22, \"yegor\", \"-----BEGIN RSA PRIVATE KEY-----...\" ) ) ); Shell.Plain Shell.Plain is a wrapper of Shell that introduces a new exec method with only one argument, a command to execute. It also doesn't return an exit code, but stdout instead. This should be very convenient when you want to execute a simple command and just get its output (I'm combining it with Shell.Safe for safety): String login = new Shell.Plain(new Shell.Safe(ssh)).exec(\"whoami\"); Download You need a single dependency jcabi-ssh.jar in your Maven project (get its latest version in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-ssh </artifactId> </dependency> The project is in Github . If you have any problems, just submit an issue. I'll try to help. "},{"title":"RESTful API and a Web Site in the Same URL","url":"/2014/09/09/restful-web-sites.html","tags":["restful","xslt","xml"],"date":"2014-09-09 00:00:00 +0000","categories":["jcg"],"body":"Look at Github RESTful API, for example. To get information about a repository you should make a GET request to api.github.com/repos/yegor256/rultor . In response, you will get a JSON document with all the details of the yegor256/rultor repository. Try it, the URL doesn't require any authentication. To open the same repository in a nice HTML+CSS page, you should use a different URL: github.com/yegor256/rultor . The URL is different, the server-side is definitely different, but the nature of the data is exactly the same. The only thing that changes is a representation layer. In the first case, we get JSON; in the second — HTML. How about combining them? How about using the same URL and the same server-side processing mechanism for both of them? How about shifting the whole rendering task to the client-side (the browser) and letting the server work solely with the data? The Good, the Bad, The Wierd (2008) by Kim Jee-woon XSLT is the technology that can help us do this. In \"XML+XSLT in a Browser\" I explained briefly how it works in a browser. In a nutshell, the server returns an XML with some data and a link to the XSL stylesheet. The stylesheet, being executed in a browser, converts XML to HTML. XSL language is as powerful as any other rendering engine, like JSP, JSF, Tiles, or what have you. Actually, it is much more powerful. Using this approach we literally remove the entire rendering layer (\"View\" in the MVC paradigm) from the server and move it to the browser. If we can make it possible, the web server will exponse just a RESTful API, and every response page will have an XSL stylesheet attached. What do we gain? We'll discuss later, at the end of the post. Now, let's see what problems we will face: JSON doesn't have a rendering layer. There is no such thing as XSLT for JSON. So, we will have to forget about JSON and stay with XML only. For me, this sounds perfectly all right. Others don't like XML and prefer to work with JSON only. Never understood them :) XSLT 2.0 is not supported by all browsers. Even XSLT 1.0 is only supported by some of them. For example, Internet Explorer 8 doesn't support XSLT at all. Browsers support only GET and POST HTTP methods, while traditional RESTful APIs exploit also, at least, PUT and DELETE . The first problem is not really a problem. It's just a matter of taste (and level of education). The last two problems are much more serious. Let's discuss them. XSL Transformation on the Server XSLT is not supported by some browsers. How do we solve this? I think that the best approach is to parse the User-Agent HTTP header in every request and make a guess, whether this particular version of the browser supports XSLT or not. It's not so difficult to do, since this compatibility information is public. If the browser doesn't support XSLT, we can do the transformation on the server side. We already have the XML with data, generated by the server, and we already have the XSL attached to it. All we need to do is to apply the latter to the former and obtain an HTML page. Then, we return the HTML to the browser. Besides that, we can also pay attention to the Accept header. If it is set to application/xml or text/xml , we return XML, no matter what User-Agent is saying. This means, basically, that some API client is talking to us, not a browser. And this client is not interested in HTML, but in pure data in XML format. POST Instead of PUT There is no workaround for this. Browsers don't know anything about PUT or DELETE . So, we should also forget them in our RESTful APIs. We should design our API using only two methods: GET and POST . Is this even possible? Yes. Why not? It won't look as fancy as with all six methods (some APIs also use OPTIONS and HEAD ), but it will work. What Do We Gain? OK, here is the question — why do we need this? What's wrong with the way most people work now? Why can't we make a web site separate from the API? What benefits do we get if we combine them? I've been combining them in all web applications I've worked with since 2011. And the biggest advantage I'm experiencing is avoiding code duplication. It is obvious that in the server we don't duplicate controllers (in the case of MVC). We have one layer of controllers, and they control both the API and the web site (since they are one thing now). Avoiding code duplication is a very important achievement. Moreover, I believe that it is the most important target for any software project. These small web apps work exactly as explained above: s3auth.com , stateful.co , bibrarian.com . They are all open source, and you can see their source code in Github. "},{"title":"Anti-Patterns in OOP","url":"/2014/09/10/anti-patterns-in-oop.html","tags":["oop","anti-pattern"],"date":"2014-09-10 00:00:00 +0000","categories":[],"body":"Here they come: NULL References Utility Classes Mutable Objects Getters and Setters Object-Relational Mapping (ORM) Singletons Controllers, Managers, Validators Public Static Methods Class Casting Avoid them at all cost. "},{"title":"Deployment Script vs. Rultor","url":"/2014/09/11/deployment-script-vs-rultor.html","tags":["rultor","devops"],"date":"2014-09-11 00:00:00 +0000","categories":["jcg"],"body":" When I explain how Rultor automates deployment/release processes, very often I hear something like: But I already have a script that deploys everything automatically. This response is very common, so I decided to summarize my three main arguments for automated Rultor deployment/release processes in one article: 1) isolated docker containers, 2) visibility of logs and 3) security of credentials. Read about them and see what Rultor gives you on top of your existing deployment script(s). Charlie and the Chocolate Factory (2005) by Tim Burton Before we start with the arguments, let me emphasize that Rultor is a useful interface to your custom scripts. When you decide to automate deployment with Rultor, you don't throw away any of your existing scripts. You just teach Rultor how to call them. Isolated Docker Containers The first advantage you get once you start calling your deployment scripts from Rultor is the usage of Docker . I'm sure you know what Docker is, but for those who don't — it is a manager of virtual Linux \"machines\". It's a command line script that you call when you need to run some script in a new virtual machine (aka \"container\"). Docker starts the container almost immediately and runs your script. The beauty of Docker is that every container is a perfectly isolated Linux environment, with its own file system, memory, processes, etc. When you tell Rultor to run your deployment script, it starts a new Docker container and runs your script there. But what benefit does this give me, you ask? The main benefit is that the container gets destroyed right after your script is done. This means that you can do all pre-configuration inside the container without any fear of conflict with your main working platform. Let me give an example. I'm developing on MacBook, where I install and remove packages which I need for development. At the same time, I have a project that, in order to be deployed, requires PHP 5.3, MySQL 5.6, phing, phpunit, phpcs and xdebug. Every MacOS version needs to be configured specifically to get these applications up and running, and it's a time-consuming job. I can change laptops, and I can change MacOS versions, but the project stays the same. It still requires the same set of packages in order to run its deployment script successfully. And the project is not in active development any more. I simply don't need these packages for my day-to-day work, since I'm working with Java more now. But, when I need to make a minor fix to that PHP project and deploy it, I have to install all the required PHP packages and configure them. Only after that can I deploy that minor fix. It is annoying, to say the least. Docker gives me the ability to automate all of this together. My existing deployment script will get a preamble, which will install and configure all necessary PHP-related packages in a clean Ubuntu container. This preamble will be executed on every run of my deployment script, inside a Docker container. For example, it may look like this: My deployment script looked like this before I started to use Rultor: 1 2 3 #!/bin/bash phing test git ftp push --user \"..\" --passwd \"..\" --syncroot php/src ftp://ftp.example.com/ Just two lines. The first one is a full run of unit tests. The second one is an FTP deployment to the production server. Very simple. But this script will only work if PHP 5.3, MySQL, phing, xdebug, phpcs and phpunit are installed. Again, it's a lot of work to install and configure them every time I upgrade my MacOS or change a laptop. Needless to say, that if/when someone joins the project and tries to run my scripts, he/she will have to do this pre-installation work again. So, here is a new script, which I'm using now. It is being executed inside a new Docker container, every time: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # First, we install all prerequisites sudo apt-get install -y php5 php5-mysql mysql sudo apt-get install php-pear sudo pear channel-discover pear.phpunit.de sudo pear install phpunit/PHPUnit sudo pear install PHP_CodeSniffer sudo pecl install xdebug sudo pear channel-discover pear.phing.info sudo pear install phing/phing # And now the same script I had before phing test git ftp push --user \"..\" --passwd \"..\" --syncroot php/src ftp://ftp.example.com/ Obviously, running this script on my MacBook (without virtualization) would cause a lot of trouble. Well, I don't even have apt-get here :) Thus, the first benefit that Rultor gives you is an isolation of your deployment script in its own virtual environment. We have this mostly thanks to Docker. Visibility of Logs Traditionally, we keep deployment scripts in some ~/deploy directory and run them with a magic set of parameters. In a small project, you do this yourself and this directory is on your own laptop. In a bigger project, there is a \"deployment\" server, that has that magic directory with a set of scripts that can be executed only by a few trusted senior developers. I've seen this setup many times. The biggest issue here is traceability. It's almost impossible to find out who deployed what and why some particular deployment failed. The senior deployment gurus simply SSH to the server and run those magic scripts with magic parameters. Logs are usually lost and problem tracking is very difficult or impossible. Rultor offers something different. With Rultor, there is no SSH access to deployment scripts any more. All scripts stay in the .rultor.yml configuration file, and you start them by posting messages in your issue tracking system (for example Github, JIRA or Trac). Rultor runs the script and publishes its full log right to your ticket. The log stays with your project forever. You can always get back to the ticket you were working with and check why deployment failed and what instructions were actually executed. For example, check out this Github issue, where I was deploying a new version of Rultor itself, and failed a few times: yegor256/rultor#563 . All my failed attempts are protocolled. I can always get back to them and investigate. For a big project this information is vital. Thus, the second benefit of Rultor versus a standalone deployment script is visibility of every single operation. Security of Credentials When you have a custom script sitting in your laptop or in that secret team deployment server, your production credentials stay close to it. There is just no other way. If your software works with a database, it has to know login credentials (user name, password, DB name, port number, etc.). Well, in the worst case, some people just hard code that information right into the source code. We aren't even going to discuss this case, that's how bad it is. But let's say you separate your DB credentials from the source code. You will have something like a db.properties or db.ini file, which will be attached to the application right before deployment. You can also keep that file directly in the production server, which is even better, but not always possible, especially with PaaS deployments, for example. A similar problem exists with deployments of artifacts to repositories. Say, you're regularly deploying to RubyGems.org. Your ~/.gem/credentials will contain your secret API key. So, very often, your deployment scripts are accompanied by some files with sensitive and secure information. And these files have this information in a plain, open format. No encryption, no protection. Just user names, passwords, codes and tokens in plain text. Why is this bad? Well, for a single developer with a single laptop this doesn't sound like a problem. Although, I don't like the idea of losing a laptop somewhere in an airport with all credentials open and ready to be used. You may argue that there are disc protection tools, like FileVault for MacOS or BestCrypt for Windows. Yes, maybe. But let's see what happens when we have a team of developers, working together and sharing those deployment scripts and files with credentials. Once you give access to your deployment scripts to a new member of the team, you have to share all that sensitive data. There is just no way around it. In order to use the scripts he/she has to be able to open files with credentials. This is a problem, if you care about the security of your data. Rultor solves this problem by offering an on-the-fly GPG decryption of your sensitive data, right before they are used by your deployment scripts. In the .rultor.yml configuration file you just say: decrypt : db.ini : \"repo/db.ini.asc\" deploy : script : ftp put db.ini production Then, you encrypt your db.ini using a Rultor GPG key, and fearlessly commit db.ini.asc to the repository. Nobody will be able to open and read that file, except the Rultor server itself, right before running the deployment script. Thus, the third benefit of Rultor versus a standalone deployment script is proper security of sensitive data. "},{"title":"Deploying to Heroku, in One Click","url":"/2014/09/13/deploying-to-heroku.html","tags":["rultor","devops","heroku","java"],"date":"2014-09-13 00:00:00 +0000","categories":[],"body":"There were a few articles already about our usage of Rultor for automating continuous delivery cycles of Java and Ruby projects, including Rubygems , CloudBees and MavenCentral . This one describes how Heroku deployment can be automated. When I need to deploy a new version of an Aintshy web application, all I do is create one message in a Github ticket. I just say @rultor release 0.1.4 and version 0.1.4 gets deployed to Heroku. See Github ticket #5 . You can do the same, with the help of Rultor.com , a free hosted DevOps assistant. Create Heroku Project Create a new project at Heroku.com . Then install their command line toolbelt . Authenticate at Heroku You should authenticate your public SSH key at Heroku, using their command line toolbelt. The process is explained here , but it is not much of a process. You just run heroku login and enter your login credentials. As a result, you will get your existing key (located at ~/.ssh/id_rsa.pub ) authenticated by Heroku. If you didn't have the key before, it will be created automatically. Encrypt SSH Key Now, encrypt id_rsa and id_rsa.pub (they are in the ~/.ssh directory) with a rultor remote : $ gem install rultor $ rultor encrypt -p me/test id_rsa $ rultor encrypt -p me/test id_rsa.pub Instead of me/test use the name of your Github project. You will get two new files id_rsa.asc and id_rsa.pub.asc . Add them to the root directory of your project, commit and push. These files contain your secret information, but only the Rultor server can decrypt them. Create Rultor Config Create a .rultor.yml file in the root directory of your project ( reference page explains this format in detail): decrypt : id_rsa : \"repo/id_rsa.asc\" id_rsa.pub : \"repo/id_rsa.pub.asc\" release : script : | mvn versions:set \"-DnewVersion=${tag}\" git commit -am \"${tag}\" mvn clean install -Pqulice --errors git remote add heroku git@heroku.com:aintshy.git mkdir ~/.ssh mv ../id_rsa ../id_rsa.pub ~/.ssh chmod -R 600 ~/.ssh/* echo -e \"Host *\\n StrictHostKeyChecking no\\n UserKnownHostsFile=/dev/null\" > ~/.ssh/config git push -f heroku $(git symbolic-ref --short HEAD):master You can compare your file with live Rultor configuration of aintshy/hub . Run It! Now it's time to see how it all works. Create a new ticket in the Github issue tracker, and post something like this into it (read more about Rultor commands ): @rultor release, tag is `0.1` You will get a response in a few seconds. The rest will be done by Rultor. Enjoy :) BTW, if something doesn't work as I've explained, don't hesitate to submit a ticket to the Rultor issue tracker . I will try to help you. "},{"title":"Getters/Setters. Evil. Period.","url":"/2014/09/16/getters-and-setters-are-evil.html","tags":["oop","anti-pattern"],"date":"2014-09-16 00:00:00 +0000","categories":["best","jcg"],"body":"There is an old debate, started in 2003 by Allen Holub in this Why getter and setter methods are evil famous article, about whether getters/setters is an anti-pattern and should be avoided or if it is something we inevitably need in object-oriented programming. I'll try to add my two cents to this discussion. The gist of the following text is this: getters and setters is a terrible practice and those who use it can't be excused. Again, to avoid any misunderstanding, I'm not saying that get/set should be avoided when possible. No. I'm saying that you should never have them near your code. Arrogant enough to catch your attention? You've been using that get/set pattern for 15 years and you're a respected Java architect? And you don't want to hear that nonsense from a stranger? Well, I understand your feelings. I felt almost the same when I stumbled upon Object Thinking by David West, the best book about object-oriented programming I've read so far. So please. Calm down and try to understand while I try to explain. Existing Arguments There are a few arguments against \"accessors\" (another name for getters and setters), in an object-oriented world. All of them, I think, are not strong enough. Let's briefly go through them. Tell, Don't Ask Allen Holub says, \"Don't ask for the information you need to do the work; ask the object that has the information to do the work for you\". Violated Encapsulation Principle An object can be teared apart by other objects, since they are able to inject any new data into it, through setters. The object simply can't encapsulate its own state safely enough, since anyone can alter it. Exposed Implementation Details If we can get an object out of another object, we are relying too much on the first object's implementation details. If tomorrow it will change, say, the type of that result, we have to change our code as well. All these justifications are reasonable, but they are missing the main point. Fundamental Misbelief Most programmers believe that an object is a data structure with methods. I'm quoting Getters and Setters Are Not Evil , an article by Bozhidar Bozhanov: But the majority of objects for which people generate getters and setters are simple data holders. This misconception is the consequence of a huge misunderstanding! Objects are not \"simple data holders\". Objects are not data structures with attached methods. This \"data holder\" concept came to object-oriented programming from procedural languages, especially C and COBOL. I'll say it again: an object is not a set of data elements and functions that manipulate them. An object is not a data entity. What is it then? A Ball and A Dog In true object-oriented programming, objects are living creatures, like you and me. They are living organisms, with their own behaviour, properties and a life cycle. Can a living organism have a setter? Can you \"set\" a ball to a dog? Not really. But that is exactly what the following piece of software is doing: Dog dog = new Dog (); dog . setBall ( new Ball ()); How does that sound? Can you get a ball from a dog? Well, you probably can, if she ate it and you're doing surgery. In that case, yes, we can \"get\" a ball from a dog. This is what I'm talking about: Dog dog = new Dog (); Ball ball = dog . getBall (); Or an even more ridiculous example: Dog dog = new Dog (); dog . setWeight ( \"23kg\" ); Can you imagine this transaction in the real world? :) Does it look similar to what you're writing every day? If yes, then you're a procedural programmer. Admit it. And this is what David West has to say about it, on page 30 of his book: Step one in the transformation of a successful procedural developer into a successful object developer is a lobotomy. Do you need a lobotomy? Well, I definitely needed one and received it, while reading West's Object Thinking . Object Thinking Start thinking like an object and you will immediately rename those methods. This is what you will probably get: Dog dog = new Dog (); dog . take ( new Ball ()); Ball ball = dog . give (); Now, we're treating the dog as a real animal, who can take a ball from us and can give it back, when we ask. Worth mentioning is that the dog can't give NULL back. Dogs simply don't know what NULL is :) Object thinking immediately eliminates NULL references from your code. A Fish Called Wanda (1988) by Charles Crichton Besides that, object thinking will lead to object immutability, like in the \"weight of the dog\" example. You would re-write that like this instead: Dog dog = new Dog ( \"23kg\" ); int weight = dog . weight (); The dog is an immutable living organism, which doesn't allow anyone from the outside to change her weight, or size, or name, etc. She can tell, on request, her weight or name. There is nothing wrong with public methods that demonstrate requests for certain \"insides\" of an object. But these methods are not \"getters\" and they should never have the \"get\" prefix. We're not \"getting\" anything from the dog. We're not getting her name. We're asking her to tell us her name. See the difference? We're not talking semantics here, either. We are differentiating the procedural programming mindset from an object-oriented one. In procedural programming, we're working with data, manipulating them, getting, setting, and deleting when necessary. We're in charge, and the data is just a passive component. The dog is nothing to us — it's just a \"data holder\". It doesn't have its own life. We are free to get whatever is necessary from it and set any data into it. This is how C, COBOL, Pascal and many other procedural languages work(ed). On the contrary, in a true object-oriented world, we treat objects like living organisms, with their own date of birth and a moment of death — with their own identity and habits, if you wish. We can ask a dog to give us some piece of data (for example, her weight), and she may return us that information. But we always remember that the dog is an active component. She decides what will happen after our request. That's why, it is conceptually incorrect to have any methods starting with set or get in an object . And it's not about breaking encapsulation, like many people argue. It is whether you're thinking like an object or you're still writing COBOL in Java syntax. PS. Yes, you may ask, — what about JavaBeans, JPA, JAXB, and many other Java APIs that rely on the get/set notation? What about Ruby's built-in feature that simplies the creation of accessors? Well, all of that is our misfortune. It is much easier to stay in a primitive world of procedural COBOL than to truly understand and appreciate the beautiful world of true objects. PPS. Forgot to say, yes, dependency injection via setters is also a terrible anti-pattern. About it, in one of the next posts :) "},{"title":"Remote Programming in Teamed.io","url":"/2014/09/22/remote-programming-interview.html","tags":["mgmt"],"date":"2014-09-22 00:00:00 +0000","categories":[],"body":"Here is an interview taken by Lisette Sutherland from www.CollaborationSuperpowers.com , a few hours ago, which I enjoyed to give :) I answered these questions (approximately): How Teamed.io differs from other software companies (0:50)? How do we control programmers remotely (1:59)? Do we compare ourselves with open source (3:52)? How do we build a network of programmers (5:10)? Why people like to work with us (5:40)? What happens when a programmer fails (7:50)? How can it be financially successful (9:40)? How do we organize \"team building\" (11:50)? What challenges do we have (14:50)? What about micro-management (17:55)? Can this work in a non-IT sector (19:40)? What do you do to manage the team (20:48)? Isn't it difficult to manage so many tasks (24:18)? Do we have cultural issues (25:35)? Is it true that people are not enough result-oriented (27:40)? Are there any other challenges (29:12)? What do I like personally about it (30:40)? How do we scale our teams when we need more programmers (32:01)? What an \"unlimited pool of talents\" means (34:40)? What advice do I have for those who work remotely (37:50)? Where do I work from, personally (39:10)? How do we find clients (42:29)? Enjoy :) "},{"title":"Built-in Fake Objects","url":"/2014/09/23/built-in-fake-objects.html","tags":["testing","java"],"date":"2014-09-23 00:00:00 +0000","categories":["jcg"],"body":"While mock objects are perfect instruments for unit testing, mocking through mock frameworks may turn your unit tests into an unmaintainable mess. Thanks to them we often hear that \"mocking is bad\" and \"mocking is evil\". The root cause of this complexity is that our objects are too big. They have many methods and these methods return other objects, which also have methods. When we pass a mock version of such an object as a parameter, we should make sure that all of its methods return valid objects. This leads to inevitable complexity, which turns unit tests to waste almost impossible to maintain. Object Hierarchy Take the Region interface from jcabi-dynamo as an example (this snippet and all others in this article are simplified, for the sake of brevity): public interface Region { Table table ( String name ); } Its table() method returns an instance of the Table interface, which has its own methods: public interface Table { Frame frame (); Item put ( Attributes attrs ); Region region (); } Interface Frame , returned by the frame() method, also has its own methods. And so on. In order to create a properly mocked instance of interface Region , one would normally create a dozen other mock objects. With Mockito it will look like this: public void testMe () { // many more lines here... Frame frame = Mockito . mock ( Frame . class ); Mockito . doReturn (...). when ( frame ). iterator (); Table table = Mockito . mock ( Table . class ); Mockito . doReturn ( frame ). when ( table ). frame (); Region region = Mockito . mock ( Region . class ); Mockito . doReturn ( table ). when ( region ). table ( Mockito . anyString ()); } And all of this is just a scaffolding before the actual testing. Sample Use Case Let's say, you're developing a project that uses jcabi-dynamo for managing data in DynamoDB. Your class may look similar to this: public class Employee { private final String name ; private final Region region ; public Employee ( String empl , Region dynamo ) { this . name = empl ; this . region = dynamo ; } public Integer salary () { return Integer . parseInt ( this . region . table ( \"employees\" ) . frame () . where ( \"name\" , this . name ) . iterator () . next () . get ( \"salary\" ) . getN () ); } } You can imagine how difficult it will be to unit test this class, using Mockito, for example. First, we have to mock the Region interface. Then, we have to mock a Table interface and make sure it is returned by the table() method. Then, we have to mock a Frame interface, etc. The unit test will be much longer than the class itself. Besides that, its real purpose, which is to test the retrieval of an employee's salary, will not be obvious to the reader. Moreover, when we need to test a similar method of a similar class, we will need to restart this mocking from scratch. Again, multiple lines of code, which will look very similar to what we have already written. Fake Classes The solution is to create fake classes and ship them together with real classes. This is what jcabi-dynamo is doing. Just look at its JavaDoc . There is a package called com.jcabi.dynamo.mock that contains only fake classes, suitable only for unit testing. Even though their sole purpose is to optimize unit testing, we ship them together with production code, in the same JAR package. This is what a test will look like, when a fake class MkRegion is used: public class EmployeeTest { public void canFetchSalaryFromDynamoDb () { Region region = new MkRegion ( new H2Data (). with ( \"employees\" , new String [] { \"name\" }, new String [] { \"salary\" } ) ); region . table ( \"employees\" ). put ( new Attributes () . with ( \"name\" , \"Jeff\" ) . with ( \"salary\" , new AttributeValue (). withN ( 50000 )) ); Employee emp = new Employee ( \"Jeff\" , region ); assertThat ( emp . salary (), equalTo ( 50000 )) } } This test looks obvious to me. First, we create a fake DynamoDB region, which works on top of H2Data storage (in-memory H2 database). The storage will be ready for a single employees table with a hash key name and a single salary attribute. Then, we put a record into the table, with a hash Jeff and a salary 50000 . Finally, we create an instance of class Employee and check how it fetches the salary from DynamoDB. I'm currently doing the same thing in almost every open source library I'm working with. I'm creating a collection of fake classes, that simplify testing inside the library and for its users. BTW, a great article on the same subject: tl;dw: Stop mocking, start testing by Ned Batchelder. "},{"title":"Why Monetary Awards Don't Work?","url":"/2014/09/24/why-monetary-awards-dont-work.html","tags":["mgmt"],"date":"2014-09-24 00:00:00 +0000","categories":[],"body":"Monetary rewards for employees. Do they work? Should we use them? Can money motivate creative minds? Will a programmer work better if he gets paid only when he reaches his goals and objectives? Much research has already been done on this subject, and most of it proves that connecting results with money is a very demotivating approach. For example, Ian Larkin says that the most productive workers \"suffered a 6-8% decrease in productivity after the award was instituted\". I believe this is completely true. Money may become a terrible de-motivator for all modern employees (not just programmers). My question is — why is this so? Why doesn't money work, even when it was invented to be a universal instrument to measure our labor? Why can't an American dollar, which has been used for centuries as a trading tool between working people, be used anymore? Why, in a modern office, do we try to hide monetary motivation and replace it with everything else , like free lunches, team building events, paid vacations, etc. Why don't we just say — \"Jeff completed his task faster than everybody else. This is his $500 check. Whoever completes the next task gets $300,\" out loud in the office?... Sounds uncomfortable, doesn't it? Why does money as a motivator scare us? I have an answer. Money doesn't work when there are no ground rules. When we say that Jeff will get a $500 bonus if he finishes his task on time, but don't say what he should do when someone distracts him — Jeff gets frustrated. He also doesn't understand who his boss is anymore. Does he just work for the bonus, or should he also satisfy a CTO who comes to his desk asking him to do something else urgently? Is Jeff allowed to tell the CTO \"to get lost\" because he's working towards his own personal objective (the bonus)? In all cases I've seen myself and in all research cases I've read about, people keep repeating the same mistake. They create a rewards program (monetary or not) without setting ground rules for the team. By doing so, they encourage people to play wild-wild west, where the fastest gets the cash bag. Obviously, the Bad and the Ugly get to the prize faster, while the Good gets demotivated and depressed. In a clockwise direction from the top left corner: The Good, the Bad and the Ugly (1966) by Sergio Leone; Roger Federer; A Serious Man (2009) by Ethan Coen and Joel Coen; Two and a Half Men (TV Series). What do I mean by ground rules? It should be a simple document ( PMBOK calls it a Staffing Management Plan) that helps me, as a team member, answer at least these basic questions: How my personal results are measured? Who gives me tasks and who do I report to? How should I resolve conflicts between tasks? What are my personal deadlines for every task? Do I have measurable quality expectations for my deliverables? How do my mistakes affect my performance grade? The ground rules document should be superior to your boss. If the document says that your results get an A+ grade, the boss should have no say. If she doesn't like you personally, it doesn't matter. You get an A+ grade, and you are the best. That's it. Does your team have such a document? Can you answer all of these questions? If not, you're not ready for a rewards program. It will only make your management situation worse, just like all the scientific research says. Rewards will motivate the most cunning to take advantage of the most hard working and good-natured. Team spirit will suffer, big time. On the other hand, if you have that \"ground rules\" document and you strictly follow it, giving monetary rewards to your workers will significantly increase their performance and motivation. They will know exactly what needs to be done to get the rewards, and they won't have any distraction. Your team won't be a group of wild west gunslingers anymore, but more like players in a sports arena. The best players will go further, and the worst will know exactly what needs to be done to improve. Fair competition will lead to a better cumulative result. Moreover, if your ground rules are strict and explicit, you can use not only rewards, but also punishments. And your team will gladly accept them, because they will help emphasize what (and who) works best and help get rid of the waste. I'm speaking from experience here. In XDSD we're not only rewarding programmers with money, but we also never pay for anything except delivered results. We manage to do this mostly because our groud rules are very strict and non-ambiguous. And we never break them. "},{"title":"DI Containers are Code Polluters","url":"/2014/10/03/di-containers-are-evil.html","tags":["oop","anti-pattern"],"date":"2014-10-03 00:00:00 +0000","categories":[],"body":"While dependency injection (aka, \"DI\") is a natural technique of composing objects in OOP (known long before the term was introduced by Martin Fowler ), Spring IoC , Google Guice , Java EE6 CDI , Dagger and other DI frameworks turn it into an anti-pattern. I'm not going to discuss obvious arguments against \"setter injections\" (like in Spring IoC ) and \"field injections\" (like in PicoContainer ). These mechanisms simply violate basic principles of object-oriented programming and encourage us to create incomplete, mutable objects, that get stuffed with data during the course of application execution. Remember: ideal objects must be immutable and may not contain setters . Instead, let's talk about \"constructor injection\" (like in Google Guice ) and its use with dependency injection containers . I'll try to show why I consider these containers a redundancy, at least. What is Dependency Injection? This is what dependency injection is (not really different from a plain old object composition): public class Budget { private final DB db ; public Budget ( DB data ) { this . db = data ; } public long total () { return this . db . cell ( \"SELECT SUM(cost) FROM ledger\" ); } } The object data is called a \"dependency\". A Budget doesn't know what kind of database it is working with. All it needs from the database is its ability to fetch a cell, using an arbitrary SQL query, via method cell() . We can instantiate a Budget with a PostgreSQL implementation of the DB interface, for example: public class App { public static void main ( String ... args ) { Budget budget = new Budget ( new Postgres ( \"jdbc:postgresql:5740/main\" ) ); System . out . println ( \"Total is: \" + budget . total ()); } } In other words, we're \"injecting\" a dependency into a new object budget . An alternative to this \"dependency injection\" approach would be to let Budget decide what database it wants to work with: public class Budget { private final DB db = new Postgres ( \"jdbc:postgresql:5740/main\" ); // class methods } This is very dirty and leads to 1) code duplication, 2) inability to reuse, and 3) inability to test, etc. No need to discuss why. It's obvious. Thus, dependency injection via a constructor is an amazing technique. Well, not even a technique, really. More like a feature of Java and all other object-oriented languages. It's expected that almost any object will want to encapsulate some knowledge (aka, a \"state\"). That's what constructors are for. What is a DI Container? So far so good, but here comes the dark side — a dependency injection container. Here is how it works (let's use Google Guice as an example): import javax.inject.Inject ; public class Budget { private final DB db ; @Inject public Budget ( DB data ) { this . db = data ; } // same methods as above } Pay attention: the constructor is annotated with @Inject . Then, we're supposed to configure a container somewhere, when the application starts: Injector injector = Guice . createInjector ( new AbstractModule () { @Override public void configure () { this . bind ( DB . class ). toInstance ( new Postgres ( \"jdbc:postgresql:5740/main\" ) ); } } ); Some frameworks even allow us to configure the injector in an XML file. From now on, we are not allowed to instantiate Budget through the new operator, like we did before. Instead, we should use the injector we just created: public class App { public static void main ( String ... args ) { Injection injector = // as we just did in the previous snippet Budget budget = injector . getInstance ( Budget . class ); System . out . println ( \"Total is: \" + budget . total ()); } } The injection automatically finds out that in order to instantiate a Budget it has to provide an argument for its constructor. It will use an instance of class Postgres , which we instantiated in the injector. This is the right and recommended way to use Guice. There are a few even darker patterns, though, which are possible but not recommended. For example, you can make your injector a singleton and use it right inside the Budget class. These mechanisms are considered wrong even by DI container makers, however, so let's ignore them and focus on the recommended scenario. What Is This For? Let me reiterate and summarize the scenarios of incorrect usage of dependency injection containers: Field injection Setter injection Passing injector as a dependency Making injector a global singleton If we put all of them aside, all we have left is the constructor injection explained above. And how does that help us? Why do we need it? Why can't we use plain old new in the main class of the application? The container we created simply adds more lines to the code base, or even more files, if we use XML. And it doesn't add anything, except an additional complexity. We should always remember this if we have the question: \"What database is used as an argument of a Budget?\" The Right Way Now, let me show you a real life example of using new to construct an application. This is how we create a \"thinking engine\" in rultor.com (full class is in Agents.java ): Impressive? This is a true object composition. I believe this is how a proper object-oriented application should be instantiated. And DI containers? In my opinion, they just add unnecessary noise. "},{"title":"10 Hosted Continuous Integration Services for a Private Repository","url":"/2014/10/05/ten-hosted-continuous-integration-services.html","tags":["ci","devops"],"date":"2014-10-05 00:00:00 +0000","categories":["jcg"],"body":"Every project I'm working with starts with a setup of continuous integration pipeline. I'm a big fan of cloud services, that's why I was always using travis-ci.org. A few of my clients questioned this choice recently, mostly because of the price. So I decided to make a brief analysis of the market. I configured rultor , an open source project, in every CI service I managed to find. All of them are free for open source projects. All of them are hosted and do not require any server installation Here they are, in order of my personal preference (first four are the best and highly recommended): Linux Windows MacOS Pull requests Log compress travis-ci.com $129/mo appveyor.com $39/mo ? wercker.com free! shippable.com $1/mo codeship.io $49/mo ? semaphoreapp.com $29/mo drone.io $25/mo magnum-ci.com ? ? snap-ci.com $30/mo ? circleci.com $19/mo sonolabs.com $15/mo ? hosted-ci.com $49/mo ? ? ship.io free! ? If you know any other good continuous integration services, email me , I'll review and add them to this list. BTW, here is a \"full\" list of continuous integration software and services. travis-ci.org is the best platform I've seen so far. Mostly because it is the most popular. Perfectly integrates with Github and has proper documentation. One important downside is the price of $129 per month. \"With this money you can get a dedicated EC2 instance and install Jenkins there\" — some of my clients say. I strongly disagree, since Jenkins will require a 24x7 administration, which costs way more than $129, but it's always difficult to explain. appveyor.com is the only one that runs Windows builds. Even though I'm working mostly with Java and Ruby, which are expected to be platform independent, they very often appear to be exactly the opposite. When your build succeedes on Linux, there is almost no guarantee it will pass on Windows or Mac. I'm planning to use appveyor in every project, in combination with some other CI service. I'm still testing it though... wercker.com is a European product from Amsterdam, which is still in beta and that's why free for all projects. The platform looks very promissing. It is still free for private repositories and is backed up by investments . They also have an interesting concept of build \"boxes\", which can be pre-configured similar to Docker containers. It works rather stable for the last few months, no complains so far. shippable.com was easy to configure since it understands .travis.yml out of the box. The user interface is easy to navigate since it doesn't have \"settings\" page at all (or I didn't find it). Everything is configured via shippable.yml file in the repository. The service looks stable and robust, no complains so far. semaphoreapp.com is easy to configure and work with. It makes an impression of a light-weight system, which I generally appreciate. As a downside, they don't have any Maven pre-installed have an old version of Maven, but this was solved easily with a short custom script that downloads and unpacks the latest Maven. Another downside is that they are not configurable through a file (like .travis.yml ) — you should do everything through a UI. They also support caching between builds . codeship.io works fine, but their web UI looks a bit out-dated. Besides that, they promise to work with pull requests, but I didn't manage to configure them. They simply don't notify our pull requests in Github, even though they build them. Maybe I'll find a way, so far it's not clear... magnum-ci.com is a very lightweight and young system. It doesn't connect automatically to Github, so you should do some manual operations of adding a web hook. Besides that, works just fine. snap-ci.com is a product of ThoughtWorks, an author of Go , an open source continuous integration server. It looks a bit more complicated than others, giving you an ability to define \"stages\" and combine them into pipelines. I'm not sure yet how these mechanisms may help in small and medium size projects we're mostly working with, but they look \"cool\". There is also a very unfortunate limitation of 2Gb RAM per build — some of my Java projects fail because of that. Besides that, they don't give full access to the build server, for example we can't modify anything in /etc — it is a show-stopper for us. drone.io works fine, but their support didn't reply to me when I asked for a Maven version update (they have an old version pre-installed). Besides that, their badge is not updated correctly in Github README.md — when the build is broken, the bange stays green... very annoying. circleci.com I still don't know why my build fails there. Really difficult to configure and understand what's going on. Trying to figure it out... zeroci.com looks like a one-man project, which definitely needs usability testing. It was rather difficult to configure a project via its web interface. The good thing is that it's free, but its quality is not high enough to recommend it. ship.io is building only mobile applications (for iOS and Android). Besides that, they don't support Maven for Android apps, only Gradle. I'll try to build iOS Swift app with them soon.... solanolabs.com looks rather immature and difficult to configure. They don't even support automatic Github hook configuration when new repository is added. However, their sales spams me rather aggressively :) hosted-ci.com is for iOS/OSX only. They don't give anything for free, even for open source projects. I didn't have a chance to test them yet. cloudbees.com testing now... dploy.io testing now... BTW, if you don't like the idea of keeping continuous integration in cloud, consider these on-premise software packages (in order or preference): Jenkins , TeamCity , Go , Strider , BuildBot . Keep in mind that no matter how good and expensive your continuous integration service is, it won't help you unless you make your master branch read-only . "},{"title":"Project Lifecycle in Teamed.io","url":"/2014/10/06/software-project-lifecycle.html","tags":["mgmt"],"date":"2014-10-06 00:00:00 +0000","categories":["jcg"],"body":"In addition to being a hands-on programmer, I'm also co-founder and CTO of Teamed.io , a custom software development company. I play the role of a technical and management leader in all projects we work with. I wrote this article for those who're interested in hiring me and/or my team. This article will demonstrate what happens from day one until the end of the project, when you choose to work with us . You will see below that our methods of software development seriously differ from what many other teams are using. I personally pay a lot of attention to quality of code and quality of the internal processes that connect our team. There are four phases in every project I work with in Teamed.io : Thinking . Here we're trying to understand: What is the problem that the product is going to solve? We're also investigating the product's boundaries — who will work with the software (actors) and how will they work with it (user stories). Deliverables: specification. Duration: from 2 days up to 3 weeks. Participants: product owner, analyst(s), architect, project manager. Building . Here the software architect is creating a proof-of-concept (aka an MVP or prototype or a skeleton). It is a one-man job that is done almost without any interaction with anyone else. The architect builds the product according to the specification in a very limited time frame. The result will have multiple bugs and open ends, but it will implement the main user story. The architect also configures continuous integration and delivery pipelines. Deliverables: working software. Duration: 2-5 days. Participants: architect. Fixing . At this phase we are adding all the meat to the skeleton. This phase takes most of the time and budget and involves many participants. In some projects we invite up to 50 people to work, at the same time. Since we treat all inconsistencies as bugs, this phase is mostly about finding, reporting and fixing bugs, in order to stabilize the product and get it ready for market launch. We increment and release the software multiple times a day, preferably to its user champions. Deliverables: bug fixes via pull requests. Duration: from weeks to months. Participants: programmer(s), designer(s), tester(s), code reviewer(s), architect, project manager. Using . At this final phase we are launching the product to its end-users, and collecting their feedback (both positive and negative). Everything they are reporting back to us is being registered as a bug. Then, we categorize the bugs and fix them. This phase may take years, but it never involves active implementation of new features. Deliverables: bug fixes via pull requests. Duration: months. Participants: programmer(s), code reviewer(s), project manager. The biggest (i.e., longest and most expensive) phase is, of course, Fixing. It usually takes the majority of time (over 70%). However, the most important and risky phase is the first one — Thinking. A mistake made during Thinking will cost much more than a mistake made later. Thinking Thinking is the first and the most important phase. First, we give a name to the project and create a Github repository. We try to keep all our projects (both open source and commercial) in Github. Mostly because the platform is very popular, very powerful, and really cheap ( $7/mo for a set of 5 private projects). We also keep all communication in the Github issue tracker. Then, we create a simple half-page SRS document (Software Requirements Specification). Usually this is done right inside the source code, but sometimes in the Github wiki. What's important is that the document should be under version control. We will modify it during the course of the project, very intensively. The SRS should briefly identify main \"actors\" of the system and define the product scope. Even though it is only half a page, the creation of this initial SRS document is the most important and the most expensive task in the entire project. We pay a lot of attention to this step. Usually this document is written by myself in direct communication with the project sponsor. We can't afford a mistake at this step. Then, we invite a few system analysts to the project. These guys are responsible for turning our initial SRS into a more complete and detailed specification. They start by asking questions, submitting them one by one as Github issues. Every question is addressed to the product owner. Using his/her answers, system analysts modify the SRS document. This article explains how Requs helps us in this process: Incremental Requirements With Requs At the end of the Thinking phase we estimate the size of the project, in lines of code. Using lines of code, we can roughly estimate a budget . I stay personally involved in the project during the entire Thinking phase. Building This is a one-man job for an architect. Every project we work on has an architect who is personally responsible for the quality and technical decisions. I try to play this role in most projects. The Building phase is rather straight forward. I have to implement the solution according to the SRS, in a few working days. No matter how big the idea and how massive the planning development, I still have to create (build from scratch!) the product in, say, three days. Besides building the software itself, I have to configure all basic DevOps processes, including: 1) automated testing and quality control, 2) deploying and releasing pipelines, 3) repository of artifacts, 4) continuous integration service, etc. The result of this phase is a working software package, deployable to its destination and available for testers. Technical quality requirements are also defined at this phase. Fixing Now it's time to build a distributed team of programmers. First, we invite those who've worked on other projects and have already have proven their quality. Very often we invite new people, finding them through StackOverflow, Github, oDesk, and other sources. An average team size of an average project is 10-20 programmers. At this phase, we understand any inconsistency as a bug. If something is not clear in the documentation, or if something can be refactored for better readability, or if a function can be improved for higher performance — it is a bug to us. And bugs are welcome in our projects. We encourage everybody to report as many bugs as possible. This is how we achieve high quality. That is why the phase is called Fixing, after all. We are reporting bugs and fixing them. Hundreds of bugs. Sometimes thousands. The product grows in front of our very eyes, because after every bug fix we re-deploy the entire product to the production platform. Every bug is reported, classified, discussed, and fixed in its own Github ticket and its own Git branch. We never allow anyone to just commit to the master branch — all changes must pass through our quality controls and be merged into master by rultor.com , our merging bot . Also important to mention is that all communications with the product owner and between programmers happen only through Github issues. We never use any chats , Skype, emails or conferencing software. We communicate only through tickets and comments in Github. Using This is the final phase and it can take quite a long time. By now, the product is ready and is launched to the market. But we still receive bug reports and feature request from the product owner, and we still fix them through the same process flow as in the Fixing phase. We try to keep this phase as quiet as possible, in terms of the amount of bugs reported and fixed. Thanks to our intensive and pro-active bug finding and fixing in the previous phase, we usually have very few problems at the Using phase. And big feature requests? At this phase, we usually try to convert them into new projects and develop them separately, starting again from Thinking. BTW, the illustrations you see above are made by Bárbara Lopes . "},{"title":"Stop Chatting, Start Coding","url":"/2014/10/07/stop-chatting-start-coding.html","tags":["xdsd"],"date":"2014-10-07 00:00:00 +0000","categories":["jcg"],"body":" The first principle of eXtremely Distributed Software Development ( XDSD ) states that \"everyone gets paid for verified deliverables\". This literally means that, in order to get paid, every programmer has to write the code, commit it to the repository, pass a code review and make sure the code is merged into the destination branch . Only then, is his result appreciated and paid for. For most of my clients this already sounds extreme. They are used to a traditional scheme of paying per hour or per month. They immediately realize the benefits of XDSD, though, because for them this approach means that project funds are not wasted on activities that don't produce results. But that's not all. Barton Fink (1991) by Joel Coen This principle also means that nobody is paid for anything except tasks explicitly assigned to him/her. Thus, when a programmer has a question about current design, specification, configuration, etc. — nobody will be interested in answering it. Why not? Because there is no payment attached to this. Answering questions in Skype or Hipchat or by email is something that is not appreciated in XDSD in any way. The project simply doesn't pay for this activity. That's why none of our programmers do this. We don't use any (I mean it!) informal communication channels in XDSD projects. We don't do meetings or conference calls. We never discuss any technical issues on Skype or by phone. So, how do we resolve problems and share information? We use task tracking systems for that. When a developer has a question, he submits it as a new \"ticket\". The project manager then picks it up and assigns it to another developer, who is able to answer it. Then, the answer goes back through the tracking system or directly into the source code. The \"question ticket\" gets closed when its author is satisfied with the answer. When the ticket is closed, those who answered it get paid. Using this model, we significantly improve project communications, by making them clean and transparent. We also save a lot of project funds, since every hour spent by a team member is traceable to the line of code he produced. You can see how this happens in action, for example, in this ticket (the project is open source; that's why all communications are open): jcabi/jcabi-github#731 . One Java developer is having a problem with his Git repository. Apparently he did something wrong and couldn't solve the problem by himself. He asked for help by submitting a new bug to the project. He was paid for the bug report. Then, another team member was assigned to help him. He did, through a number of suggestions and instructions. In the end, the problem was solved, and he was also paid for the solution. In total, the project spent 45 minutes, and the problem was solved. "},{"title":"Continuous Integration is Dead","url":"/2014/10/08/continuous-integration-is-dead.html","tags":["mgmt","devops"],"date":"2014-10-08 00:00:00 +0000","categories":["jcg","best"],"body":"A few days ago, my article \"Why Continuous Integration Doesn’t Work\" was published at DevOps.com . Almost the same day I received a few strongly negative critiques on Twitter. Here is my response to the un-asked question: Why the hell shouldn't continuous integration work, being such a brilliant and popular idea? Even though I have some experience in this area, I won't use it as an argument. I'll try to rely only on logic instead. BTW, my experience includes five years of using Apache Continuum, Hudson, CruiseControl, and Jenkins in over 50 open source and commercial projects. Besides that, a few years ago I created a hosted continuous integration service called fazend.com , renamed to rultor.com in 2013. Currently, I'm also an active user of Travis and AppVeyor . How Continuous Integration Should Work The idea is simple and obvious. Every time you make a new commit to the master branch (or /trunk in Subversion), a continuous integration server (or service) attempts to build the entire product. \"Build\" means compile, unit test, integration test, quality analysis, etc. The result is either \"success\" or \"failure\". If it is a success, we say that \"the build is clean\". If it is a failure, we say that \"the build is broken\". The build usually gets broken because someone breaks it by commiting new code that turns previously passing unit tests into failing ones. This is the technical side of the problem. It always works. Well, it may have its problems, like hard-coded dependencies, lack of isolation between environments or parallel build collisions, but this article is not about those. If the application is well written and its unit tests are stable, continuous integration is easy. Technically. Let's see the organizational side. Continuous integration is not only a server that builds, but a management/organizational process that should \"work\". Being a process that works means exactly what Jez Humble said in Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation , on page 55: Crucially, if the build fails, the development team stops whatever they are doing and fixes the problem immediately This is what doesn't work and can't work. Who Needs This? As we see, continuous integration is about setting the entire development team on pause and fixing the broken build. Let me reiterate. Once the build is broken, everybody should focus on fixing it and making a commit that returns the build to the stable state. Now, my question is — who, in an actively working team, may need this? A product owner, who is interested in launching new features to the market as soon as possible? Or maybe a project manager, who is responsible for the deadlines? Or maybe programmers, who hate to fix someone else's bugs, especially under pressure. Who likes this continuous integration and who needs it? Nobody. What Happens In Reality? I can tell you. I've seen it multiple times. The scenario is always the same. We just start to ignore that continuous integration build status. Either the build is clean or it is broken, and we continue to do what we were doing before. We don't stop and fix it, as Jez Humble recommends. Instead, we ignore the information that's coming from the continuous integration server. Eventually, maybe tomorrow or on Monday, we'll try to find some spare time and will try to fix the build. Only because we don't like that red button on the dashboard and want to turn it into a green one. What About Discipline? Yes, there is another side of this coin. We can try to enforce discipline in the team. We can make it a strict rule, that our build is always clean and whoever breaks it gets some sort of a punishment. Try doing this and you will get a fear driven development . Programmers will be afraid of committing anything to the repository because they will know that if they cause a build failure they will have to apologize , at least. A strict discipline (which I'm a big fan of) in this case only makes the situation worse. The entire development process slows down and programmers keep their code to themselves for as long as possible, to avoid possibly broken builds. When it's time to commit, their changes are so massive that merging becomes very difficult and sometimes impossible. As a result you get a lot of throw-away code, written by someone but never committed to master , because of that fear factor. OK, What Is The Solution? I wrote about it before; it is called \"read-only master branch\" . It is simple — prohibit anyone from merging anything into master and create a script that anyone can call. The script will merge, test, and commit. The script will not make any exceptions. If any branch breaks at even one unit test, the entire branch will be rejected. In other words: raise the red flag before the code gets into master . This solves all problems. First, the build is always clean. We simply can't break it because nobody can commit unless his code keeps the build clean. Second, there is no fear of breaking anything. Simply because you technically can't do it. All you can do is get a negative response from a merging script. Then you fix your errors and tell the script to try again. Nobody sees these attempts, and you don't need to apologize. Fear factor is gone. BTW, try to use rultor.com to enforce this \"read-only master branch\" principle in your project. "},{"title":"What Does a Software Architect Do?","url":"/2014/10/12/who-is-software-architect.html","tags":["mgmt","architect"],"date":"2014-10-12 00:00:00 +0000","categories":["jcg"],"body":"Do you have a software architect in your project? Do you need one? Well, most agile teams do not define such a role explicitly and work in a democratic mode. Every important technical decision is discussed with the entire team, and the most voted for solution wins. When such a team eventually decides to put a \"software architect\" badge on someone's t-shirt, the most reputable programmer gets it. The badge rarely changes his responsibilities, though. After all, the team stays the same and enjoys having technical discussions together, involving everyone. In the end, a software architect is more of a status than a role with explicitly defined responsibilities. It is a sign of respect, paid by other team players to the oldest and the most authoritative one among them. Right? Absolutely wrong! Obviously, an architect is usually someone who has the most knowledge, skills, experience, and authority. Of course, an architect usually knows more than others and is able to communicate his knowledge with diplomacy and pedagogy when required. An architect is usually one of the smartest guys on the team. This is not, however, what makes him/her an architect. And this is not what the team needs. My definition of a software architect is this: An architect is the one who takes the blame for the quality . You can replace \"blame\" with accountability or responsibility. Although, I prefer to use \"blame\", because it much better emphasizes the fact that every quality issue in the product under development is a personal fault of the architect. Of course, together with the blame he also takes all the credit from happy customers, when the quality is good. This is what the team needs — someone personally responsible for the quality of the software being developed. How this guy will delegate this responsibility to others is his job. Whether he will use his knowledge and skills, or quality control tools, or unit testing frameworks, or authority, or coaching, or corporal punishment — it's his business. A project manager delegates quality control to the software architect, and it is up to the software architect how to delegate it further. The role of a software architect is crucial for every project, even if there are just two coders working at the same desk. One of them has to be the architect. An ideal architect has all the merits mentioned above. He listens to everybody and takes their opinions into account. He is a good coach and a teacher, with a lot of patience. He is an effective communicator and negotiator. He is a diplomat. And he is an expert in the technical domain. But, even if he doesn't have all these merits, his decision is always final . And this is the job of the project manager, to make sure that every technical decision the architect makes is not doubted by anyone. This is what delegation is all about — responsibility should always come with power. As a project manager, you should regularly evaluate the results of your architect. Remember, the quality of the product your team is working on is his personal (!) responsibility. Any problems you see are his problems. Don't be afraid to blame him and punish him. But, always remember that in order to make your punishments productive you should give your architect full power in his actions. Let me reiterate: his decisions should be final. If you, as a project manager, are not happy with the quality of the product and the architect doesn't improve the situation, replace him. Downgrade him to a programmer and promote one of the programmers to an architect. But always remember that there can only be one architect in the team, and that his decisions are final. That's the only way of having a chance of building a perfect product. "},{"title":"How We Write a Product Vision","url":"/2014/10/20/how-we-write-product-vision.html","tags":["mgmt"],"date":"2014-10-20 00:00:00 +0000","categories":[],"body":" Every software project we work with is started from a Product Vision document. We create it during our Thinking phase. Even though the document is as short as two pages of English text, its development is the most painstaking task in the whole project. There are a few tricks and recommendations which I'd like to share. We usually design a Product Vision in four sections: product statement, stakeholders and needs, features, and quality requirements. Product Statement Product Statement is a one-paragraph declaration of intent, explaining to an absolute stranger what this product is about and what it is for. It is very similar to an elevator pitch . The Statement must answer these questions, preferrably in this specific order: Who is the customer? What does she want? What is the market offering now? What is wrong with existing offers? How will our product fix this? You should answer all these questions in less than 60 words altogether. If you need more words, something is wrong with your understanding of the product under development. If you can answer them in 20 words, your product will conquer the world. By the way, don't confuse a Product Statement with a Mission , which is a much broader declaration of an overall goal of your business. You may have a hundred products but only a single mission. For example, Disney says that its mission is: \"to make people happy\". They've got hundreds of products that help them accomplish this mission. And each product has its own Product Statement. I find these articles helpful: The Product Vision , Agile Artifacts: The Product Vision Statement , The Art of Agile Development: Vision . Stakeholders and Needs This section must list everybody whose life will be affected by the product (positively or negatively). Your list of stakeholders may include: sponsors, developers, users, competitors, government, banks, web hosting providers, Apple Store, hackers, etc. It is very important to list both positive and negative stakeholders. If your product is going to automate some routine manual operations, don't forget that someone will be made redundant because of it. No matter how \"good\" your product is, there is always an \"evil\" side. The invention of the iPhone made millions of people happy, but also caused a lot of trouble for Nokia and Blackberry. An eventual invention of a cancer vaccine will make millions of people healthier, but will also make thousands of oncologists jobless. My point is that any project has both positive and negative stakeholders. Each stakeholder must have a list of needs. They have to be simple and straight forward, like \"earn money\", \"increase profit\", \"share photos\", or \"host a website\". I would recommend defining one or two needs for each stakeholder. If there are more than three, think again — do you really understand what your stakeholders need? Your project will be considered successful if you satisfy all the needs of all your positive stakeholders and neutralize negative ones. This Stakeholder Needs and Requirements article from SEBOK will be helpful. Actors and Features In this section we list actors (entities communicating with the product) and the key functionalities they use. This is the most abstract definition of functional requirements of the product. It doesn't need to be detailed. Instead, it has to be very high-level and abstract. For example, this is how our interaction with a well-known product may be described in two lines: User can post tweets, read tweets of his friends, follow new friends and re-tweet their tweets. Is it clear for a stranger what we're talking about here? Absolutely not — what is a \"tweet\", what does it mean to \"follow\" and what is a \"re-tweet\"? These questions have no answers in the Product Vision document, but it's clear that a user will have four main features available. All other features will be similar to those. Twitter is a multi-billion dollar business with a multi-million dollar product. However, we managed to explain its key features in just two lines of text. You should do the same with your product. If you can't fit all its features into just two-three lines, reconsider your understanding of the product you're going to develop. Also, read about \"feature bloat dilemma\" . Each actor must have at least three and at most six features. If there are more, you should group them somehow. If there are less, break them into smaller and more detailed features. Quality Requirements This section lists all important non-functional requirements. Any product may have hundreds of quality requirements , as well as hundreds of features. However, a Product Vision document must be focused on the most important ones. Consider some examples: Any web page must open in less than 300ms. Total cost of ownership must be less than $5000/mo. Mobile app must be tailored for 10+ popular screen sizes. Mean time to recover must be less than 2 hours. DB must be scalable up to 5Tb without cost increases. It is also very important to keep requirements measurable (like each of these examples). Every line in this section is a message to product developers. They will read this document in order to understand what is most important to the sponsor of the project. For example, these quality requirements are useless: \"user interface must be attractive\", \"web site must be fast\" or \"the system must be stable\". They are not measurable or testable. All they do is distract developers. If you can't make a strict and measurable statement about your quality objectives, don't write anything. It's better to say nothing than set false or ambiguous goals here. Try to keep this section short. There should be six quality requirements, at most. Remove Noise Every section must be no more than twenty lines in length. Even if you're developing a Google killer with a $50 million dollar budget, your Vision document must be as short as two pages. For most of my clients this is a very complex and brain damaging task. They usually come to us with a 50-page document explaining their business ideas with all the important details. From this document, we should only extract information that really makes a difference. The Product Vision document must keep its reader on the highest level of abstraction. The document must take less than a minute to read, from start to finish. If you can't keep it short — you don't understand your product well enough. Example Here is an example of a very simple Product Vision for a Facebook killer: Statement Facebook doesn't allow users to purchase \"likes\", our social network will have this. Stakeholders and Needs Sponsor: to raise investments. Developer: to earn money by programming. Users: to share photos and purchase popularity. Bank: to make commission on every purchase. Government: to protect society against abusive content. Competitors: to wipe us off the market. Actors and Features User can create account, upload photos, share photos, send personal messages, like other photos, purchase likes. Admin can ban user accounts, view summary reports, authorize credit card transactions, configure system parameters, monitor server resource usage. Bank can process credit card transactions. Quality Requirements Any page must open in less than 300ms. User interface must be attractive and simple. Availability must be over 99.999%. Test coverage must be over 80%. Development pipeline must be fully automated. Interfaces must include web site and iOS/Android app. Diplomacy We follow all these recommendations in our projects, in teamed.io . You can use them in your projects as well, but keep in mind that the process of defining a Product Vision could be very painful. You may sometimes offend a client by over-simplifying their \"great\" business idea. 'Really? I'm ready to pay $250,000 for something awesome and you're telling me that you've only got ten lines for it? Huh?' To work around this situation, split the client's documentation into two parts. The first part will fit into the Product Vision document; the second one will be called \"supplementary documentation\" and will contain all that valuable information you've got from the client. You may use that documentation later, during the course of product development. But don't cut corners. Don't allow your client (or anyone else) to force you to bloat the Product Vision. The document has to be very short and explicit. No lyrics, only statements. "},{"title":"Incremental Billing","url":"/2014/10/21/incremental-billing.html","tags":["xdsd","mgmt"],"date":"2014-10-21 00:00:00 +0000","categories":[],"body":"When you hire a software developer (individual or a team), there are basically two types of contracts: fixed price or time-and-material . They are fundamentally different but the truth is that in either case — you lose . In the eXtremely Distributed Software Development ( XDSD ) methodology everything is different, including the way we invoice our clients. Let's see what happens in traditional contracts and what changes in XDSD, which we practice in Teamed.io . The difference between fixed-cost and T&M is in who takes the risk of spending money and getting nothing in return. This risk is huge in the software development industry, especially in outsourcing. Over 80% of all software projects fail to achieve their objectives and about 30% of startups fail by running out of cash. However, very few programmers (if any) fail to get their monthly salaries on time. What does this tell us? I guess it means that in all failures you — the client — will be the loser. Time and Material In T&M you will simply pay and pray. If your programmers appear to be honest workaholics you may get lucky and get something done. As you can see from the numbers above, however, this is rarely the case. Don't fool yourself; there won't be any workaholics in your project. Even if you adopt micro-management and corporal punishment, your overall costs will be much higher than expected and the quality will suffer. This is what a monthly T&M invoice will look like. You will pay for the time spent by programmers pretending to be working on your project. Well, as I said above, some of them will ocassionally do something useful, but overall statistics tell us that most of that time will be wasted. No matter how good or bad the code written during that month — you still have to pay the bill. How many more invoices you will get until the product is done? Nobody knows. In the end — you lose. Fixed Price In Fixed Price you will feel secure at the beginning — \"the statement of work specifies everything and the price is fixed, how can I lose?\" According to the statistics above, however, programmers are much smarter than their clients. You will lose in quality. Yes, you will get something for that fixed price, but it will be a throw-away software. And when you decide to modify it, new costs will bubble up. In the end, the whole project will be ruined and your money will simply be turned into programmers' salaries. This model is even more risky than T&M, where you at least have a chance. Once in a while you will receive an invoice with a list of milestones reached. Every milestone will contain a certain set of features implemented in the product. Keep in mind that the primary motivation of your programmers will be to do less and charge more. Every time you ask for improvements or corrections, there will be a fight about budget. You will either give up and lose a lot of money or your team will significantly jeopardize quality, in order to stay profitable. In either case — you lose. Incremental Billing So, what is the solution? Is it possible to have win-win contracts with programmers? Yes, it is. We call it \"Incremental Billing\". Remember, in XDSD we work with a stream of micro-tasks, usually completed in less than an hour. Each completed task produces a new increment (aka a \"release\" or \"version\") of software. An increment could be a bug fix, a bug report, a new feature or a micro-step towards any of these. By the end of a week you get a bill that lists every single increment delivered during the week, the amount of time spent on its development and its total cost. Every increment costs you 30-60 minutes of a programmer's time (plus our fees). Besides that, by the end of the week, you get an updated version of a project plan, with a re-estimated budget. Thus, you see what was done so far and how much needs to be done, according to our estimate. How does this help you not lose/waste money? Here's how: you fully control your budget you pay only for the work completed you track the progress with few-minutes-granularity you don't pay for meetings, chats, lunches or coffee breaks programmers stay very motivated, since they are paid by result there is no long-term commitment, and you can stop at any time every increment passes all quality checks As you can see, XDSD methodology not only improves the way we develop software but also fixes the flaws in the way you pay for it. Since it is a win-win model, it is beneficial for both programmers and for you — the paying project sponsor. "},{"title":"Paired Brackets","url":"/2014/10/23/paired-brackets-notation.html","tags":["java","oop","programming"],"date":"2014-10-23 00:00:00 +0000","categories":[],"body":"Here is a notation rule I'm using in Java code: a bracket should either start/end a line or be paired on the same line . The notation applies universally to any programming language (incl. Java, Ruby, Python, C++, PHP, etc.) where brackets are used for method/function calls. Here is how your code will look, if you follow this \"Paired Brackets\" notation: 1 2 3 4 5 6 7 8 9 10 11 12 new Foo ( // ends the line Math . max ( 10 , 40 ), // open/close at the same line String . format ( \"hello, %s\" , new Name ( Arrays . asList ( \"Jeff\" , \"Lebowski\" ) ) ) // starts the line ); Obviously, the line with a closing bracket should start at the same indentation level as the line with its opening pair. This is how your IDE will render the code if you follow this notation (IntelliJ IDEA): Sublime Text will also appreciate it: As you see, those light vertical lines at the left side of the code help you to navigate, if you follow the notation. Those multiple closing brackets may look strange to you at the beginning — but give yourself some time and you will get used to them :) Fluent This is how I would recommend formatting fluent method calls (this is Java in NetBeans ): Arrays Here is how you format an array in \"Paired Brackets\" notation (this is Ruby in RubyMine ): As you see, the same principle applies to square and curled brackets. JSON The same principle is applicable to JSON formatting. This is a small JSON document in Coda 2 : JavaScript JavaScript should also follow the same principle. This is how your .js code would look in Atom : Python Finally, here is Python in PyCharm : "},{"title":"Are You a Hacker or a Designer?","url":"/2014/10/26/hacker-vs-programmer-mentality.html","tags":["mgmt","programming"],"date":"2014-10-26 00:00:00 +0000","categories":[],"body":"Twenty years ago, the best programmer was the one capable of fitting an entire application into a 64Kb .COM file. Those who were able to get the most out of that poor Intel 80386 were the icons of programming. That's because twenty years ago computers were expensive and programmers were cheap. That was the time of the \"hacker mentality\". That time is over. That mentality is not appreciated any more, because the market situation is completely opposite. Today, computers are cheap and programmers are expensive. This is the era of the \"designer mentality\", when the readability of our code is much more important than its performance. Prices vs Salaries Look at this graph. It is a comparison of two trends over the last twenty years (1994-2014). The first trend falls down and shows how much cheaper computer memory and HDD storage have become over the last twenty years. The second trend demonstrates how much software developers' salaries escalated over the same period. More accurately, they tripled. I didn't find an official report about that, but I'm sure it's no secret to anyone that the salaries of programmers keep growing — $200,000 per year for a senior developer is not a dream any more... while twenty years ago $60K was the best offer around. I found this article very interesting about this very subject. Basically, this means that in order to create a PHP website in 1994 we had to spend 1000 times more on hardware and three times less on programmers than we do now, in 2014. And we're talking about the same stack of technologies here. The same Linux box with an Apache HTTP Server inside. The difference is that in 1994, if our application had performance problems because of hardware limitations, we paid $35,000 per each additional gigabyte of RAM, while in 2014 we pay $10. In 1994 it was much more efficient to hire more programmers and ask them to optimize the code or even rewrite it, instead of buying new hardware. In 2014 the situation is exactly the opposite. It is now much cheaper to double the size of the server (especially if the server is a virtual cloud one) instead of paying salaries for optimizing the software. In 1994 the best engineers had that \"hacker mentality\", while in 2014 the \"designer mentality\" is much more appreciated. Hacker Mentality Someone with a hacker mentality would call this Fibonacci Java method an \"elegant code\" (would you?): public int f ( int n ) { return n > 2 ? f ( n - 1 )+ f ( n - 2 ): 1 ; } I would highlight these qualities of a good hacker : uses all known (and unknown) features of a programming language discriminates others as hackers and newbies and writes for hackers gets bored and frustrated by rules and standards doesn't write unit tests — juniors will write them later enjoys fire-fighting — that's how his talent manifests prefers talks over docs, since they are much more fun hates to see his code being modified by someone else likes to dedicate himself to one project at a time A hacker is a talented individual. He wants to express his talent in the software he writes. He enjoys coding and does it mostly for fun. I would say, he is married to his code and can't imagine its happy life after an eventual divorce. Code ownership is what a hacker is about — he understands himself as an \"owner\" of the code. When I ask one of my hacker friends — \"How will someone understand what this code does?\" I almost always hear the same answer — \"They will ask me!\" (usually said very proudly, with a sincere smile). Designer Mentality Someone with a designer mentality would refactor the code above to make it easier to read. He would call this Java function an \"elegant code\" (how about you?): public int fibo ( final int pos ) { final int num ; if ( pos > 2 ) { num = fibo ( pos - 1 ) + fibo ( pos - 2 ); } else { num = 1 ; } return num ; } I think these qualities can be attributed to a good designer: tends to use traditional programming techniques assumes everybody is a newbie and writes accordingly enjoys setting rules and following them prefers docs over talks and automation over docs spends most of his coding time on unit tests hates fire-fighting and working over time loves to see his code being modified and refactored works with a few projects at the same time A designer is a talented team player. He contributes to the team processes, standards, rules, education, and discipline as much as he contributes to the source code. He always makes sure that once he leaves the project his code and his ideas stay and work. The highest satisfaction for a good designer is to see his code living its own life — being modified, improved, refactored and eventually retired. A designer sees himself as a parent of the code — once it is old enough to walk and talk, it has to live its own life. The Future If you consider yourself a hacker, I believe it's time to change. The time of hackers is fading out. In the near future we will probably even stop thinking in terms of \"hardware\" and will run our applications in elastic computational platforms with unlimited amounts of memory, CPU power and storage space. We will simply pay for resource utilization and almost any performance issue will just add a few extra dollars to our monthly bills. We won't care about optimization any more. At the same time, good software engineers will become more and more expensive and will charge $500+ per hour just to check out software and give a diagnosis. Just like good lawyers or dentists. That's why, while developing a new software product, those who pay for it will care mostly about its maintainability. Project sponsors will understand that the best solution they can get for their money is the one that is the most readable, maintainable, and automated. Not the fastest. "},{"title":"How Much Do You Cost?","url":"/2014/10/29/how-much-do-you-cost.html","tags":["mgmt"],"date":"2014-10-29 00:00:00 +0000","categories":[],"body":" I'm getting a few emails every day from programmers interested in working with Teamed.io remotely. The first question I usually ask is \"what is your rate?\" (we pay by the hour ) What surprises me is how often people incorrectly estimate themselves, in both directions. I hear very different numbers, from $5 to $500 per hour. I never say no, but usually come up with my own hourly rate estimate. This article explains what factors I do and don't take into account. These are my personal criteria; don't take them as an industry standard. I do find them objective and logical, though — so let me explain. Open Source Contribution This is the first and the most important characteristic of a software developer. Do you contribute to open source projects? Do you have your own open source libraries that are used by some community? Do you write code that is publicly available and used? If you have nothing to show here, I see three posible causes. First, you're too shy to share your code because it's crap . Obviously, this is not a good sign. Not because your code could be bad, but because you're not brave enough to face this fact and improve. In our teams we pay a lot of attention to the quality of code and most of our new team members get surprised by just how high our quality bar is. You will also be surprised. The question is whether you will be able to adapt and improve or if you will give up and quit. If you didn't share your code before and have never dealt with negative feedback, you won't feel comfortable in our projects, where quality requirements are very high. The second possible cause is that you work from nine till five, for food, without passion . Actually, nobody manifests it that way. Instead, I often hear something like \"my company doesn't pay me for open source contribution and at home I want to spend time with my family\". In modern software development, most of the code we're working with is open source — libraries, frameworks, tools, instruments, etc. Almost everything you're using in your commercial projects is open source. By paying your salary your employer does already invest in open source products, because you're an active user of them. The problem is that you are not interested in becoming more active in that contribution. I see this as a lack of passion and self-motivation. Will you be an effective developer in our projects? Not at all, because our entire management model relies on self-motivation . The last possible cause is that you don't know what to write and where to contribute, which means lack of creativity . As I mentioned above, almost everything we're using now is open source, and these tools are full of bugs and not-yet-implemented features. At the same time, you don't see any areas for improvement? You don't know what can be done better? You're not able to at least find, report and fix one bug in some open source product you're using every day? This means that you won't be able to find areas of improvement in our projects either, while we rely on your ability to discover problems creatively . Thus, if your Github account is empty and your CV doesn't position you as \"an active contributor to Linux kernel\" (yeah, why not?), I immediately lose interest. On the other hand, when I see a 100+ stars project in your Github account, I get excited and ready to offer a higher rate. Geographic Location It is a common practice to pay higher rates to those who live in more expensive countries. When I'm getting resumes from San Francisco programmers, their rates are $70+ per hour. The same skills and experience cost $15-20 in Karachi. The reason here is the cost of living — it is much higher in the US than in Pakistan. However, this reason doesn't sound logical to me. If you're driving a more expensive car, we have to pay you a higher salary? The same with the place to stay. You've chosen the country that you live in. You're using all the benefits of a well-developed country and you're paying for them. It's your choice. You decided to spend more money for the quality of your life — what does it have to do with me? Want to pay $30 for a lunch? Become a better engineer. Until then, buy a hot dog for a few bucks. Just saying that \"I'm already here and my lunch costs $30\" is not an argument. Thus, the more expensive the place you live, the less money stays in your pocket. For us this means that $100 will motivate a programmer from Karachi much stronger than the same $100 will motivate the same person, if she lives in San Francisco. Thus, we prefer to work with people whose expenses are lower. Our money will simply work better. StackOverflow.com Reputation We all know what StackOverflow has but very few people (suprisingly few!) actively contributing to it. If your profile there is empty (or you don't have one) I realize that you 1) don't have any questions to ask and 2) you have nothing to answer. First, if you're not asking anything there, you are not growing. Your education process stopped some time ago, probably right after you got an office job. Or maybe you're too shy to ask? Or you can't describe your questions in an accurate and precise format? Or maybe all your questions already have answers? In any case, it's sad. Second, if you're not answering, you simply have nothing to say. In most cases, this means that you're not solving complex and unique problems. You're simply wiring together well-known components and collecting your paychecks. Very often I hear people saying that they solve most of their problems by asking their colleagues sitting next to them in the office. They say they simply don't need StackOverflow (or similar resources, if they exist) because their team is so great that any questions can be answered internally. That's good for the team and bad for you. Why? You don't have a very important skill — finding an answer in a public Internet. In our projects we discourage any horizontal communications between programmers, and you won't be able to get any help from anyone. You will be on your own and you will fail, because you are used to patronizing someone senior, in your office. StackOverflow is not just an indicator of how smart you are and how many upvotes you got for the \"best programming joke\" . It is proof that you can find answers to your questions by communicating with people you don't know. It is a very important skill. Years of Experience \"I've written Java for 10 years!\" — so what? This number means only one thing to me — you managed to survive in some office for ten years. Or maybe in a few offices. You managed to convince someone that he has to pay you for ten years of sitting in his building. Does it mean that you were writing something useful? Does it mean that your code was perfect? It doesn't mean any of that. Years of experience is a false indicator. It actually may play against you, in combination with other indicators mentioned above. If your CV says that you just started to program two years ago and your Github and StackOverflow accounts are empty — there is still a chance you will improve. You're just in the beginning of your career. However, if your CV says that you're a \"10-year seasoned architect\" with zero open source contribution — this means that you're either lying about that ten years or you're absolutely useless as an architect. My point is that the \"years of experience\" argument should be used very carefully. Play this card only if you have other merits. Otherwise, keep it to yourself. Certifications Oracle, Zend, Amazon, IBM, MySQL, etc. — I'm talking about these certifications. In order to get them you should pass an exam. Not an easy one and not online. It is a real exam taken in a certification center, where you're sitting in front of a computer for a few hours, without any books or Internet access, answering questions. Rather humiliating activity for a respected software developer? Indeed. And there is a high probability of failure, which is also rather embarassing. It is a very good sign, if you managed to go through this. If you've done it a few times, even better. However, if you've earned no certifications in your entire career, it is for one of the following reasons: First, you're afraid to lose . A serious certification may cost a few hundred dollars (I paid over $700 for SCEA ) and you will not get a refund if you fail. If you're afraid to lose, you're afraid to fight. This means you'll chicken out in a real-life situation, where a complex problem will need to be solved. Second, you don't invest in your profile. This most probably means that you don't want to change companies and prefer to find a peaceful office, where you can stay forever. I remember I once said to a friend of mine — \"you will greatly improve your CV if you pass this certification\". He answered with a smile — \"I hope I won't need a CV any more, I like this company\". This attitude is very beneficial for the company you're working for, but it definitely works against you. In my experience, the best team players are those who work for themselves. Healthy individualism is a key success factor. If your primary objective is to earn for yourself (money, reputation, skills, or knowledge) — you will be very effective in our projects. Certifications in your profile is an indicator of that healthy individualism we're looking for. Skills Variety The more technologies or programming languages you know, the less you cost. I'm not saying that it's not possible to be an expert in many things at the same time — that's entirely possible. But let me give you a pragmatic reason why you shouldn't — competition. There are thousands of \"Java7 programmers\" on the market — we can easily choose whoever we need. But there are not so many \"Hadoop programmers\" or \"XSLT designers\". If you focus on some specific area and become an expert there, your chances of finding a job are lower, but the payout will be bigger. We usually end up paying more to narrow-skilled specialists, mostly because we have no choice. If a project we're working on needs a Lucene expert, we'll find the right person and do our best to get him/her on board. Doing our best means increasing the price, in most cases. Thus, when I hear that you're \"experienced in MySQL, PostgreSQL, Oracle and Sqlite\" I realize that you know very little about databases. Talks and Publications I think it is obvious that having a blog (about programming, not about your favorite cat) is a positive factor. Even better is to be an occasional speaker at conferences or meetups. When it is a blog, I pay attention to the amount of comments people leave for your articles. If it is a conference, the most important criteria is how difficult it was to get to the list of speakers. Both blog articles and conference presentations make you much more valuable as a specialist. Mostly because these things demonstrate that some people already reviewed your work and your talent. And it was not just a single employer, but a group of other programmers and engineers. This means that we also can rely on your opinions. Besides that, if you write and present regularly, you have a very important skill/talent — you can present your ideas in a \"digestable\" way. In our projects we discourage informal communications and use ticketing systems instead. In those tickets you will have to explain your ideas, questions or concerns so that everybody can understand you. Without enough presentation skills, you won't survive in your projects. BTW, some software developers even file patents in their names — why can't you do this? Previous Employment I usually don't pay much attention to this section of your CV. Our management model is so different from anything you can see anywhere else that it doesn't really matter how many times you were fired before and how senior of a position you have/had with your full-time employer. Even if your title is \"CTO of Twitter\" — it doesn't mean anything to me. My experience tells me that the bigger the company and the higher the position in it — the further away you stay from the source code and from real technical decisions. VPs and CTOs spend most of their time on management meetings and internal politics. I'm much more interested in what you've done over the last few years than in where you've done it and what they called you while you were doing it. Education BSc, MSc, PhD... do we care? Not really. Education is very similar to the \"previous employment\" mentioned above. It doesn't really matter where exactly you've spent those five years after school. What matters is what have you done during that time. If you have nothing to say about your activity in the university than what will the name of it tell me? Well, of course, if it is Stanford or MIT, this will make a difference. In this case I can see that you managed to pass their graduation standards and managed to find money to study there. This is a good sign and will definitely increase your hourly rate. But if it is some mambo-jambo university from nowhere (like the one I graduated from), keep this information to yourself. Rates $100+ per hour we gladly pay to an expert who owns a few popular open source products; has a StackOverflow score above 20K; has certifications, articles, presentations, and maybe even patents. $50+ per hour we pay to a professional programmer who has open source projects on his own or is an active contributor; has a StackOverflow score over 5K; is writing about software development; possesses a few certifications. $30+ per hour we pay to a programmer who regularly contributes to open source code; is present in StackOverflow; has some certifications. $15 per hour we pay to everybody else. Don't get me wrong and don't take these numbers personally. The rate you're getting is a measurable metric of your professional level, not of you as a person. Besides, the level is not static, it is changing every day, and it's entirely in your hands. I wrote this article mostly in order to motivate you to grow. All these criteria are applicable to new members of our teams. Once you start writing some code, we measure your performance and you may get completely different numbers, see How Hourly Rate Is Calculated BTW, illustrations you see above are created by Andreea Mironiuc. "},{"title":"An Empty Line is a Code Smell","url":"/2014/11/03/empty-line-code-smell.html","tags":["design"],"date":"2014-11-03 00:00:00 +0000","categories":[],"body":"The subject may sound like a joke, but it is not. An empty line, used as a separator of instructions in an object method, is a code smell . Why? In short, because a method should not contain \"parts\". A method should always do one thing and its functional decomposition should be done by language constructs (for example, new methods), and never by empty lines. Look at this Java class (it does smell, doesn't it?): final class TextFile { private final File file ; TextFile ( File src ) { this . file = src ; } public int grep ( Pattern regex ) throws IOException { Collection < String > lines = new LinkedList <>(); try ( BufferedReader reader = new BufferedReader ( new FileReader ( this . file ))) { while ( true ) { String line = reader . readLine (); if ( line == null ) { break ; } lines . add ( line ); } } int total = 0 ; for ( String line : lines ) { if ( regex . matcher ( line ). matches ()) { ++ total ; } } return total ; } } This method first loads the content of the file. Second, it counts how many lines match the regular expression provided. So why does method grep smell? Because it does two things instead of one — it loads and it greps. If we make a rule, to avoid empty lines in method bodies, the method will have to be refactored in order to preserve the \"separation of concerns\" introduced by that empty line: final class TextFile { private final File file ; TextFile ( File src ) { this . file = src ; } public int grep ( Pattern regex ) throws IOException { return this . count ( this . lines (), regex ); } private int count ( Iterable < String > lines , Pattern regex ) { int total = 0 ; for ( String line : lines ) { if ( regex . matcher ( line ). matches ()) { ++ total ; } } return total ; } private Iterable < String > lines () throws IOException { Collection < String > lines = new LinkedList <>(); try ( BufferedReader reader = new BufferedReader ( new FileReader ( this . file ))) { while ( true ) { String line = reader . readLine (); if ( line == null ) { break ; } lines . add ( line ); } return lines ; } } } I believe it is obvious that this new class has methods that are much more cohesive and readable. Now every method is doing exactly one thing, and it's easy to understand which thing it is. This idea about avoiding empty lines is also applicable to other languages, not just Java/C++/Ruby, etc. For example, this CSS code is definitely begging for refactoring: .container { width : 80% ; margin-left : auto ; margin-right : auto ; font-size : 2em ; font-weight : bold ; } The empty line here is telling us (screaming at us, actually) that this .container class is too complex and has to be decomposed into two classes: .wide { width : 80% ; margin-left : auto ; margin-right : auto ; } .important { font-size : 2em ; font-weight : bold ; } Unfortunately, using empty lines to separate blocks of code is a very common habit. Moreover, very often I see empty blocks of two or even three lines, which are all playing this evil role of a separator of concerns. Needless to say, a properly designed class must have just a few public methods and a properly designed method must have up to ten instructions (according to Bob Martin). Empty lines inside methods encourage us to break this awesome rule and turn them into multi-page poems. Of course, it's easier to just click enter a few times and continue to code right in the same method, instead of thinking and refactoring first. This laziness will eventually lead to code that is hardly maintainable at all. To prevent this from happening in your projects, stop using empty lines inside methods, completely. Ideally, prohibit them in your automated build. In qulice.com , a static analysis tool we're using in all Java projects, we created a custom Checkstyle check that prohibits empty lines in every method. "},{"title":"How Immutability Helps","url":"/2014/11/07/how-immutability-helps.html","tags":["jcabi","java","oop"],"date":"2014-11-07 00:00:00 +0000","categories":["jcg"],"body":"In a few recent posts, including \"Getters/Setters. Evil. Period.\" , \"Objects Should Be Immutable\" , and \"Dependency Injection Containers are Code Polluters\" , I universally labelled all mutable objects with \"setters\" (object methods starting with set ) evil. My argumentation was based mostly on metaphors and abstract examples. Apparently, this wasn't convincing enough for many of you — I received a few requests asking to provide more specific and practical examples. Thus, in order to illustrate my strongly negative attitude to \"mutability via setters\", I took an existing commons-email Java library from Apache and re-designed it my way, without setters and with \"object thinking\" in mind. I released my library as part of the jcabi family — jcabi-email . Let's see what benefits we get from a \"pure\" object-oriented and immutable approach, without getters. Here is how your code will look, if you send an email using commons-email: Email email = new SimpleEmail (); email . setHostName ( \"smtp.googlemail.com\" ); email . setSmtpPort ( 465 ); email . setAuthenticator ( new DefaultAuthenticator ( \"user\" , \"pwd\" )); email . setFrom ( \"yegor@teamed.io\" , \"Yegor Bugayenko\" ); email . addTo ( \"dude@jcabi.com\" ); email . setSubject ( \"how are you?\" ); email . setMsg ( \"Dude, how are you?\" ); email . send (); Here is how you do the same with jcabi-email : Postman postman = new Postman . Default ( new SMTP ( \"smtp.googlemail.com\" , 465 , \"user\" , \"pwd\" ) ); Envelope envelope = new Envelope . MIME ( new Array < Stamp >( new StSender ( \"Yegor Bugayenko <yegor@teamed.io>\" ), new StRecipient ( \"dude@jcabi.com\" ), new StSubject ( \"how are you?\" ) ), new Array < Enclosure >( new EnPlain ( \"Dude, how are you?\" ) ) ); postman . send ( envelope ); I think the difference is obvious. In the first example, you're dealing with a monster class that can do everything for you, including sending your MIME message via SMTP, creating the message, configuring its parameters, adding MIME parts to it, etc. The Email class from commons-email is really a huge class — 33 private properties, over a hundred methods, about two thousands lines of code. First, you configure the class through a bunch of setters and then you ask it to send() an email for you. In the second example, we have seven objects instantiated via seven new calls. Postman is responsible for packaging a MIME message; SMTP is responsible for sending it via SMTP; stamps ( StSender , StRecipient , and StSubject ) are responsible for configuring the MIME message before delivery; enclosure EnPlain is responsible for creating a MIME part for the message we're going to send. We construct these seven objects, encapsulating one into another, and then we ask the postman to send() the envelope for us. What's Wrong With a Mutable Email? From a user perspective, there is almost nothing wrong. Email is a powerful class with multiple controls — just hit the right one and the job gets done. However, from a developer perspective Email class is a nightmare. Mostly because the class is very big and difficult to maintain. Because the class is so big , every time you want to extend it by introducing a new method, you're facing the fact that you're making the class even worse — longer, less cohesive, less readable, less maintainable, etc. You have a feeling that you're digging into something dirty and that there is no hope to make it cleaner, ever. I'm sure, you're familiar with this feeling — most legacy applications look that way. They have huge multi-line \"classes\" (in reality, COBOL programs written in Java) that were inherited from a few generations of programmers before you. When you start, you're full of energy, but after a few minutes of scrolling such a \"class\" you say — \"screw it, it's almost Saturday\". Because the class is so big , there is no data hiding or encapsulation any more — 33 variables are accessible by over 100 methods. What is hidden? This Email.java file in reality is a big, procedural 2000-line script, called a \"class\" by mistake. Nothing is hidden, once you cross the border of the class by calling one of its methods. After that, you have full access to all the data you may need. Why is this bad? Well, why do we need encapsulation in the first place? In order to protect one programmer from another, aka defensive programming . While I'm busy changing the subject of the MIME message, I want to be sure that I'm not interfered with by some other method's activity, that is changing a sender and touching my subject by mistake. Encapsulation helps us narrow down the scope of the problem, while this Email class is doing exactly the opposite. Because the class is so big , its unit testing is even more complicated than the class itself. Why? Because of multiple inter-dependencies between its methods and properties. In order to test setCharset() you have to prepare the entire object by calling a few other methods, then you have to call send() to make sure the message being sent actually uses the encoding you specified. Thus, in order to test a one-line method setCharset() you run the entire integration testing scenario of sending a full MIME message through SMTP. Obviously, if something gets changed in one of the methods, almost every test method will be affected. In other words, tests are very fragile, unreliable and over-complicated. I can go on and on with this \" because the class is so big \", but I think it is obvious that a small, cohesive class is always better than a big one. It is obvious to me, to you, and to any object-oriented programmer. But why is it not so obvious to the developers of Apache Commons Email? I don't think they are stupid or un-educated. What is it then? How and Why Did It Happen? This is how it always happens. You start to design a class as something cohesive, solid, and small. Your intentions are very positive. Very soon you realize that there is something else that this class has to do. Then, something else. Then, even more. The best way to make your class more and more powerful is by adding setters that inject configuration parameters into the class so that it can process them inside, isn't it? This is the root cause of the problem! The root cause is our ability to insert data into mutable objects via configuration methods, also known as \"setters\". When an object is mutable and allows us to add setters whenever we want, we will do it without limits. Let me put it this way — mutable classes tend to grow in size and lose cohesiveness . If commons-email authors made this Email class immutable in the beginning, they wouldn't have been able to add so many methods into it and encapsulate so many properties. They wouldn't be able to turn it into a monster. Why? Because an immutable object only accepts a state through a constructor. Can you imagine a 33-argument constructor? Of course, not. When you make your class immutable in the first place, you are forced to keep it cohesive, small, solid and robust. Because you can't encapsulate too much and you can't modify what's encapsulated. Just two or three arguments of a constructor and you're done. How Did I Design An Immutable Email? When I was designing jcabi-email I started with a small and simple class: Postman . Well, it is an interface, since I never make interface-less classes. So, Postman is... a post man. He is delivering messages to other people. First, I created a default version of it (I omit the ctor, for the sake of brevity): import javax.mail.Message ; @Immutable class Postman . Default implements Postman { private final String host ; private final int port ; private final String user ; private final String password ; @Override void send ( Message msg ) { // create SMTP session // create transport // transport.connect(this.host, this.port, etc.) // transport.send(msg) // transport.close(); } } Good start, it works. What now? Well, the Message is difficult to construct. It is a complex class from JDK that requires some manipulations before it can become a nice HTML email. So I created an envelope, which will build this complex object for me (pay attention, both Postman and Envelope are immutable and annotated with @Immutable from jcabi-aspects ): @Immutable interface Envelope { Message unwrap (); } I also refactor the Postman to accept an envelope, not a message: @Immutable interface Postman { void send ( Envelope env ); } So far, so good. Now let's try to create a simple implementation of Envelope : @Immutable class MIME implements Envelope { @Override public Message unwrap () { return new MimeMessage ( Session . getDefaultInstance ( new Properties ()) ); } } It works, but it does nothing useful yet. It only creates an absolutely empty MIME message and returns it. How about adding a subject to it and both To: and From: addresses (pay attention, MIME class is also immutable): @Immutable class Envelope . MIME implements Envelope { private final String subject ; private final String from ; private final Array < String > to ; public MIME ( String subj , String sender , Iterable < String > rcpts ) { this . subject = subj ; this . from = sender ; this . to = new Array < String >( rcpts ); } @Override public Message unwrap () { Message msg = new MimeMessage ( Session . getDefaultInstance ( new Properties ()) ); msg . setSubject ( this . subject ); msg . setFrom ( new InternetAddress ( this . from )); for ( String email : this . to ) { msg . setRecipient ( Message . RecipientType . TO , new InternetAddress ( email ) ); } return msg ; } } Looks correct and it works. But it is still too primitive. How about CC: and BCC: ? What about email text? How about PDF enclosures? What if I want to specify the encoding of the message? What about Reply-To ? Can I add all these parameters to the constructor? Remember, the class is immutable and I can't introduce the setReplyTo() method. I have to pass the replyTo argument into its constructor. It's impossible, because the constructor will have too many arguments, and nobody will be able to use it. So, what do I do? Well, I started to think: how can we break the concept of an \"envelope\" into smaller concepts — and this what I invented. Like a real-life envelope, my MIME object will have stamps. Stamps will be responsible for configuring an object Message (again, Stamp is immutable, as well as all its implementors): @Immutable interface Stamp { void attach ( Message message ); } Now, I can simplify my MIME class to the following: @Immutable class Envelope . MIME implements Envelope { private final Array < Stamp > stamps ; public MIME ( Iterable < Stamp > stmps ) { this . stamps = new Array < Stamp >( stmps ); } @Override public Message unwrap () { Message msg = new MimeMessage ( Session . getDefaultInstance ( new Properties ()) ); for ( Stamp stamp : this . stamps ) { stamp . attach ( msg ); } return msg ; } } Now, I will create stamps for the subject, for To: , for From: , for CC: , for BCC: , etc. As many stamps as I like. The class MIME will stay the same — small, cohesive, readable, solid, etc. What is important here is why I made the decision to refactor while the class was relatively small. Indeed, I started to worry about these stamp classes when my MIME class was just 25 lines in size. That is exactly the point of this article — immutability forces you to design small and cohesive objects . Without immutability, I would have gone the same direction as commons-email. My MIME class would grow in size and sooner or later would become as big as Email from commons-email. The only thing that stopped me was the necessity to refactor it, because I wasn't able to pass all arguments through a constructor. Without immutability, I wouldn't have had that motivator and I would have done what Apache developers did with commons-email — bloat the class and turn it into an unmaintainable monster. That's jcabi-email . I hope this example was illustrative enough and that you will start writing cleaner code with immutable objects. "},{"title":"Hits-of-Code Instead of SLoC","url":"/2014/11/14/hits-of-code.html","tags":["mgmt"],"date":"2014-11-14 00:00:00 +0000","categories":[],"body":"Lines-of-Code (aka SLoC) is a metric with a terrible reputation. Try to google it yourself and you'll find tons of articles bad-mouthing about its counter-effectiveness and destructiveness for a software development process. The main argument is that we can't measure the progress of programming by the number of lines of code written. Probably the most famous quote is attributed to Bill Gates : Measuring programming progress by lines of code is like measuring aircraft building progress by weight Basically, this means that certain parts of the aircraft will take much more effort at the same time being much lighter than others (like a central computer, for example). Instead of measuring the weight of the aircraft we should measure the effort put into it... somehow. So, here is the idea. How about we measure the amount of times programmers touch the lines. Instead of counting the number of lines we'll count how many times they were actually modified — we can get this information from Git (or any other SCM). The more you touch that part of the aircraft — the more effort you spent on it, right? I called it Hits-of-Code (HoC) and created a small tool to help us calculate this number in just one line. It's a Ruby gem , install it and run: $ gem install hoc $ hoc 54687 The number 54687 is a total number of Hits-of-Code in your code base. The principle behind this number is primitive — every time a line of code is modified, created or deleted in a Git commit, the counter increments. The main reason why this metric is better than LoC is that it is much better aligned with the actual effort invested into the code base. Here is why. It Always Increments The HoC metric always goes up. Today it can not be lower than it was yesterday — just like the effort, it always increments. Lines-of-Code is not acting like this. You may have a huge code base today, but after refactoring it will become much smaller. The number of lines of code is decreased. Does it mean you are less effective? Definitely not, but the LoC metric says so, to a non-programmer. A project manager, for example, may decide that since the size of the code base stayed the same over the last month, the team is not working. HoC doesn't have this counter-intuitive effect. Instead, HoC grows together with your every commit. The more you work on the code base, the bigger the HoC. It doesn't matter how big or small the absolute size of the your product. What matters is how much effort you put into it. That's why HoC is very intuitive and may be used as a measurement of software development progress. Look at this 18-month graph; it shows both metrics together. I used the same Java code base of rultor , a DevOps assistant . The code base experienced a major refactoring a few months ago, as you see on the graph. I think it is obvious which metric on this graph tells us more about the efforts being invested into the product. It Is Objective For HoC it doesn't matter how big the absolute size of the code base, but only how big your relative contribution to it. Let's say, you have 300K lines of code and 95% of them were copy-pasted from some third-party libraries (by the way, it is a very common and terrible practice — to keep third-party code inside your own repository). The amount of lines of code will be big, but the actual custom code part will be relatively small. Thus, the LoC metric will be misleading — it will always show 300K with small increments or decrements around it. Everybody will have a feeling that the team is working with 300K lines code base. On the other hand, HoC will always take into account the part of code that is actually being modified. The value of HoC will be objectively correlated with the actual effort of programmers working with the code base. It Exposes Complexity of Lines LoC is usually critized for its neutrality towards code complexity. An auto-generated ORM class or a complex sorting algorithm may have the same size in terms of lines of code, but the first takes seconds to write, while the second may take weeks or months. That's why lines of code is usually considered a false metric. Hits-of-Code takes complexity into account, because the longer you work with that sorting algorithm the more modifications you make to its lines. Well, this statement is true if you use Git regularly and commit your changes frequently — that is how you tell Git about your work progress. Conclusion Finally, look at this list of open projects completed by our team over the last few years. Every project has two metrics: Lines-of-Code and Hits-of-Code. It is interesting to see how relatively small projects have very big (over a million) HoC numbers. This immediately reminds me how much time we invested into it and how old they are. I used the HoC metric in this analysis: How much do you pay per line of code? . That post compares a traditional project that paid $3.98 per HoC and an open source one, managed by Teamed.io, that paid ¢13. My conclusion is that this Hits-of-Code metric can be used as a tool of progress tracking in a software development project. Moreover, it can be used for estimations of team size, project budget, development schedule and so forth. Obviously, LoC can't be the only metric, but in combination with others it may greatly help in estimating, planning and tracking. "},{"title":"Seven Virtues of a Good Object","url":"/2014/11/20/seven-virtues-of-good-object.html","tags":["oop"],"date":"2014-11-20 00:00:00 +0000","categories":["best"],"body":"Martin Fowler says : A library is essentially a set of functions that you can call, these days usually organized into classes. Functions organized into classes? With all due respect, this is wrong. And it is a very common misconception of a class in object-oriented programming. Classes are not organizers of functions. And objects are not data structures. So what is a \"proper\" object? Which one is not a proper one? What is the difference? Even though it is a very polemic subject, it is very important. Unless we understand what an object is, how can we write object-oriented software? Well, thanks to Java, Ruby, and others, we can. But how good will it be? Unfortunately, this is not an exact science, and there are many opinions. Here is my list of qualities of a good object. Class vs. Object Before we start talking about objects, let's define what a class is. It is a place where objects are being born (a.k.a. instantiated ). The main responsibility of a class is to construct new objects on demand and destruct them when they are not used anymore. A class knows how its children should look and how they should behave. In other words, it knows what contracts they should obey. Sometimes I hear classes being called \"object templates\" (for example, Wikipedia says so ). This definition is not correct because it places classes into a passive position. This definition assumes that someone will get a template and build an object by using it. This may be true, technically speaking, but conceptually it's wrong. Nobody else should be involved — there are only a class and its children. An object asks a class to create another object, and the class constructs it; that's it. Ruby expresses this concept much better than Java or C++: photo = File . new ( ' / tmp / photo . png ' ) The object photo is constructed by the class File ( new is an entry point to the class). Once constructed, the object is acting on its own. It shouldn't know who constructed it and how many more brothers and sisters it has in the class. Yes, I mean that reflection is a terrible idea, but I'll write more about it in one of the next posts :) Now, let's talk about objects and their best and worst sides. 1. He Exists in Real Life First of all, an object is a living organism . Moreover, an object should be anthropomorphized , i.e. treated like a human being (or a pet, if you like them more). By this I basically mean that an object is not a data structure or a collection of functions. Instead, it is an independent entity with its own life cycle, its own behavior, and its own habits. An employee, a department, an HTTP request, a table in MySQL, a line in a file, or a file itself are proper objects — because they exist in real life, even when our software is turned off. To be more precise, an object is a representative of a real-life creature. It is a proxy of that real-life creature in front of all other objects. Without such a creature, there is — obviously — no object. photo = File . new ( ' / tmp / photo . png ' ) puts photo . width () In this example, I'm asking File to construct a new object photo , which will be a representative of a real file on disk. You may say that a file is also something virtual and exists only when the computer is turned on. I would agree and refine the definition of \"real life\" as follows: It is everything that exists aside from the scope of the program the object lives in. The disk file is outside the scope of our program; that's why it is perfectly correct to create its representative inside the program. A controller, a parser, a filter, a validator, a service locator, a singleton, or a factory are not good objects (yes, most GoF patterns are anti-patterns!). They don't exist apart from your software, in real life. They are invented just to tie other objects together. They are artificial and fake creatures. They don't represent anyone. Seriously, an XML parser — who does it represent? Nobody. Some of them may become good if they change their names; others can never excuse their existence. For example, that XML parser can be renamed to \"parseable XML\" and start to represent an XML document that exists outside of our scope. Always ask yourself, \"What is the real-life entity behind my object?\" If you can't find an answer, start thinking about refactoring. 2. He Works by Contracts A good object always works by contracts. He expects to be hired not because of his personal merits but because he obeys the contracts. On the other hand, when we hire an object, we shouldn't discriminate and expect some specific object from a specific class to do the work for us. We should expect any object to do what our contract says. As long as the object does what we need, we should not be interested in his class of origin, his sex, or his religion. For example, I need to show a photo on the screen. I want that photo to be read from a file in PNG format. I'm contracting an object from class DataFile and asking him to give me the binary content of that image. But wait, do I care where exactly the content will come from — the file on disk, or an HTTP request, or maybe a document in Dropbox? Actually, I don't. All I care about is that some object gives me a byte array with PNG content. So my contract would look like this: interface Binary { byte [] read (); } Now, any object from any class (not just DataFile ) can work for me. All he has to do, in order to be eligible, is to obey the contract — by implementing the interface Binary . The rule here is simple: every public method in a good object should implement his counterpart from an interface. If your object has public methods that are not inherited from any interface, he is badly designed. There are two practical reasons for this. First, an object working without a contract is impossible to mock in a unit test. Second, a contractless object is impossible to extend via decoration . 3. He Is Unique A good object should always encapsulate something in order to be unique. If there is nothing to encapsulate, an object may have identical clones, which I believe is bad. Here is an example of a bad object, which may have clones: class HTTPStatus implements Status { private URL page = new URL ( \"http://www.google.com\" ); @Override public int read () throws IOException { return HttpURLConnection . class . cast ( this . page . openConnection () ). getResponseCode (); } } I can create a few instances of class HTTPStatus , and all of them will be equal to each other: first = new HTTPStatus (); second = new HTTPStatus (); assert first . equals ( second ); Obviously utility classes, which have only static methods, can't instantiate good objects. More generally, utility classes don't have any of the merits mentioned in this article and can't even be called \"classes\". They are simply terrible abusers of an object paradigm and exist in modern object-oriented languages only because their inventors enabled static methods. 4. He Is Immutable A good object should never change his encapsulated state. Remember, an object is a representative of a real-life entity, and this entity should stay the same through the entire life of the object. In other words, an object should never betray those whom he represents. He should never change owners. :) Be aware that immutability doesn't mean that all methods always return the same values. Instead, a good immutable object is very dynamic. However, he never changes his internal state. For example: @Immutable final class HTTPStatus implements Status { private URL page ; public HTTPStatus ( URL url ) { this . page = url ; } @Override public int read () throws IOException { return HttpURLConnection . class . cast ( this . page . openConnection () ). getResponseCode (); } } Even though the method read() may return different values, the object is immutable. He points to a certain web page and will never point anywhere else. He will never change his encapsulated state, and he will never betray the URL he represents. Why is immutability a virtue? This article explains in detail: Objects Should Be Immutable . In a nutshell, immutable objects are better because: Immutable objects are simpler to construct, test, and use. Truly immutable objects are always thread-safe. They help avoid temporal coupling. Their usage is side-effect free (no defensive copies). They always have failure atomicity. They are much easier to cache. They prevent NULL references . Of course, a good object doesn't have setters , which may change his state and force him to betray the URL. In other words, introducing a setURL() method would be a terrible mistake in class HTTPStatus . Besides all that, immutable objects will force you to make more cohesive, solid, and understandable designs, as this article explains: How Immutability Helps . 5. His Class Doesn't Have Anything Static A static method implements a behavior of a class, not an object. Let's say we have class File , and his children have method size() : final class File implements Measurable { @Override public int size () { // calculate the size of the file and return } } So far, so good; the method size() is there because of the contract Measurable , and every object of class File will be able to measure his size. A terrible mistake would be to design this class with a static method instead (this design is also known as a utility class and is very popular in Java, Ruby, and almost every OOP language): // TERRIBLE DESIGN, DON'T USE! class File { public static int size ( String file ) { // calculate the size of the file and return } } This design runs completely against the object-oriented paradigm. Why? Because static methods turn object-oriented programming into \"class-oriented\" programming. This method, size() , exposes the behavior of the class, not of his objects. What's wrong with this, you may ask? Why can't we have both objects and classes as first-class citizens in our code? Why can't both of them have methods and properties? The problem is that with class-oriented programming, decomposition doesn't work anymore. We can't break down a complex problem into parts, because only a single instance of a class exists in the entire program. The power of OOP is that it allows us to use objects as an instrument for scope decomposition. When I instantiate an object inside a method, he is dedicated to my specific task. He is perfectly isolated from all other objects around the method. This object is a local variable in the scope of the method. A class, with his static methods, is always a global variable no matter where I use him. Because of that, I can't isolate my interaction with this variable from others. Besides being conceptually against object-oriented principles, public static methods have a few practical drawbacks: First, it's impossible to mock them (Well, you can use PowerMock , but this will then be the most terrible decision you could make in a Java project ... I made it once, a few years ago). Second, they are not thread-safe by definition, because they always work with static variables, which are accessible from all threads. You can make them thread-safe, but this will always require explicit synchronization. Every time you see a public static method, start rewriting immediately. I don't even want to mention how terrible static (or global) variables are. I think it is just obvious. 6. His Name Is Not a Job Title The name of an object should tell us what this object is , not what it does , just like we name objects in real life: book instead of page aggregator, cup instead of water holder, T-shirt instead of body dresser. There are exceptions, of course, like printer or computer, but they were invented just recently and by those who didn't read this article. :) For example, these names tell us who their owners are: an apple, a file, a series of HTTP requests, a socket, an XML document, a list of users, a regular expression, an integer, a PostgreSQL table, or Jeffrey Lebowski. A properly named object is always possible to draw as a small picture. Even a regular expression can be drawn. In the opposite, here is an example of names that tell us what their owners do: a file reader, a text parser, a URL validator, an XML printer, a service locator, a singleton, a script runner, or a Java programmer. Can you draw any of them? No, you can't. These names are not suitable for good objects. They are terrible names that lead to terrible design. In general, avoid names that end with \"-er\" — most of them are bad. \"What is the alternative of a FileReader ?\" I hear you asking. What would be a better name? Let's see. We already have File , which is a representative of a real-world file on disk. This representative is not powerful enough for us, because he doesn't know how to read the content of the file. We want to create a more powerful one that will have that ability. What would we call him? Remember, the name should say what he is, not what he does. What is he? He is a file that has data; not just a file, like File , but a more sophisticated one, with data. So how about FileWithData or simply DataFile ? The same logic should be applicable to all other names. Always think about what it is rather than what it does. Give your objects real, meaningful names instead of job titles. More about this in Don't Create Objects That End With -ER . 7. His Class Is Either Final or Abstract A good object comes from either a final or abstract class. A final class is one that can't be extended via inheritance. An abstract class is one that can't have children. Simply put, a class should either say, \"You can never break me; I'm a black box for you\" or \"I'm broken already; fix me first and then use\". There is nothing in between. A final class is a black box that you can't modify by any means. He works as he works, and you either use him or throw him away. You can't create another class that will inherit his properties. This is not allowed because of that final modifier. The only way to extend such a final class is through decoration of his children. Let's say I have the class HTTPStatus (see above), and I don't like him. Well, I like him, but he's not powerful enough for me. I want him to throw an exception if HTTP status is over 400. I want his method, read() , to do more that it does now. A traditional way would be to extend the class and overwrite his method: class OnlyValidStatus extends HTTPStatus { public OnlyValidStatus ( URL url ) { super ( url ); } @Override public int read () throws IOException { int code = super . read (); if ( code > 400 ) { throw new RuntimException ( \"unsuccessful HTTP code\" ); } return code ; } } Why is this wrong? It is very wrong because we risk breaking the logic of the entire parent class by overriding one of his methods. Remember, once we override the method read() in the child class, all methods from the parent class start to use his new version. We're literally injecting a new \"piece of implementation\" right into the class. Philosophically speaking, this is an offense. On the other hand, to extend a final class, you have to treat him like a black box and decorate him with your own implementation (a.k.a. Decorator Pattern ): final class OnlyValidStatus implements Status { private final Status origin ; public OnlyValidStatus ( Status status ) { this . origin = status ; } @Override public int read () throws IOException { int code = this . origin . read (); if ( code > 400 ) { throw new RuntimException ( \"unsuccessful HTTP code\" ); } return code ; } } Make sure that this class is implementing the same interface as the original one: Status . The instance of HTTPStatus will be passed into him through the constructor and encapsulated. Then every call will be intercepted and implemented in a different way, if necessary. In this design, we treat the original object as a black box and never touch his internal logic. If you don't use that final keyword, anyone (including yourself) will be able to extend the class and ... offend him :( So a class without final is a bad design. An abstract class is the exact oposite case — he tells us that he is incomplete and we can't use him \"as is\". We have to inject our custom implementation logic into him, but only into the places he allows us to touch. These places are explicitly marked as abstract methods. For example, our HTTPStatus may look like this: abstract class ValidatedHTTPStatus implements Status { @Override public final int read () throws IOException { int code = this . origin . read (); if (! this . isValid ()) { throw new RuntimException ( \"unsuccessful HTTP code\" ); } return code ; } protected abstract boolean isValid (); } As you see, the class doesn't know how exactly to validate the HTTP code, and he expects us to inject that logic through inheritance and through overloading the method isValid() . We're not going to offend him with this inheritance, since he defended all other methods with final (pay attention to the modifiers of his methods). Thus, the class is ready for our offense and is perfectly guarded against it. To summarize, your class should either be final or abstract — nothing in between. "},{"title":"Five Principles of Bug Tracking","url":"/2014/11/24/principles-of-bug-tracking.html","tags":["mgmt"],"date":"2014-11-24 00:00:00 +0000","categories":[],"body":"A team working remotely requires much stronger discipline than a co-located crew sitting in the same office. First of all, I mean discipline of communications. At teamed.io , we have developed software remotely for the last five years. We manage tasks strictly through ticketing systems (like Github, JIRA, Trac, Basecamp, etc.) and discourage any informal communications, like Skype, HipChat, emails, or phone calls. Every ticket for us is an isolated task with its own life cycle, its own participants, and its own goal. Over these years, we've learned a few lessons that I want to share. If you also work remotely with your team, you may find them useful. Monty Python Flying Circus, TV Series (1969-1974) 1. Keep It One-on-One Each ticket (aka \"bug\") is a link between two people: problem specifier and problem solver. If it is a bug, I'm reporting it — you're solving it. If it is a question, I'm asking for an explanation — you're explaining. If it is a task, I'm ordering you to do it — you're doing it. In any case, there are two main characters. No matter how many people are involved in the ticket resolution, only these two characters have formal roles. The responsibility of the ticket reporter is to defend the problem . When I report a bug, I have to insist that it exists — this is my job. Others may tell me that I'm wrong and the bug is not there. They may tell me that they can't reproduce it. They may say that my description of a task is too vague and nobody understands it. There may be many issues of that kind. My job is to do the best I can in order to keep the ticket alive . Obviously, if the bug is not reproducible, I'll be forced to close the ticket. However, until the ticket is closed, I'm its guardian angel. :) On the other hand, the responsibility of the ticket solver is to defend the solution . When a ticket is assigned to me and I have to resolve it, my job is to convince the reporter that my solution is good enough. He may tell me that my solution is not sufficient, not the most efficient, or incomplete. My job is to insist that I'm right and he is wrong. Well, of course, in a reasonable way. And in order to create a solution that will be accepted as sufficient enough, I have to understand the problem first, investigate all possible options, and propose the most elegant implementation. But all this is secondary. The first thing I will be focused on is how to convince the reporter. I will always remember that my primary goal is to close the ticket . My point here is that no matter how many people are involved in the ticket discussion, always remember what is happening there — one person is selling his solution to another person. Everybody else around them is help or distraction (see below). 2. Close It! Remember that a ticket is not a chat. You're not there to talk. You're there to close . When the ticket is assigned to you, focus on closing it as soon as possible. Also, keep in mind that the sooner you close the ticket, the better job you will do for the project. Long-living tickets are a management nightmare. It is difficult to track them and control them. It's difficult to understand what's going on. Have you seen those two-year-old tickets in open source projects that have hundreds of comments and no deliverables? It is a mistake by their project managers and ticket participants. Each ticket should be short and focused — 1) a problem, 2) a refinement question, 3) a short explanation, 4) a solution, 5) closed, thanks everybody. This is an ideal scenario. As soon as you realize that your ticket is turning into a long discussion, try to close it even faster. How can I close it if the reporter doesn't like my solution? Find a temporary solution that will satisfy the reporter and allow you to close the ticket. Use \"TODO\" in your code or dirty workarounds — they are all better than a ticket hovering for a long time. Once you see that the solution is provided and is sufficient enough to close the ticket, ask its reporter to close it. Explicitly ask for that; don't dance arround with \"looks like this solution may be accepted, if you don't mind\". Be explicit in your intention to close the ticket and move on. Try this: \"@jeff, please close the ticket if you don't have any further questions.\" 3. Don't Close It! Every time you raise a bug and create a new ticket, you consume project resources. Every bug report means money spent on the project: 1) money for your time spent finding the problem and reporting it; 2) for the time of the project manager who is working with the ticket and finding who will fix it; 3) for the time of the ticket solver, who is trying to understand your report and provide a solution; and also 4) for the time of everybody else who will participate in the discussion. If you close the ticket without a problem being properly solved, you put this money into the trash bin. Once the ticket is started, there is no way back. We can't just say, \"Nah, ignore it; it's not important anymore.\" Your ticket already consumed project time and budget resources, and in order to turn them into something useful, you have to make sure that some solution is delivered. It can be a temporary solution. It can be a single line change in the project documentation. It can be a TODO marker in the code saying that \"we are aware of the problem but won't fix it because we're lazy\". Anything would work, but not nothing. Look at it from a different perspective. When you started that ticket, you had something in mind. Something was not right with the product. That's why you reported a bug. If you close the ticket without anyone even touching that place of code, someone else will have the same concern in a few days or a few years. And then the project will have to pay again for a similar ticket or discussion of the same problem. Even if you're convinced that the issue you found in the code is not really an issue, ask a ticket resolver to document it right in the source code in order to prevent such confusion from happening again in the future. 4. Avoid Noise — Address Your Comments Every time you post a message to the ticket, address it to someone. Otherwise, if you post just because you want to express your opinion, your comments become communication noise. Remember, a ticket is a conversation between two people — one of them reported an issue and the other one is trying to fix it. Comments like, \"How about we try another approach\" or \"I remember I had a similar issue some time ago\" are very annoying and distracting. Let's be honest, nobody really needs or cares about \"opinions\". All we need in a ticket is a solution(s). If you think the ticket should be closed because the introduced solution is good enough, address your comment to the ticket reporter. And start it with \"@jeff, I think the solution you've got already is good enough, because ...\" This way, you will help the assignee to close the ticket and move on. If you think the solution is wrong, address your comment to the assignee of the ticket, starting with \"@jeff, I believe your solution is not good enough because ...\" This way, you will help the ticket reporter keep the ticket open until a proper solution comes up. Again, don't pollute the air with generic opinions. Instead, be very specific and take sides — you either like the solution and want the ticket to be closed, or you don't like it and want the ticket to stay open. Everything in between is just making the situation more complex and isn't helping the project at all. 5. Report When It Is Broken I think it is obvious, but I will reiterate: Every bug has to be reproducible. Every time you report a bug, you should explain how exactly the product is broken. Yes, it is your job to prove that the software doesn't work as intended, or is not documented properly, or doesn't satisfy the requirements, etc. Every bug report should follow the same simple formula: \"This is what we have , this is what we should have instead, so fix it\". Every ticket, be it a bug, a task, a question, or a suggestion, should be formatted in this way. By submitting it, you're asking the project to move from point A to point B. Something is not right at point A, and it will be much better for all of us to be at that point B. So it's obvious that you have to explain where these points A and B are. It is highly desirable if you can explain how to get there — how to reproduce a problem and how to fix it. Even when you have a question, you should also follow that format. If you have a question, it means the project documentation is not sufficient enough for you to find an answer there. This is what is broken. You should ask for a fix. So instead of reporting, \"How should I use class X?\", say something like, \"The current documentation is not complete; it doesn't explain how I should use class X. Please fix.\" If you can't explain how to get there, say so in the ticket: \"I see that this class doesn't work as it should, but I don't know how to reproduce the problem and how to fix it.\" This will give everybody a clear message that you are aware that your bug report is not perfect. The first step for its resolver will be to refine the problem and find a way to reproduce it. If such a replica can't be found, obviously your bug will be forced into closing. Let me reiterate again: Every ticket is dragging the project from point A, where something is not right, to point B, where it is fixed. Your job, as a ticket reporter, is to draw that line — clearly and explicitly. "},{"title":"ORM Is an Offensive Anti-Pattern","url":"/2014/12/01/orm-offensive-anti-pattern.html","tags":["oop"],"date":"2014-12-01 00:00:00 +0000","categories":["best"],"body":"TL;DR ORM is a terrible anti-pattern that violates all principles of object-oriented programming, tearing objects apart and turning them into dumb and passive data bags. There is no excuse for ORM existence in any application, be it a small web app or an enterprise-size system with thousands of tables and CRUD manipulations on them. What is the alternative? SQL-speaking objects . Vinni-Pukh (1969) by Fyodor Khitruk How ORM Works Object-relational mapping (ORM) is a technique (a.k.a. design pattern) of accessing a relational database from an object-oriented language (Java, for example). There are multiple implementations of ORM in almost every language; for example: Hibernate for Java, ActiveRecord for Ruby on Rails, Doctrine for PHP, and SQLAlchemy for Python. In Java, the ORM design is even standardized as JPA . First, let's see how ORM works, by example. Let's use Java, PostgreSQL, and Hibernate. Let's say we have a single table in the database, called post : +-----+------------+--------------------------+ | id | date | title | +-----+------------+--------------------------+ | 9 | 10/24/2014 | How to cook a sandwich | | 13 | 11/03/2014 | My favorite movies | | 27 | 11/17/2014 | How much I love my job | +-----+------------+--------------------------+ Now we want to CRUD-manipulate this table from our Java app (CRUD stands for create, read, update, and delete). First, we should create a Post class (I'm sorry it's so long, but that's the best I can do): @Entity @Table ( name = \"post\" ) public class Post { private int id ; private Date date ; private String title ; @Id @GeneratedValue public int getId () { return this . id ; } @Temporal ( TemporalType . TIMESTAMP ) public Date getDate () { return this . date ; } public Title getTitle () { return this . title ; } public void setDate ( Date when ) { this . date = when ; } public void setTitle ( String txt ) { this . title = txt ; } } Before any operation with Hibernate, we have to create a session factory: SessionFactory factory = new AnnotationConfiguration () . configure () . addAnnotatedClass ( Post . class ) . buildSessionFactory (); This factory will give us \"sessions\" every time we want to manipulate with Post objects. Every manipulation with the session should be wrapped in this code block: Session session = factory . openSession (); try { Transaction txn = session . beginTransaction (); // your manipulations with the ORM, see below txn . commit (); } catch ( HibernateException ex ) { txn . rollback (); } finally { session . close (); } When the session is ready, here is how we get a list of all posts from that database table: List posts = session . createQuery ( \"FROM Post\" ). list (); for ( Post post : ( List < Post >) posts ){ System . out . println ( \"Title: \" + post . getTitle ()); } I think it's clear what's going on here. Hibernate is a big, powerful engine that makes a connection to the database, executes necessary SQL SELECT requests, and retrieves the data. Then it makes instances of class Post and stuffs them with the data. When the object comes to us, it is filled with data, and we should use getters to take them out, like we're using getTitle() above. When we want to do a reverse operation and send an object to the database, we do all of the same but in reverse order. We make an instance of class Post , stuff it with the data, and ask Hibernate to save it: Post post = new Post (); post . setDate ( new Date ()); post . setTitle ( \"How to cook an omelette\" ); session . save ( post ); This is how almost every ORM works. The basic principle is always the same — ORM objects are anemic envelopes with data. We are talking with the ORM framework, and the framework is talking to the database. Objects only help us send our requests to the ORM framework and understand its response. Besides getters and setters, objects have no other methods. They don't even know which database they came from. This is how object-relational mapping works. What's wrong with it, you may ask? Everything! What's Wrong With ORM? Seriously, what is wrong? Hibernate has been one of the most popular Java libraries for more than 10 years already. Almost every SQL-intensive application in the world is using it. Each Java tutorial would mention Hibernate (or maybe some other ORM like TopLink or OpenJPA) for a database-connected application. It's a standard de-facto and still I'm saying that it's wrong? Yes. I'm claiming that the entire idea behind ORM is wrong. Its invention was maybe the second big mistake in OOP after NULL reference . Actually, I'm not the only one saying something like this, and definitely not the first. A lot about this subject has already been published by very respected authors, including OrmHate by Martin Fowler, Object-Relational Mapping Is the Vietnam of Computer Science by Jeff Atwood, The Vietnam of Computer Science by Ted Neward, ORM Is an Anti-Pattern by Laurie Voss, and many others. However, my argument is different than what they're saying. Even though their reasons are practical and valid, like \"ORM is slow\" or \"database upgrades are hard\", they miss the main point. You can see a very good, practical answer to these practical arguments given by Bozhidar Bozhanov in his ORM Haters Don’t Get It blog post. The main point is that ORM, instead of encapsulating database interaction inside an object, extracts it away, literally tearing a solid and cohesive living organism apart. One part of the object keeps the data while another one, implemented inside the ORM engine (session factory), knows how to deal with this data and transfers it to the relational database. Look at this picture; it illustrates what ORM is doing. I, being a reader of posts, have to deal with two components: 1) the ORM and 2) the \"obtruncated\" object returned to me. The behavior I'm interacting with is supposed to be provided through a single entry point, which is an object in OOP. In the case of ORM, I'm getting this behavior via two entry points — the ORM and the \"thing\", which we can't even call an object. Because of this terrible and offensive violation of the object-oriented paradigm, we have a lot of practical issues already mentioned in respected publications. I can only add a few more. SQL Is Not Hidden . Users of ORM should speak SQL (or its dialect, like HQL ). See the example above; we're calling session.createQuery(\"FROM Post\") in order to get all posts. Even though it's not SQL, it is very similar to it. Thus, the relational model is not encapsulated inside objects. Instead, it is exposed to the entire application. Everybody, with each object, inevitably has to deal with a relational model in order to get or save something. Thus, ORM doesn't hide and wrap the SQL but pollutes the entire application with it. Difficult to Test . When some object is working a list of posts, it needs to deal with an instance of SessionFactory . How can we mock this dependency? We have to create a mock of it? How complex is this task? Look at the code above, and you will realize how verbose and cumbersome that unit test will be. Instead, we can write integration tests and connect the entire application to a test version of PostgreSQL. In that case, there is no need to mock SessionFactory , but such tests will be rather slow, and even more important, our having-nothing-to-do-with-the-database objects will be tested against the database instance. A terrible design. Again, let me reiterate. Practical problems of ORM are just consequences. The fundamental drawback is that ORM tears objects apart, terribly and offensively violating the very idea of what an object is . SQL-Speaking Objects What is the alternative? Let me show it to you by example. Let's try to design that class, Post , my way. We'll have to break it down into two classes: Post and Posts , singular and plural. I already mentioned in one of my previous articles that a good object is always an abstraction of a real-life entity. Here is how this principle works in practice. We have two entities: database table and table row. That's why we'll make two classes; Posts will represent the table, and Post will represent the row. As I also mentioned in that article , every object should work by contract and implement an interface. Let's start our design with two interfaces. Of course, our objects will be immutable. Here is how Posts would look: @Immutable interface Posts { Iterable < Post > iterate (); Post add ( Date date , String title ); } This is how a single Post would look: @Immutable interface Post { int id (); Date date (); String title (); } Here is how we will list all posts in the database table: Posts posts = // we'll discuss this right now for ( Post post : posts . iterate ()){ System . out . println ( \"Title: \" + post . title ()); } Here is how we will create a new post: Posts posts = // we'll discuss this right now posts . add ( new Date (), \"How to cook an omelette\" ); As you see, we have true objects now. They are in charge of all operations, and they perfectly hide their implementation details. There are no transactions, sessions, or factories. We don't even know whether these objects are actually talking to the PostgreSQL or if they keep all the data in text files. All we need from Posts is an ability to list all posts for us and to create a new one. Implementation details are perfectly hidden inside. Now let's see how we can implement these two classes. I'm going to use jcabi-jdbc as a JDBC wrapper, but you can use something else or just plain JDBC if you like. It doesn't really matter. What matters is that your database interactions are hidden inside objects. Let's start with Posts and implement it in class PgPosts (\"pg\" stands for PostgreSQL): @Immutable final class PgPosts implements Posts { private final Source dbase ; public PgPosts ( DataSource data ) { this . dbase = data ; } public Iterable < Post > iterate () { return new JdbcSession ( this . dbase ) . sql ( \"SELECT id FROM post\" ) . select ( new ListOutcome < Post >( new ListOutcome . Mapping < Post >() { @Override public Post map ( final ResultSet rset ) { return new PgPost ( rset . getInteger ( 1 )); } } ) ); } public Post add ( Date date , String title ) { return new PgPost ( this . dbase , new JdbcSession ( this . dbase ) . sql ( \"INSERT INTO post (date, title) VALUES (?, ?)\" ) . set ( new Utc ( date )) . set ( title ) . insert ( new SingleOutcome < Integer >( Integer . class )) ); } } Next, let's implement the Post interface in class PgPost : @Immutable final class PgPost implements Post { private final Source dbase ; private final int number ; public PgPost ( DataSource data , int id ) { this . dbase = data ; this . number = id ; } public int id () { return this . number ; } public Date date () { return new JdbcSession ( this . dbase ) . sql ( \"SELECT date FROM post WHERE id = ?\" ) . set ( this . number ) . select ( new SingleOutcome < Utc >( Utc . class )); } public String title () { return new JdbcSession ( this . dbase ) . sql ( \"SELECT title FROM post WHERE id = ?\" ) . set ( this . number ) . select ( new SingleOutcome < String >( String . class )); } } This is how a full database interaction scenario would look like using the classes we just created: Posts posts = new PgPosts ( dbase ); for ( Post post : posts . iterate ()){ System . out . println ( \"Title: \" + post . title ()); } Post post = posts . add ( new Date (), \"How to cook an omelette\" ); System . out . println ( \"Just added post #\" + post . id ()); You can see a full practical example here . It's an open source web app that works with PostgreSQL using the exact approach explained above — SQL-speaking objects. What About Performance? I can hear you screaming, \"What about performance?\" In that script a few lines above, we're making many redundant round trips to the database. First, we retrieve post IDs with SELECT id and then, in order to get their titles, we make an extra SELECT title call for each post. This is inefficient, or simply put, too slow. No worries; this is object-oriented programming, which means it is flexible! Let's create a decorator of PgPost that will accept all data in its constructor and cache it internally, forever: @Immutable final class ConstPost implements Post { private final Post origin ; private final Date dte ; private final String ttl ; public ConstPost ( Post post , Date date , String title ) { this . origin = post ; this . dte = date ; this . ttl = title ; } public int id () { return this . origin . id (); } public Date date () { return this . dte ; } public String title () { return this . ttl ; } } Pay attention: This decorator doesn't know anything about PostgreSQL or JDBC. It just decorates an object of type Post and pre-caches the date and title. As usual, this decorator is also immutable. Now let's create another implementation of Posts that will return the \"constant\" objects: @Immutable final class ConstPgPosts implements Posts { // ... public Iterable < Post > iterate () { return new JdbcSession ( this . dbase ) . sql ( \"SELECT * FROM post\" ) . select ( new ListOutcome < Post >( new ListOutcome . Mapping < Post >() { @Override public Post map ( final ResultSet rset ) { return new ConstPost ( new PgPost ( rset . getInteger ( 1 )), Utc . getTimestamp ( rset , 2 ), rset . getString ( 3 ) ); } } ) ); } } Now all posts returned by iterate() of this new class are pre-equipped with dates and titles fetched in one round trip to the database. Using decorators and multiple implementations of the same interface, you can compose any functionality you wish. What is the most important is that while functionality is being extended, the complexity of the design is not escalating, because classes don't grow in size. Instead, we're introducing new classes that stay cohesive and solid, because they are small. What About Transactions? Every object should deal with its own transactions and encapsulate them the same way as SELECT or INSERT queries. This will lead to nested transactions, which is perfectly fine provided the database server supports them. If there is no such support, create a session-wide transaction object that will accept a \"callable\" class. For example: final class Txn { private final DataSource dbase ; public < T > T call ( Callable < T > callable ) { JdbcSession session = new JdbcSession ( this . dbase ); try { session . sql ( \"START TRANSACTION\" ). exec (); T result = callable . call (); session . sql ( \"COMMIT\" ). exec (); return result ; } catch ( Exception ex ) { session . sql ( \"ROLLBACK\" ). exec (); throw ex ; } } } Then, when you want to wrap a few object manipulations in one transaction, do it like this: new Txn ( dbase ). call ( new Callable < Integer >() { @Override public Integer call () { Posts posts = new PgPosts ( dbase ); Post post = posts . add ( new Date (), \"How to cook an omelette\" ); posts . comments (). post ( \"This is my first comment!\" ); return post . id (); } } ); This code will create a new post and post a comment to it. If one of the calls fail, the entire transaction will be rolled back. This approach looks object-oriented to me. I'm calling it \"SQL-speaking objects\", because they know how to speak SQL with the database server. It's their skill, perfectly encapsulated inside their borders. "},{"title":"Synchronization Between Nodes","url":"/2014/12/04/synchronization-between-nodes.html","tags":["stateful"],"date":"2014-12-04 00:00:00 +0000","categories":[],"body":"When two or more software modules are accessing the same resource, they have to be synchronized. This means that only one module at a time should be working with the resource. Without such synchronization, there will be collisions and conflicts. This is especially true when we're talking about \"resources\" that do not support atomic transactions. To solve this issue and prevent conflicts, we have to introduce one more element into the picture. All software modules, before accessing the resource, should capture a lock from a centralized server. Once the manipulations with the resource are complete, the module should release the lock. While the lock is being captured by one module, no other modules will be able to capture it. The approach is very simple and well-known. However, I didn't find any cloud services that would provide such a locking and unlocking service over a RESTful API. So I decided to create one — stateful.co . No Retreat, No Surrender (1986) by Corey Yuen Here is a practical example. I have a Java web app that is hosted at Heroku. There are three servers (a.k.a. \"dynos\") running the same .war application. Why three? Because the web traffic is rather active, and one server is not powerful enough. So I have to have three of them. They all run exactly the same applications. Each web app works with a table in Amazon DynamoDB. It updates the table, puts new items into it, deletes some items sometimes, and selects them. So far, so good, but conflicts are inevitable. Here is an example of a typical interaction scenario between the web app and DynamoDB (I'm using jcabi-dynamo): Table table = region . table ( \"posts\" ); Item item = table . frame () . where ( \"name\" , \"Jeff\" ) . iterator (). next (); String salary = item . get ( \"salary\" ); item . put ( \"salary\" , this . recalculate ( salary )); The logic is obvious here. First, I retrieve an item from the table posts , then read its salary , and then modify it according to my recalculation algorithm. The problem is that another module may start to do the same while I'm recalculating. It will read the same initial value from the table and will start exactly the same recalculation. Then it will save a new value, and I will save one too. We will end up having Jeff's salary modified only once, while users will expect a double modification since two of them initiated two transactions with two different web apps. The right approach here is to \"lock\" the DynamoDB table first, even before reading the salary. Then do the modifications and eventually unlock it. Here is how stateful.co helps me. All I need to do is create a new named lock in the stateful.co web panel, get my authentication keys, and modify my Java code: Sttc sttc = new RtSttc ( new URN ( \"urn:github:526301\" ), // my Github ID \"9FF3-4320-73FB-EEAC\" // my secret key! ); Locks locks = sttc . locks (); Lock lock = locks . get ( \"posts-table-lock\" ); Table table = region . table ( \"posts\" ); Item item = table . frame () . where ( \"name\" , \"Jeff\" ) . iterator (). next (); new Atomic ( lock ). call ( new Callable < Void >() { @Override public void call () { String salary = item . get ( \"salary\" ); item . put ( \"salary\" , this . recalculate ( salary )); return null ; } } ); As you see, I wrap that critical transaction into Callable , which will be executed in isolation. This approach, obviously, doesn't guarantee atomicity of transaction — if part of the transaction fails, there won't be any automatic rollbacks and the DynamoDB table will be left in a \"broken\" state. Locks from stateful.co guarantee isolation in resource usage, and you can use any type of resources, including NoSQL tables, files, S3 objects, embedded software, etc. I should not forget to add this dependency to my pom.xml : <dependency> <groupId> co.stateful </groupId> <artifactId> java-sdk </artifactId> </dependency> Of course, you can do the same; the service is absolutely free of charge. And you can use any other languages, not just Java. BTW, if interested, contribute with your own SDK in your preferred language; I'll add it to the Github collection . "},{"title":"How an Immutable Object Can Have State and Behavior?","url":"/2014/12/09/immutable-object-state-and-behavior.html","tags":["oop"],"date":"2014-12-09 00:00:00 +0000","categories":[],"body":"I often hear this argument against immutable objects : \"Yes, they are useful when the state doesn't change. However, in our case, we deal with frequently changing objects. We simply can't afford to create a new document every time we just need to change its title .\" Here is where I disagree: object title is not a state of a document, if you need to change it frequently. Instead, it is a document's behavior . A document can and must be immutable, if it is a good object , even when its title is changed frequently. Let me explain how. Once Upon a Time in the West (1968) by Sergio Leone Identity, State, and Behavior Basically, there are three elements in every object: identity, state, and behavior. Identity is what distinguishes our document from other objects, state is what a document knows about itself (a.k.a. \"encapsulated knowledge\"), and behavior is what a document can do for us on request. For example, this is a mutable document: class Document { private int id ; private String title ; Document ( int id ) { this . id = id ; } public String getTitle () { return this . title ; } public String setTitle ( String text ) { this . title = text ; } @Override public String toString () { return String . format ( \"doc #%d about '%s'\" , this . id , this . text ); } } Let's try to use this mutable object: Document first = new Document ( 50 ); first . setTitle ( \"How to grill a sandwich\" ); Document second = new Document ( 50 ); second . setTitle ( \"How to grill a sandwich\" ); if ( first . equals ( second )) { // FALSE System . out . println ( String . format ( \"%s is equal to %s\" , first , second ) ); } Here, we're creating two objects and then modifying their encapsulated states. Obviously, first.equals(second) will return false because the two objects have different identities, even though they encapsulate the same state. Method toString() exposes the document's behavior — the document can convert itself to a string. In order to modify a document's title, we just call its setTitle() once again: first . setTitle ( \"How to cook pasta\" ); Simply put, we can reuse the object many times, modifying its internal state. It is fast and convenient, isn't it? Fast, yes. Convenient, not really. Read on. Immutable Objects Have No Identity As I've mentioned before , immutability is one of the virtues of a good object, and a very important one. A good object is immutable, and good software contains only immutable objects. The main difference between immutable and mutable objects is that an immutable one doesn't have an identity and its state never changes. Here is an immutable variant of the same document: @Immutable class Document { private final int id ; private final String title ; Document ( int id , String text ) { this . id = id ; this . title = text ; } public String title () { return this . title ; } public Document title ( String text ) { return new Document ( this . id , text ); } @Override public boolean equals ( Object doc ) { return doc instanceof Document && Document . class . cast ( doc ). id == this . id && Document . class . cast ( doc ). title . equals ( this . title ); } @Override public String toString () { return String . format ( \"doc #%d about '%s'\" , this . id , this . text ); } } This document is immutable, and its state ( id ad title ) is its identity. Let's see how we can use this immutable class (by the way, I'm using @Immutable annotation from jcabi-aspects ): Document first = new Document ( 50 , \"How to grill a sandwich\" ); Document second = new Document ( 50 , \"How to grill a sandwich\" ); if ( first . equals ( second )) { // TRUE System . out . println ( String . format ( \"%s is equal to %s\" , first , second ) ); } We can't modify a document any more. When we need to change the title, we have to create a new document: Document first = new Document ( 50 , \"How to grill a sandwich\" ); first = first . title ( \"How to cook pasta\" ); Every time we want to modify its encapsulated state, we have to modify its identity too, because there is no identity. State is the identity. Look at the code of the equals() method above — it compares documents by their IDs and titles. Now ID+title of a document is its identity! What About Frequent Changes? Now I'm getting to the question we started with: What about performance and convenience? We don't want to change the entire document every time we have to modify its title. If the document is big enough, that would be a huge obligation. Moreover, if an immutable object encapsulates other immutable objects, we have to change the entire hierarchy when modifying even a single string in one of them. The answer is simple. A document's title should not be part of its state . Instead, the title should be its behavior . For example, consider this: @Immutable class Document { private final int id ; Document ( int id ) { this . id = id ; } public String title () { // read title from storage } public void title ( String text ) { // save text to storage } @Override public boolean equals ( Object doc ) { return doc instanceof Document && Document . class . cast ( doc ). id == this . id ; } @Override public String toString () { return String . format ( \"doc #%d about '%s'\" , this . id , this . title ()); } } Conceptually speaking, this document is acting as a proxy of a real-life document that has a title stored somewhere — in a file, for example. This is what a good object should do — be a proxy of a real-life entity. The document exposes two features: reading the title and saving the title. Here is how its interface would look like: @Immutable interface Document { String title (); void title ( String text ); } title() reads the title of the document and returns it as a String , and title(String) saves it back into the document. Imagine a real paper document with a title. You ask an object to read that title from the paper or to erase an existing one and write new text over it. This paper is a \"copy\" utilized in these methods. Now we can make frequent changes to the immutable document, and the document stays the same. It doesn't stop being immutable, since it's state ( id ) is not changed. It is the same document, even though we change its title, becuase the title is not a state of the document. It is something in the real world, outside of the document. The document is just a proxy between us and that \"something\". Reading and writing the title are behaviors of the document, not its state. Mutable Memory The only question we still have unanswered is what is that \"copy\" and what happens if we need to keep the title of the document in memory? Let's look at it from an \"object thinking\" point of view. We have a document object, which is supposed to represent a real-life entity in an object-oriented world. If such an entity is a file, we can easily implement title() methods. If such an entity is an Amazon S3 object, we also implement title reading and writing methods easily, keeping the object immutable. If such an entity is an HTTP page, we have no issues in the implementation of title reading or writing, keeping the object immutable. We have no issues as long as a real-world document exists and has its own identity. Our title reading and writing methods will communicate with that real-world document and extract or update its title. Problems arise when such an entity doesn't exist in a real world. In that case, we need to create a mutable object property called title , read it via title() , and modify it via title(String) . But an object is immutable, so we can't have a mutable property in it — by definition! What do we do? Think. How could it be that our object doesn't represent a real-world entity? Remember, the real world is everything around the living environment of an object. Is it possible that an object doesn't represent anyone and acts on its own? No, it's not possible. Every object is a representantive of a real-world entity. So, who does it represent if we want to keep title inside it and we don't have any file or HTTP page behind the object? It represents computer memory . The title of immutable document #50, \"How to grill a sandwich\", is stored in the memory, taking up 23 bytes of space. The document should know where those bytes are stored, and it should be able to read them and replace them with something else. Those 23 bytes are the real-world entity that the object represents. The bytes have nothing to do with the state of the object. They are a mutable real-world entity, similar to a file, HTTP page, or an Amazon S3 object. Unfortunately, Java (and many other modern languages) do not allow direct access to computer memory. This is how we would design our class if such direct access was possible: @Immutable class Document { private final int id ; private final Memory memory ; Document ( int id ) { this . id = id ; this . memory = new Memory (); } public String title () { return new String ( this . memory . read ()); } public void title ( String text ) { this . memory . write ( text . getBytes ()); } } That Memory class would be implemented by JDK natively, and all other classes would be immutable. The class Memory would have direct access to the memory heap and would be responsible for malloc and free operations on the operating system level. Having such a class would allow us to make all Java classes immutable, including StringBuffer , ByteArrayOutputStream , etc. The Memory class would explicitly emphasize the mission of an object in a software program, which is to be a data animator . An object is not holding data; it is animating it. The data exists somewhere, and it is anemic, static, motionless, stationary, etc. The data is dead while the object is alive . The role of an object is to make a piece of data alive, to animate it but not to become a piece of data. An object needs some knowledge in order to gain access to that dead piece of data. An object may need a database unique key, an HTTP address, a file name, or a memory address in order to find the data and animate it. But an object should never think of itself as data. What Is the Practical Solution? Unfortunately, we don't have such a memory-representing class in Java, Ruby, JavaScript, Python, PHP, and many other high-level languages. It looks like language designers didn't get the idea of alive objects vs. dead data, which is sad. We're forced to mix data with object states using the same language constructs: object variables and properties. Maybe someday we'll have that Memory class in Java and other languages, but until then, we have a few options. Use C++ . In C++ and similar low-level languages, it is possible to access memory directly and deal with in-memory data the same way we deal with in-file or in-HTTP data. In C++, we can create that Memory class and use it exactly the way we explained above. Use Arrays . In Java, an array is a data structure with a unique property — it can be modified while being declared as final . You can use an array of bytes as a mutable data structure inside an immutable object. It's a surrogate solution that conceptually resembles the Memory class but is much more primitive. Avoid In-Memory Data . Try to avoid in-memory data as much as possible. In some domains, it is easy to do; for example, in web apps, file processing, I/O adapters, etc. However, in other domains, it is much easier said than done. For example, in games, data manipulation algorithms, and GUI, most of the objects animate in-memory data mostly because memory is the only resource they have. In that case, without the Memory class, you end up with mutable objects :( There is no workaround. To summarize, don't forget that an object is an animator of data. It is using its encapsulated knowledge in order to reach the data. No matter where the data is stored — in a file, in HTTP, or in memory — it is conceptually very different from an object state, even though they may look very similar. A good object is an immutable animator of mutable data. Even though it is immutable and data is mutable, it is alive and data is dead in the scope of the object's living environment. "},{"title":"How Much Your Objects Encapsulate?","url":"/2014/12/15/how-much-your-objects-encapsulate.html","tags":["oop"],"date":"2014-12-15 00:00:00 +0000","categories":[],"body":"Which line do you like more, the first or the second: new HTTP ( \"http://www.google.com\" ). read (); new HTTP (). read ( \"http://www.google.com\" ); What is the difference? The first class HTTP encapsulates a URL, while the second one expects it as an argument of method read() . Technically, both objects do exactly the same thing: they read the content of the Google home page. Which one is the right design? Usually I hate to say this, but in this case I have to — it depends. The Truman Show (1998) by Peter Weir As we discussed before , a good object is a representative of a real-life entity. Such an entity exists outside of the object's living environment. The object knows how to access it and how to communicate with it. What is that real-life entity in the example above? Each class gives its own answer. And the answer is given by the list of arguments its constructors accept. The first class accepts a single URL as an argument of its constructor. This tells us that the object of this class, after being constructed, will represent a web page. The second class accepts no arguments, which tells us that the object of it will represent ... the Universe. I think this principle is applicable to all classes in object-oriented programming — in order to understand what real-life entity an object represents, look at its constructor. All arguments passed into the constructor and encapsulated by the object identify a real-life entity accessed and managed by the object. Of course, I'm talking about good objects , which are immutable and don't have setters and getters . Pay attention that I'm talking about arguments encapsulated by the object. The following class doesn't represent the Universe, even though it does have a no-arguments constructor: class Time { private final long msec ; public Time () { this ( System . currentTimeMillis ()); } public Time ( long time ) { this . msec = time ; } } This class has two constructors. One of them is the main one, and one is supplementary. We're interested in the main one, which implements the encapsulation of arguments. Now, the question is which is better: to represent a web page or the Universe? It depends, but I think that in general, the smaller the real-life entity we represent, the more solid and cohesive design we give to the object. On the other hand, sometimes we have to have an object that represents the Universe. For example, we may have this: class HTTP { public String read ( String url ) { // read via HTTP and return } public boolean online () { // check whether we're online } } This is not an elegant design, but it demonstrates when it may be necessary to represent the entire Universe. An object of this HTTP class can read any web page from the entire web (it is almost as big as the Universe, isn't it?), and it can check whether the entire web is accessible by it. Obviously, in this case, we don't need it to encapsulate anything. I believe that objects representing the Universe are not good objects, mostly because there is only one Universe; why do we need many representatives of it? :) "},{"title":"You Do Need Independent Technical Reviews!","url":"/2014/12/18/independent-technical-reviews.html","tags":["mgmt"],"date":"2014-12-18 00:00:00 +0000","categories":["jcg"],"body":"Do you have a team of brilliant and enthusiastic programmers? Of course! You've carefully chosen them from a hundred candidates! Are they passionate about the product? Absolutely! They use cutting-edge technologies, never sleep, and hardly eat or drink anything except coffee! Do they believe in your business success? No doubts about it; they live and breathe all those features, releases, continuous delivery, user experience, etc. Are you sure they are developing the product correctly? Well, yes, you're pretty sure; why wouldn't they? ... Does this sound familiar? I can't count how many times I've heard these stories told by startup founders. Most of them are in love with their teams ... until that day when it's time to hire a new one. There could be many possible reasons for such a fiasco, but one of them is a lack of regular, systematic, and independent technical reviews . Nothing demotivates a development team more than a lack of attention to their deliverables. On the other hand, a regular reconciliation of their results and your quality expectations is one of the key factors that will guarantee technical success for your startup. Below I summarize my experience with organizing such technical reviews. Arizona Dream (1992) by Emir Kusturica An independent review is when you ask someone outside of your team to look at your source code and other technical resources and give you an objective opinion about them. Every modern software team should also use internal code reviews, which is is something else entirely. An internal review occurs when one programmer shows his code to other peers on the team and asks their opinion. This usually happens as a daily activity and has nothing to do with independent reviews. An independent review is performed by a programmer who knows nothing about your team. He comes on board, checks out the code from your repository, spends a few hours (or days) looking at it and trying to understand what it does. Then, he tells you what is wrong and where. He explains how he would do it better, where he would change it, and what he would do instead. Then, you pay him and he leaves. You may never see him again, but his conclusions and suggestions help you check the reality of your code and evaluate how your team is really doing. We, at teamed.io , do independent reviews with every project of ours, and this is a list of principles we use: Make Independent Reviews Systematic . This is the first and most important rule — organize such technical reviews regularly. Moreover, inform your team about the schedule, and let them be prepared for the reviews. Once a month is a good practice, according to my experience. Depending on your source code size, a full review should take from two to eight hours . Don't spend more than eight hours; there is no point in going too deep into the code during independent reviews. Pay for Bugs Found . We always pay for bugs, not for the time spent finding them. We ask our reviewers to look at the code and report as many bugs as we think we need. For each bug, we pay 15 minutes for their time. In other words, we assume that a good reviewer can find and report approximately four problems in one hour. For example, a reviewer charges $150 per hour. We hire him and ask him to find and report the 20 most criticial issues he can discover. Our estimate is that he should spend five hours on this work. Thus, he will get $750 when we have 20 bugs in our tracking system reported by him. If he finds fewer, he gets proportionally less money. This payment schedule will help you focus your reviewer on the main objective of the review process — finding and reporting issues. There are no other goals. The only thing you're interested in is knowing what the issues with your current technical solution are. That's what you're paying for. Hire the Best and Pay Well . My experience tells me that the position of an independent reviewer is a very important one. He is not just a programmer but more of an architect who is capable of looking at the solution from a very high level of abstraction, while at the same time paying a lot of attention to details; he should be very good at designing similar systems; he should know how to report a bug correctly and with enough detail; he should understand your business domain; etc. Besides all that, he should be well motivated to help you. You're not hiring him for full-time work but rather just for a few-hour gig. My advice is to try to get the best guys , and pay them as much as they ask, usually over $100 per hour. Don't negotiate, just pay. It's just a few hundred dollars for you, but the effect of their contribution will be huge. Ask For and Expect Criticism . It is a very common mistake to ask a reviewer, \"Do you like our code?\" Don't expect him to tell you how great your code is. This is not what you're paying him for! You already have a full team of programmers for cheering you up; they can tell you a lot about the code they are creating and how awesome it is. You don't want to hear this again from the reviewer. Instead, you want to know what is wrong and needs to be fixed. So your questions should sound like, \"What problems do you think we should fix first?\" Some reviewers will try to please you with positive comments, but ignore that flattery and bring them back to the main goal — bugs. The payment schedule explained above should help. Regularly Change Reviewers . Try not to use the same reviewer more than once on the same project (I mean the same code base). I believe the reason here is obvious, but let me re-iterate: You don't need your reviewer to be nice to you and tell you how great your code is. You want him to be objective and focused on problems, not on bright sides. If you hire the same person again and again, psychologically you make him engaged to the source code. He's seen it once; now he has to see it again. He already told you about some problem, and now he has to repeat it again. He won't feel comfortable doing it. Instead, he will start feeling like a member of the team and will feel responsible for the source code and its mistakes. He, as any other team member, will start hiding issues instead of revealing them. Thus, for every independent technical review, get a new person. Be Polite and Honest With Your Team . Independent reviews can be rather offensive to your programmers. They may think that you don't trust them. They may feel that you don't respect them as technical specialists. They may even decide that you're getting ready to fire them all and are currently looking for new people. This is a very possible and very destructive side effect of an independent review. How do you avoid it? I can't give you universal advice, but the best suggestion I can give is this: be honest with them. Tell them that the quality of the product is critical for you and your business. Explain to them that the business is paying them for their work and that in order to keep paychecks coming, you have to stress quality control — regularly, objectively, independently, and honestly. In the end, if you manage to organize reviews as this article explains, the team will be very thankful to you. They will gain a lot of new ideas and thoughts from every review and will ask you to repeat them regularly. Review From Day One . Don't wait until the end of the project! I've seen this mistake many times. Very often startup founders think that until the product is done and ready for the market, they shouldn't distract their team. They think they should let the team work toward project milestones and take care of quality later, \"when we have a million visitors per day\". This day will never come if you let your team run without control! Start conducting independent reviews from the moment your Git repository has its first file. Until the repository is big enough, you may only spend $300 once a month to receive an objective, independent opinion about its quality. Will this ruin your budget? Prohibit Discussions, and Ask for Formal Reporting . Don't let your reviewers talk to the team. If you do, the entire idea of a review being independent falls apart. If a reviewer is able to ask informal questions and discuss your system design with your programmers, their answers will satisfy him, and he will move on. But you, the owner of the business, will stay exactly where you were before the review. The point of the review is not to make the reviewer happy. It is exactly the opposite. You want to make him confused! If he is confused, your design is wrong and he feels the need to report a bug. The source code should speak for itself, and it should be easy enough for a stranger (the reviewer) to understand how it works. If this is not the case, there is something wrong that should be fixed. Treat Any Question as a Bug . Don't expect a review to produce any bugs in functionality, like \"I click this button and the system crashes\". This will happen rarely, if ever. Your team is very good at discovering these issues and fixing them. Independent reviews are not about that kind of bugs. The main goal of an independent review is to discover bugs in the architecture and design. Your product may work, but its architecture may have serious design flaws that won't allow you, for example, to handle exponential growth in web traffic. An independent reviewer will help you find those flaws and address them sooner than later. In order to get bugs of that kind from the reviewer, you should encourage him to report anything he doesn't like — unmotivated use of a technology, lack of documentation, unclear purpose of a file, absence of a unit test, etc. Remember, the reviewer is not a member of your team and has his own ideas about the technologies you're using and software development in general. You're interested in matching his vision with your team's. Then, you're interested in fixing all critical mismatches. Review Everything, Not Just Source Code . Let your reviewer look at all technical resources you have, not just source code files ( .java , .rb , .php , etc.) Give him access to the database schema, continuous integration panel, build environment, issue tracking system, plans and schedules, work agendas, uptime reports, deployment pipeline, production logs, customer bug reports, statistics, etc. Everything that could help him understand how your system works, and more importantly, where and how it breaks, is very useful. Don't limit the reviewer to the source code only — this is simply not enough! Let him see the big picture, and you will get a much more detailed and professional report. Track How Inconsistencies Are Resolved . Once you get a report from the reviewer, make sure that the most important issues immediately get into your team's backlog. Then, make sure they are addressed and closed. That doesn't mean you should fix them all and listen to everything said by the reviewer. Definitely not! Your architect runs the show, not the reviewer. Your architect should decide what is right and what is wrong in the technical implementation of the product. But it's important to make him resolve all concerns raised by the reviewer. Very often you will get answers like these from him: \"We don't care about it now\", \"we won't fix it until the next release\", or \"he is wrong; we're doing it better\". These answers are perfectly valid, but they have to be given (reviewers are people and they also make mistakes). The answers will help you, the founder, understand what your team is doing and how well they understand their business. If you can offer more suggestions, based on your experience, please post them below in the comments, and I'll add them to the list. I'm still thinking that I may have forgotten something important :) "},{"title":"Immutable Objects Are Not Dumb","url":"/2014/12/22/immutable-objects-not-dumb.html","tags":["oop"],"date":"2014-12-22 00:00:00 +0000","categories":[],"body":"After a few recent posts about immutability, including \"Objects Should Be Immutable\" and \"How an Immutable Object Can Have State and Behavior?\" , I was surprised by the number of comments saying that I badly misunderstood the idea. Most of those comments stated that an immutable object must always behave the same way — that is what immutability is about. What kind of immutability is it, if a method returns different results each time we call it? This is not how well-known immutable classes behave. Take, for example, String , BigInteger , Locale , URI , URL , Inet4Address , UUID , or wrapper classes for primitives, like Double and Integer . Other comments argued against the very definition of an immutable object as a representative of a mutable real-world entity. How could an immutable object represent a mutable entity? Huh? I'm very surprised. This post is going to clarify the definition of an immutable object. First, here is a quick answer. How can an immutable object represent a mutable entity? Look at an immutable class, File , and its methods, for example length() and delete() . The class is immutable, according to Oracle documentation, and its methods may return different values each time we call them. An object of class File , being perfectly immutable, represents a mutable real-world entity, a file on disk. The Usual Suspects (1995) by Bryan Singer In this post , I said that \"an object is immutable if its state can't be modified after it is created.\" This definition is not mine; it's taken from Java Concurrency in Practice by Goetz et al. , Section 3.4 (by the way, I highly recommend you read it). Now look at this class (I'm using jcabi-http to read and write over HTTP): @Immutable class Page { private final URI uri ; Page ( URI addr ) { this . uri = addr ; } public String load () { return new JdkRequest ( this . uri ) . fetch (). body (); } public void save ( String content ) { new JdkRequest ( this . uri ) . method ( \"PUT\" ) . body (). set ( content ). back () . fetch (); } } What is the \"state\" in this class? That's right, this.uri is the state. It uniquely identifies every object of this class, and it is not modifiable. Thus, the class makes only immutable objects. And each object represents a mutable entity of the real world, a web page with a URI. There is no contradiction in this situation. The class is perfectly immutable, while the web page it represents is mutable. Why do most programmers I have talked to believe that if an underlying entity is mutable, an object is mutable too? I think the answer is simple — they think that objects are data structures with methods. That's why, from this point of view, an immutable object is a data structure that never changes. This is where the fallacy is coming from — an object is not a data structure . It is a living organism representing a real-world entity inside the object's living environment (a computer program). It does encapsulate some data, which helps to locate the entity in the real world. The encapsulated data is the coordinates of the entity being represented. In the case of String or URL , the coordinates are the same as the entity itself, but this is just an isolated incident, not a generic rule. An immutable object is not a data structure that doesn't change, even though String , BigInteger , and URL look like one. An object is immutable if and only if it doesn't change the coordinates of the real-world entity it represents. In the Page class above, this means that an object of the class, once instantiated, will never change this.uri . It will always point to the same web page, no matter what. And the object doesn't guarantee anything about the behavior of that web page. The page is a dynamic creature of a real world, living its own life. Our object can't promise anything about the page. The only thing it promises is that it will always stay loyal to that page — it will never forget or change its coordinates. Conceptually speaking, immutability means loyalty, that's all. "},{"title":"How to Be Honest and Keep a Customer","url":"/2015/01/05/how-to-be-honest-and-keep-customer.html","tags":["mgmt"],"date":"2015-01-05 00:00:00 +0000","categories":["jcg"],"body":"Most of our clients are rather surprised when we explain to them that they will have full access to the source code from the first day of the project. We let them see everything that is happening in the project, including the Git repository, bug reports, discussions between programmers, continuous integration fails, etc. They often tell me that other software development outsourcing teams keep this information in-house and deliver only final releases, rarely together with the source code. I understand why other developers are trying to hide as much as possible. Giving a project sponsor full access to the development environment is not easy at all. Here is a summary of problems we've been having and our solutions. I hope they help you honestly show your clients all project internals and still keep them on board. 99 francs (2007) by Jan Kounen He Is Breaking Our Process This is the most popular problem we face with our new clients. Once they gain access to the development environment, they try to give instructions directly to programmers, walking around our existing process . \"I'm paying these guys; why can't I tell them what to do?\" is a very typical mindset. Instead of submitting requests through our standard change management mechanism, such a client goes directly to one of the programmers and tells him what should be fixed, how, and when. It's micro-management in its worst form. We see it very often. What do we do? First, we try to understand why it's happening. The simplest answer is that the client is a moron. Sometimes this is exactly the case, but it's a rare one. Much more often, our clients are not that bad. What is it, then? Why can't they follow the process and abide by the rules? There are a few possible reasons. Maybe the rules are not explained well . This is the most popular root cause — the rules of work are not clear enough for the client. He just doesn't know what he is supposed to do in order to submit a request and get it implemented. To prevent this, we try to educate our clients at the beginning of a new project. We even write guidance manuals for clients. Most of them are happy to read them and learn the way we work, because they understand that this is the best way to achieve success while working with us. Maybe our management is chaotic , and the client is trying to \"organize\" us by giving explicit instructions regarding the most important tasks. We've seen it before, and we are always trying to learn from this. As soon as we see that the client is trying to micro-manage us, we ask ourselves: \"Is our process transparent enough? Do we give enough information to the client about milestones, risks, plans, costs, etc.?\" In most cases, it's our own fault, and we're trying to learn and improve. If so, it's important to react fast, before the client becomes too agressive in his orders and instructions. It will be very difficult to escort him back to the normal process once he gets \"micro-management\" in his blood. Maybe the client is not busy enough and has a lot of free time , which he is happy to spend by giving orders and distracting your team. I've seen this many times. A solution? Keep him busy. Turn him into a member of the team and assign him some tasks related to documentation and research. In my experience, most clients would be happy to do this work and help the project. He Is Asking Too Much A technically-savvy client can turn the life of an architect into a nightmare by constantly asking him to explain every single technical decision made, from \"Why PostgreSQL instead of MySQL?\" to \"Why doesn't this method throw a checked exception?\" Constantly answering such questions can turn a project into a school of programming. Even though he is paying for our time, that doesn't mean we should teach him how to develop software, right? On the other hand, he is interested in knowing how his software is developed and how it works. It's a fair request, isn't it? I believe there is a win-win solution to this problem. Here is how we manage it. First of all, we make all his requests formal. We ask a client to create a new ticket for each request, properly explaining what is not clear and how much detail is expected in the explanation. Second, we look at such requests positively — they are good indicators of certain inconsistencies in the software. If it's not clear for the client why PostgreSQL is used and not MySQL, it's a fault of our architect . He didn't document his decision and didn't explain how it was made, what other options were considered, what selection criteria were applied, etc. Thus, a request from a client is a bug we get for free. So, we look at it positively. Finally, we charge our clients for the answers given. Every question, submitted as a ticket, goes through the full flow and gets billed just as any other ticket. This approach prevents the client from asking for too much. He realizes that we're ready to explain anything he wants, but he will pay for it. He Is Telling Too Much This problem is even bigger than the previous one. Some clients believe they are savvy enough to argue with our architect and our programmers about how the software should be developed. They don't just ask why PostgreSQL is used, they tell us that we should use MySQL, because \"I know that it's a great database; my friend is using it, and his business is growing!\" Sometimes it gets even worse, when suggestions are directed at every class or even a method, like \"You should use a Singleton pattern here!\" Our first choice is to agree and do what he wants. But it's a road to nowhere. Once you do it, your project is ruined, and you should start thinking about a divorce with this client. Your entire team will quickly turn into a group of coding monkeys, micro-managed by someone with some cash. It's a very wrong direction; don't even think about going there. The second choice is to tell the client to mind his own business and let us do ours. He hired us because we're professional enough to develop the software according to his requirements. If he questions our capabilities, he is free to change the contractor. But until then, he has to trust our decisions. Will this work? I doubt it. It's the same as giving him the finger. He will get offended, and you won't get anything. The solution here is to turn the client's demands into project requirements. Most of them will be lost in the process, because they won't be sane enough to form a good requirement. Others will be documented, estimated, and crossed-out by the client himself, becuase he will realize they are pointless or too expensive. Only a few of them will survive, since they will be reasonable enough. And they will help the project. So it is also a win-win solution. For example, he says that \"you should use MySQL because it's great\". You tell him that the project requirements document doesn't limit you to choose whichever database you like. Should it? He says yes, of course! OK, let's try to document such a requirement. How will it sound? How about, \"We should only use great databases?\" Sound correct? If so, then PostgreSQL satisfies this requirement. Problem solved; let us continue to do our work. He will have a hard time figuring out how to write a requirement in a way that disallows PostgreSQL but allows MySQL. It is simply not possible in most cases. Sometimes, though, it will make sense; for example, \"We should use a database server that understands our legacy data in MySQL format\". This is a perfectly sane requirement, and the only way to satisfy it is to use MySQL. Thus, my recommendation is to never take a client's demands directly to execution, but rather use them first to amend the requirements documentation. Even if you don't have such documentation, create a simple one-page document. Agree with the client that you work against this document, and when anyone wants to change something, you first have to amend the document and then have your team ensure the software satisfies it. This kind of discipline will be accepted by any client and will protect you against sudden and distracting corrections. He Is Questioning Our Skills When source code is open to the client, and he is technically capable of reading it, it is very possible that one day he will tell us that our code is crap and we have to learn how to program better. It has not happened in our projects for many years, but it has happened before, when we weren't using static analysis as a mandatory step in our continuous integration pipeline. Another funny possibility is when the client shows the source code to a \"friend\", and he gives a \"professional\" opinion, which sounds like, \"They don't know what they are doing.\" Once such an opinion hits your client's ears, the project is at a significant risk of closure. It'll be very difficult, almost impossible, to convince the client not to listen to the \"friend\" and continue to work with you. That's why most outsourcers prefer to keep their sources private until the very end of the project, when the final invoice is paid. I think that an accidental appearance of a \"friend\" with a negative opinion is un-preventable. If it happens, it happens. You can't avoid it. On the other hand, if you think your code is perfect and your team has only talented programmers writing beautiful software, this is not going to protect you either. An opinion coming from a \"friend\" won't be objective; it will just be very personal, and that's why it's very credible. He is a friend of a client, and he doesn't send him bills every week. Why would he lie? Of course, he is speaking from the heart! (I'm being sarcastic.) So, no matter how beautiful your architecture and your source code is, the \"friend\" will always be right. In my opinion, the only way to prevent such a situation or minimize its consequences is to organize regular and systematic independent technical reviews . They will give confindence to the client that the team is not lying to him about the quality of the product and key technical decisions made internally. To conclude, I strongly believe it is important to be honest and open with each client, no matter how difficult it is. Try to learn from every conflict with each client, and improve your management process and your principles of work. Hiding source code is not professional and makes you look bad in the eyes of your clients and the entire industry. "},{"title":"Daily Stand-Up Meetings Are a Good Tool for a Bad Manager","url":"/2015/01/08/morning-standup-meetings.html","tags":["mgmt"],"date":"2015-01-08 00:00:00 +0000","categories":["best"],"body":"A stand-up meeting (or simply \"stand-up\") is \"a daily team-meeting held to provide a status update to the team members\", according to Wikipedia . In the next few paragraphs, I attempt to explain why these meetings, despite being so popular in software development teams, are pure evil and should never be used by good managers. I'm not saying they can be done right or wrong; there are plenty of articles about that. I'm not trying to give advice about how to do them properly so they work, either. I'm saying that a good manager should never have daily stand-ups. Because they not only \"don't work\" but also do very bad, sometimes catastrophic, things to your management process, whether it's agile or not. On the other hand, a bad manager will always use daily stand-ups as his or her key management instrument. Cool Hand Luke (1967) by Stuart Rosenberg To explain what I mean, let's look at management from a few different angles and compare how good and bad managers would organize their work. Information A Bad Manager Asks How Things Are Going . Strolling around the office asking how things are going is a great habit of a terrible manager. He doesn't know what his team is doing because he is not smart enough to organize the process and information flow correctly. However, he needs to know what's going on because his boss is also asking him from time to time. So the only way to collect the required information is to ask the team, \"What are you working on right now?\" Morning stand-up is a perfect place to ask this annoying question officially without being marked as a manager who doesn't know what he is doing. A Good Manager Is Being Told When Necessary . Managing a project involves management of communications. When information flows are organized correctly, every team member knows when and how he or she has to report to the manager. When something goes wrong, everybody knows how such a situation has to be reported: immediately and directly. When a backlog task is completed, everybody understands how to inform a project manager if he needs this information. A perfect project manager never asks his people. Instead, they tell him when necessary. And when someone does stop to tell him something, a good project manager fixes such a broken communication channel. But he never uses daily meetings to collect information. As a good manager, inform your team what your goals are and what's important to you as a project manager (or Scrum master). They should know what's important for you to know about their progress, risks, impediments, and failures. They should understand what trouble you will get into if they let you down. It is your job, as a good manager, to inform them about the most important issues the project and the team are working through. It's their job, as a good team, to inform you immediately when they have some important information. This is what perfect management is about. If you manage to organize teamwork like that, you won't need to wait until the next morning to ask your developers what they were doing yesterday and what problems they experienced. You would have seen this information earlier, exactly when you needed it. You would stay informed about your project affairs even outside of the office. Actually, you would not need an office at all, but that's a subject for another discussion :) Someone may say that daily stand-ups are a perfect place and time to exchange information among programmers, not just to inform the Scrum master and get his feedback. Again, we have the same argument here — why can't they exchange information when it's required, during the day? Why do we need to put 10 people together every morning to discuss something that concerns only five of them? I can answer. Bad managers, who don't know how else to organize the exchange of information between team members, use morning stand-ups as a replacement for a correct communication model. These morning meetings give the impression that the manager is working hard and well deserves his overblown salary. To the contrary, a good manager would never have any regular status update meetings, becuase he knows how to use effective communication instruments, like issue tracking tools, emails, code reviews, decision-making meetings, pair programming, etc. Responsibility A Bad Manager Micro-Manages . This guy knows very little about project management, and that's why he feels very insecure. He is afraid of losing control of the team; he doesn't trust his own people; and he always feels under-informed and shakes when his own boss asks him, \"What's going on?\" Because of all this, he uses his people as anti-depressant pills — when they are doing what he says, he feels more secure and stable. A daily stand-up meeting is a great place where he can ask each of us what we're doing and then tell us what we should do instead. This manager forces us to disclose our personal goals and plans in order to correct them when he feels necessary. How many times have you heard something like this: \" I'm planning to test X. ... No, next week; today you work with Y \" This is micro-management. Daily stand-ups are the perfect tool for a micro-manager. A Good Manager Delegates Responsibility . Ideal management involves four steps: 1) Breaking a complex task into smaller sub-tasks; 2) Delegating them to subordinates; 3) Declaring awards, penalties, and rules; and 4) Making sure that awards are generous, penalties are inevitable, and rules are strictly followed. A perfect manager never tells his people what to do every day and how to organize their work time. He trusts and controls. He never humiliates his people by telling them how to do their work. A great manager would say: \" You're planning to test X today? It's your decision, and I fully respect it. Just remember that if Y isn't ready by the end of the week, you lose the project, as we agreed. \" Why would such a manager need daily stand-ups? Why would he need to ask his people what they are doing? He is not meddling in their plans. Instead, he trusts them and controls their results only. Let me reiterate: I strongly believe that responsibility must be delegated, and this delegation consists of three components: awards, penalties, and rules . In a modern Western culture, it may be rather difficult to define them — we have long-term contracts and monthly salaries. But a good manager has to find a way. Each task has to be delegated and isolated. This means that the programmer working on the task has to be personally responsible for its success or failure. And he or she has to know the consequences. A good manager understands that any team member inevitably tries to avoid personal responsibility. Everybody is trying to put a responsibility monkey back on the shoulders of the manager. It is natural and inevitable. And daily stand-up meetings only help everybody do this trick. When you ask me in the morning how things are going, I'll say that there are some problems and I'm not sure that I will be able to finish the task by the end of the week. That's it! I'm not responsible for the task anymore. It's not my fault if I fail. I told you that I may fail, remember? From now, the responsibility is yours. A good manager knows about this trick and prevents it by explicitly defining awards, penalties, and rules. When I tell you that I may fail, you remind me that I'm going to lose my awards and will get penalties instead: - I'm not sure I can meet the deadline ... - Sorry to hear that you're going to lose your $200 weekend bonus because of that :( Have you seen many project managers or Scrum masters saying such a thing? Not so many, I believe. Yes, a good manager is a rare creature. But only a good manager is capable of defining awards, penalties, and rules so explicitly and strictly. When this triangle is defined, nobody needs status update meetings every morning. Everything is clear as it is. We all know our goals and our objectives. We know what will happen if we fail, and we also understand how much we're going to get if we succeed. We don't need a manager to remind us about that every morning. And we don't need a manager to check our progress. He already gave us a very clear definition of our objectives. Why would we talk about them again every morning? A bad manager isn't capable of defining objectives; that's why he wants to micro-manage us every morning. Actually, a bad manager is doing it during the day too. He is afraid that without well-known goals and rules, the team will do something wrong or won't do anything at all. That's why he has \"to keep his hand on the pulse\". In reality, he keeps his hand on the neck of the team. Motivation A Bad Manager De-Motivates by Public Embarassment . He doesn't know how to organize a proper motivational system within the team; that's why he relies on a natural fear of public embarassment. It's only logical that no one would feel comfortable saying, \" I forgot it \" in front of everybody. So the daily stand-up meeting is where he puts everybody in a line and asks, \" What did you do yesterday? \" This fearful moment is a great motivator for the team, isn't it? I don't think so. A Good Manager Motivates by Objectives . Ideal management defines objectives and lets people achieve them using their skills, resources, knowledge, and passion. A properly defined objective always has three components: awards, penalties, and rules. A great manager knows how to translate corporate objectives into personal ones: \" If we deliver this feature before the weekend, the company will generate extra profit. You, Sally, will personally get $500. If you fail, you will be moved to another, less interesting project. \" This is a perfectly defined objective. Do we need to ask Sally every morning, in front of everybody, if she forgot to implement the feature? If she is working hard? Will this questioning help her? Absolutely not! She already knows what she is working for, and she is motivated enough. When she finishes on time, organize a meeting and give her a $500 check in front of everybody. This is what a good manager uses meetings for. There's more to this, too, as daily status updates in front of everybody motivate the best team players to backslide and become the same as the worst ones. Well, this is mostly because they don't want to offend anyone by their super performance. It is in our nature to try to look similar to everybody else while being in a group. When everybody reports, \" I still have nothing to show \", it would be strange to expect a good programmer to say, \" I finished all my tasks and want to get more \". Well, this may happen once, but after a few times, this A player will either stop working hard or will change the team. He will see that his performance is standing out and that this can't be appreciated by the group, no matter what the manager says. A good manager understands that each programmer has his or her own speed, quality, and salary. A good manager gives different tasks to different people and expects different results from them. Obviously, lining everybody up in the morning and expecting similar reports from them is a huge mistake. The mistake will have a catastrophic effect on A players, who are interested in achieving super results and expect to be super-appreciated and compensated. A bad manager can't manage different people differently, just because he doesn't know how. That's why he needs daily stand-ups, where everybody reports almost the same, and it's easy to compare their results to each other. Also, it's easier to blame or to cheer up those who don't report similar to others. In other words, a bad manager uses daily stand-ups as an instrument of equality, which in this case only ruins the entire team's motivation. Daily stand-ups, as well as any status update meetings, are a great instrument to hide and protect a lazy and stupid manager. To hide his inability to manage people. To hide his lack of competence. To hide his fear of problems, challenges, and risks. If you're a good manager, don't embarrass yourself with daily stand-ups. "},{"title":"Continuous Integration on Windows, with Appveyor and Maven","url":"/2015/01/10/windows-appveyor-maven.html","tags":["devops"],"date":"2015-01-10 00:00:00 +0000","categories":[],"body":" The purpose of Continuous Integration is to tell us, the developers, when the product we're working on is not \"packagable\" any more. The sooner we get the signal, the better. Why? Because the damage will be younger if we find it sooner. The younger the damage, the easier it is to fix. There are many modern and high-quality hosted continuous integration services , but only one of them (to my knowledge) supports Windows as a build platform — appveyor.com . My experience tells me that it's a good practice to continuously integrate on different platforms at the same time, especially when developing an open source library. That's why, in teamed.io we're using AppVeyor in combination with Travis . This is how I managed to configure AppVeyor to build my Java Maven projects (this is appveyor.yml configuration file you're supposed to place in the root directory of your Github repository): version : '{build}' os : Windows Server 2012 install : - ps : | Add-Type -AssemblyName System.IO.Compression.FileSystem if (!(Test-Path -Path \"C:\\maven\" )) { (new-object System.Net.WebClient).DownloadFile( 'http://www.us.apache.org/dist/maven/maven-3/3.2.5/binaries/apache-maven-3.2.5-bin.zip', 'C:\\maven-bin.zip' ) [System.IO.Compression.ZipFile]::ExtractToDirectory(\"C:\\maven-bin.zip\", \"C:\\maven\") } - cmd : SET PATH=C:\\maven\\apache-maven-3.2.5\\bin;%JAVA_HOME%\\bin;%PATH% - cmd : SET MAVEN_OPTS=-XX:MaxPermSize=2g -Xmx4g - cmd : SET JAVA_OPTS=-XX:MaxPermSize=2g -Xmx4g build_script : - mvn clean package --batch-mode -DskipTest test_script : - mvn clean install --batch-mode cache : - C:\\maven\\ - C:\\Users\\appveyor\\.m2 It was not that easy at all, so I decided to share. You can see how this configuration works in these projects: jcabi-aspects , jcabi-email , jcabi-dynamo , and rultor . "},{"title":"A Compound Name Is a Code Smell","url":"/2015/01/12/compound-name-is-code-smell.html","tags":["oop"],"date":"2015-01-12 00:00:00 +0000","categories":["jcg"],"body":"Do you name variables like textLength , table_name , or current-user-email ? All three are compound names that consist of more than one word. Even though they look more descriptive than name , length , or email , I would strongly recommend avoiding them. I believe a variable name that is more complex than a noun is a code smell. Why? Because we usually give a variable a compound name when its scope is so big and complex that a simple noun would sound ambiguous. And a big, complex scope is an obvious code smell. The Meaning of Life (1983) by Terry Jones and Terry Gilliam The scope of a variable is the place where it is visible, like a method, for example. Look at this Ruby class: class CSV def initialize ( csvFileName ) @fileName = csvFileName end def readRecords () File . readLines ( @fileName ) . map | csvLine | csvLine . split ( ',' ) end end end The visible scope of variable csvFileName is method initialize() , which is a constructor of the class CSV . Why does it need a compound name that consists of three words? Isn't it already clear that a single-argument constructor of class CSV expects the name of a file with comma-separated values? I would rename it to file . Next, the scope of @fileName is the entire CSV class. Renaming a single variable in the class to just @file won't introduce any confusion. It's still clear what file we're dealing with. The same situation exists with the csvLine variable. It is clear that we're dealing with CSV lines here. The csv prefix is just a redundancy. Here is how I would refactor the class: class CSV def initialize ( file ) @file = file end def records () File . readLines ( @file ) . map | line | line . split ( ',' ) end end end Now it looks clear and concise. If you can't perform such a refactoring, it means your scope is too big and/or too complex. An ideal method should deal with up to five variables, and an ideal class should encapsulate up to five properties. If we have five variables, can't we find five nouns to name them? Adam and Eve didn't have second names. They were unique in Eden, as were many other characters in the Old Testament. Second and middle names were invented later in order to resolve ambiguity. To keep your methods and classes clean and solid, and to prevent ambiguity, try to give your variables and methods unique single-word names, just like Adam and Eve were named by you know who :) "},{"title":"How to Cut Corners and Stay Cool","url":"/2015/01/15/how-to-cut-corners.html","tags":["mgmt","agile"],"date":"2015-01-15 00:00:00 +0000","categories":["jcg"],"body":"You have a task assigned to you, and you don't like it. You are simply not in the mood. You don't know how to fix that damn bug. You have no idea how that bloody module was designed, and you don't know how it works. But you have to fix the issue, which was reported by someone who has no clue how this software works. You get frustrated and blame that stupid project manager and programmers who were fired two years ago. You spend hours just to find out how the code works. Then even more hours trying to fix it. In the end, you miss the deadline and everybody blames you . Been there, done that? There is, however, an alternative approach that provides a professional exit from this situation. Here are some tips I recommend to my peers who code with me in teamed.io projects. In a nutshell, I'm going to explain how you can cut corners and remain professional, 1) protecting your nerves, 2) optimizing your project's expenses, and 3) increasing the quality of the source code. Here is a list of options you have, in order of preference. I would recommend you start with the first one on the list and proceed down when you have to. Create Dependencies, Blame Them, and Wait This is the first and most preferable option. If you can't figure out how to fix an issue or how to implement a new feature, it's a fault of the project, not you. Even if you can't figure it out because you don't know anything about Ruby and they hired you to fix bugs in a Ruby on Rails code base — it's their fault. Why did they hire you when you know nothing about Ruby? So be positive; don't blame yourself. If you don't know how this damn code works, it's a fault of the code, not you. Good code is easy to understand and maintain. Don't try to eat spaghetti code; complain to the chef and ask him or her to cook something better (BTW, I love spaghetti). How can you do that? Create dependencies — new bugs complaining about unclear design, lack of unit tests, absence of necessary classes, or whatever. Be creative and offensive — in a constructive and professional way, of course. Don't get personal. No matter who cooked that spaghetti, you have nothing against him or her personally. You just want another dish, that's all. Once you have those dependencies reported, explain in the main ticket that you can't continue until all of them are resolved. You will legally stop working, and someone else will improve the code you need. Later, when all dependencies are resolved and the code looks better, try to get back to it again. If you still see issues, create new dependencies. Keep doing this until the code in front of you is clean and easy to fix. Don't be a hero — don't rush into fixing the bad code you inherited. Think like a developer, not a hacker . Remember that your first and most important responsibility as a disciplined engineer is to help the project reveal maintainability issues. Who will fix them and how is the responsibility of a project manager. Your job is to reveal, not to hide. By being a hero and trying to fix everything in the scope of a single task, you're not doing the project a favor — you're concealing the problem(s). Edit: Another good example of a dependency may be a question raised at, for example, StackOverflow.com or a user list of a third-party library. If you can't find a solution yourself and the problem is outside of the scope of your project — submit a question to SO and put its link to the source code (in JavaDoc block, for example). Demand Better Documentation and Wait All dependencies are resolved and the code looks clean, but you still don't understand how to fix the problem or implement a new feature. It's too complex. Or maybe you just don't know how this library works. Or you've never done anything like that before. Anyhow, you can't continue because you don't understand. And in order to understand, you will need a lot of time — much more than you have from your project manager or your Scrum board. What do you do? Again, think positively and don't blame yourself. If the software is not clear enough for a total stranger, it's \"their\" fault, not yours. They created the software in a way that's difficult to digest and modify. But the code is clean; it's not spaghetti anymore. It's a perfectly cooked lobster, but you don't know how to eat lobster! You've never ate it before. The chef did a good job; he cooked it well, but the restaraunt didn't give you any instructions on how to eat such a sophisticated dish. What do you do? You ask for a manual. You ask for documentation. Properly designed and written source code must be properly documented. Once you see that something is not clear for you, create new dependencies that ask for better documentation of certain aspects of the code. Again, don't be a hero and try to understand everything yourself. Of course you're a smart guy, but the project doesn't need a single smart guy. The project needs maintainable code that is easy to modify, even by someone who is not as smart as yourself. So do your project a favor: reveal the documentation issue, and ask someone to fix it for you. Not just for you, for everybody. The entire team will benefit from such a request. Once the documentation is fixed, you will continue with your task, and everybody will get source code that is a bit better than it was before. Win-win, isn't it? Reproduce the Bug and Call It a Day Now the code is clean, the documentation is good enough, but you're stuck anyway. What to do? Well, I'm a big fan of test-driven development, so my next suggestion would be to create a test that reproduces the bug. Basically, this is what you should start every ticket with, be it a bug or a feature. Catch the bug with a unit test! Prove that the bug exists by failing the build with a new test. This may be rather difficult to achieve, especially when the software you're trying to fix or modify was written by idiots someone who had no idea about unit testing. There are plenty of techniques that may help you find a way to make such software more testable. I would highly recommend you read Working Effectively with Legacy Code by Michael Feathers. There are many different patterns, and most of them work. Once you manage to reproduce the bug and the build fails, stop right there. That's more than enough for a single piece of work. Skip the test (for example, using @Ignore annotation in JUnit 4) and commit your changes. Then add documentation to the unit test you just created, preferably in the form of a @todo . Explain there that you managed to reproduce the problem but didn't have enough time to fix it. Or maybe you just don't know how to fix it. Be honest and give all possible details. I believe that catching a bug with a unit test is, in most cases, more than 80% of success. The rest is way more simple: just fix the code and make the test pass. Leave this job to someone else. Prove a Bug's Absence Very often you simply can't reproduce a bug. That's not because the code is not testable and can't be used in a unit test but because you can't reproduce an error situation. You know that the code crashes in production, but you can't crash it in a test. The error stack trace reported by the end user or your production logging system is not reproducable. It's a very common situation. What do you do? I think the best option here is to create a test that will prove that the code works as intended. The test won't fail, and the build will remain clean. You will commit it to the repository and ... report that the problem is solved. You will say that the reported bug doesn't really exist in real life. You will state that there is no bug — \"our software works correctly; here is the proof: see my new unit test.\" Will they believe you? I don't think so, but they don't have a choice. They can't push you any further. You've already done something — created a new test that proves everything is fine. The ticket will be closed and the project will move on. If, later on, the same problem occurs in production, a new bug will be reported. It will be linked to your ticket. Your experience will help someone investigate the bug further. Maybe that guy will also fail to catch the bug with a test and will also create a new, successful and \"useless\" test. And this may happen again and again. Eventually, this cumulative group experience will help the last guy catch the bug and fix it. Thus, a new passing test is a good response to a bug that you can't catch with a unit test. Disable the Feature Sometimes the unit test technique won't work, mostly because a bug will be too important to be ignored. They won't agree with you when you show them a unit test that proves the bug doesn't exist. They will tell you that \"when our users are trying to download a PDF, they get a blank page.\" And they will also say they don't really care about your bloody unit tests. All they care about is that PDF document that should be downloadable. So the trick with a unit test won't work. What do you do? It depends on many factors, and most of these factors are not technical. They are political, organizational, managerial, social, you name it. However, in most cases, I would recommend you disable that toxic feature, release a new version, and close the ticket. You will take the problem off your shoulders and everybody will be pleased. Well, except that poor end user. But this is not your problem. This is the fault of management, which didn't organize pre-production testing properly. Again, don't take this blame on yourself. Your job is to keep the code clean and finish your tickets in a reasonable amount of time. Their job is to make sure that developers, testers, DevOps, marketers, product managers, and designers work together to deliver the product with an acceptable number of errors. Production errors are not programmers' mistakes, though delayed tickets are. If you keep a ticket in your hands for too long, you become an unmanageable unit of work. They simply can't manage you anymore. You're doing something, trying to fix the bug, saying \"I'm trying, I'm trying ...\" How can they manage such a guy? Instead, you should deliver quickly, even if it comes at the cost of a temporarily disabled feature. Say No OK, let's say none of the above works. The code is clean, the documentation is acceptable, but you can't catch the bug, and they don't accept a unit test from you as proof of the bug's absence. They also don't allow you to disable a feature, because it is critical to the user experience. What choices do you have? Just one. Be professional and say \"No, I can't do this; find someone else.\" Being a professional developer doesn't mean being able to fix any problem. Instead, it means honesty. If you see that you can't fix the problem, say so as soon as possible. Let them decide what to do. If they eventually decide to fire you because of that, you will remain a professional. They will remember you as a guy who was honest and took his reputation seriously. In the end, you will win. Don't hold the task in your hands. The minute you realize you're not the best guy for it or you simply can't fix it — notify your manager. Make it his problem. Actually, it is his problem in the first place. He hired you. He interviewed you. He decided to give you this task. He estimated your abilities and your skills. So it's payback time. Your \"No!\" will be very valuable feedback for him. It will help him make his next important management decisions. On the other hand, if you lie just to give the impression you're a guy who can fix anything and yet fail in the end, you will damage not only your reputation but also the project's performance and objectives. "},{"title":"If. Then. Throw. Else. WTF?","url":"/2015/01/21/if-then-throw-else.html","tags":["java","oop"],"date":"2015-01-21 00:00:00 +0000","categories":["jcg"],"body":"This is the code I could never understand: if ( x < 0 ) { throw new Exception ( \"X can't be negative\" ); } else { System . out . println ( \"X is positive or zero\" ); } I have been trying to find a proper metaphor to explain its incorrectness. Today I finally found it. If-then-else is a forking mechanism of procedural programming. The CPU either goes to the left and then does something or goes to the right and does something else . Imagine yourself driving a car and seeing this sign: It looks logical, doesn't it? You can go in the left lane if you're not driving a truck. Otherwise you should go in the right lane. Both lanes meet up in a while. No matter which one you choose, you will end up on the same road. This is what this code block does: if ( x < 0 ) { System . out . println ( \"X is negative\" ); } else { System . out . println ( \"X is positive or zero\" ); } Now, try to imagine this sign: It looks very strange to me, and you will never see this sign anywhere simply because a dead end means an end , a full stop, a finish. What is the point of drawing a lane after the dead end sign? There is no point. This is how a proper sign would look: This is how a proper code block would look: if ( x < 0 ) { throw new Exception ( \"X can't be negative\" ); } System . out . println ( \"X is positive or zero\" ); The same is true for loops. This is wrong: for ( int x : numbers ) { if ( x < 0 ) { continue ; } else { System . out . println ( \"found positive number\" ); } } While this is right: for ( int x : numbers ) { if ( x < 0 ) { continue ; } System . out . println ( \"found positive number\" ); } There is no road after the dead end! If you draw it, your code looks like this very funny snippet I found a few years ago reviewing sources written by some very well-paid developer in one very serious company: if ( x < 0 ) { throw new Exception ( \"X is negative\" ); System . exit ( 1 ); } Don't do this. "},{"title":"Making Your Boss Happy Is a False Objective","url":"/2015/01/26/happy-boss-false-objective.html","tags":["mgmt"],"date":"2015-01-26 00:00:00 +0000","categories":["jcg"],"body":"We all have bosses. We also have customers who pay us for running their software projects. They are my bosses for the time of the contract. I'm also acting as a boss for developers who are working for teamed.io . It is obvious that a good employee/contractor is one who makes his boss/customer happy. But only a bad employee works toward this goal. Trying to make your boss happy is a false target that, if pursued, ruins the project. A professional employee works for the project, not for the boss. The Million Dollar Hotel (2000) by Wim Wenders We all work on projects as developers, designers, programmers, managers, testers, you name it. The boss is also a member of the project. More formally, he or she is a stakeholder , same as every one of us. Each stakeholder has his own needs for the project: 1) Jeff, the developer, wants to learn Scala and collect his paychecks every two weeks; 2) Sally, the product owner, wants to attend an expo in Paris and also collect her paychecks; 3) Bob, the CTO, wants to raise round A funding and collect a big paycheck; etc. The project has its own objectives , to achieve 1 million downloads in less than six months and under $300,000, for example. This is what the project works for. This is what all of us are here for. Our personal needs may be fully satisfied while we're all working toward this goal, or some of them may be sacrificed. I mean all of us, including the boss, whoever he or she is, either a CTO, a co-founder, a project manager, or a team lead. The project is the source of our checks. Not the CFO. The CFO is a stakeholder, like everyone else. The project gives him more power than others because it's necessary for the whole mechanism to work properly. Every project member has his or her own roles and responsibilities . I write code; the CFO writes checks. I eat at McDonalds; he drives a Jaguar. We have different needs, and we both agreed that the project would satisfy them. Otherwise we wouldn't be here, right? We're all parts of a mechanism called a \"project\", which works according to the rules and principles of project management whether we are aware of them or not. Whether we have a project manager or not. Even if we violate all of them and manage ourselves in total chaos, we still have a scope, cost, schedule, and all other attributes of project management. A professional and savvy boss understands that his role in the mechanism is to clearly define project objectives and make sure everybody's needs are aligned with those objectives. In a properly managed and organized project, everybody sees and feels how his or her personal needs are satisfied when the project achieves its objectives: Jeff learns Scala, Sally sees Paris, and Bob buys a new house. However, if Jeff wants to learn Scala and we're developing an iOS application, that is a problem for the boss to resolve. Either convince Jeff to fall in love with Swift (I doubt that's possible) or replace him with someone who is already in love with it. It's clear that a professional boss will resort to such a tragic act as firing Jeff not because of his personal feelings towards Jeff but because they are both working toward the project objectives. Jeff and the boss will both understand that Jeff's need to learn Scala is not aligned with the objective of the project. It is the CTO's responsibility to do something about Jeff when his personal needs become misaligned with the objectives of the project that is paying his salary. A professional CEO understands that and always acts in the best interest of the project, not of himself or anyone else personally. I believe a professional team player does two things: obeys and resists . First, you have to understand that the boss is here in order to help you organize your time, your tasks, your communications, your plans, etc. He knows more about the project and uses that information to help you do your job. Your real boss is the project; the boss you interact with is just a hired manager who translates project objectives into plans, instructions, schedules, etc. This boss is your colleague who does management while you're writing code. You're both equal. You and he are in the same boat. Your functions are different than his; that's all. You're not working for him but with him on a project. A true professional team player feels himself equal to all other members of the project, no matter how high they are in the hierarchy. At the same time, he strictly follows the process and obeys all project rules and instructions, not because he is afraid of being fired but because he wants the project to succeed. Second, being a professional team player requires a constant readiness to resist each and every instruction if you feel it contradicts the project objectives. A true professional doesn't work for a boss. He doesn't want to make the boss happy. He actually doesn't care whether the boss is happy or not. He knows that the real boss is the project and tries to make the project successful and ... happy. A true professional always works for himself. Jeff wants to learn Scala and earn a certain amount of cash. He joined the project in order to satisfy these needs. If the project fails, Jeff won't get the money and won't fully learn Scala. So if the boss tells Jeff to do something that may jeopardize the project's success, will Jeff do it? Does he care about disappointing the boss? Absolutely not. All he cares about is the project's success, which translates to his personal success. Thus, making your boss happy is a goal for the immature, fearsome, lazy, and weak. Making your project successful is an objective for professional, strong, mature, and brave team players. "},{"title":"XSL Transformation in Java: An Easy Way","url":"/2015/02/02/xsl-transformations-in-java.html","tags":["xml","java","jcabi","xslt"],"date":"2015-02-02 00:00:00 +0000","categories":["jcg"],"body":" XSL transformation (XSLT) is a powerful mechanism for converting one XML document into another. However, in Java, XML manipulations are rather verbose and complex. Even for a simple XSL transformation, you have to write a few dozen lines of code — and maybe even more than that if proper exception handling and logging is needed. jcabi-xml is a small open source library that makes life much easier by enabling XML parsing and XPath traversing with a few simple methods . Let's see how this library helps in XSL transformations. First, take a look at a practical example — rultor.com — a hosted DevOps assistant that automates release, merge, and deploy operations. Rultor keeps each conversation session with an end user (a.k.a. \"talk\") in a DynamoDB record. There are multiple situations to handle in each talk; that's why using multiple columns of a record is not really feasible. Instead, we're keeping only a few parameters of each talk in record columns (like ID and name) and putting all the rest in a single XML column. This is approximately how our DynamoDB table looks: +----+---------------+--------------------------------------+ | id | name | xml | +----+---------------+--------------------------------------+ | 12 | jcabi-xml#54 | <?xml version='1.0'?> | | | | <talk public=\"true\"> | | | | <request id=\"e5f4b3\">...</request> | | | | </talk> | +----+---------------+--------------------------------------+ | 13 | jcabi-email#2 | <?xml version='1.0'?> | | | | <talk public=\"true\"> | | | | <daemon id=\"f787fe\">...</daemon> | | | | </talk> | +----+---------------+--------------------------------------+ Once a user posts @rultor status into a Github ticket, Rultor has to answer with a full status report about the current talk. In order to create such a text answer (a regular user would not appreciate an XML response), we have to fetch that xml column from the necessary DynamoDB record and convert it to plain English text. Here is how we're doing that with the help of jcabi-xml and its class, XSLDocument . final String xml = // comes from DynamoDB final XSL xsl = new XSLDocument ( this . getClass (). getResourceAsStream ( \"status.xsl\" ) ); final String text = xsl . applyTo ( xml ); That's it. Now let's see what's there in that status.xsl file (this is just a skeleton of it; the full version is here ): <xsl:stylesheet xmlns:xsl= \"http://www.w3.org/1999/XSL/Transform\" version= \"2.0\" > <xsl:output method= \"text\" /> <xsl:template match= \"/talk\" > <xsl:text> Hi, here is your status report: </xsl:text> ... </xsl:template> <xsl:template match= \"node()|@*\" > <xsl:copy> <xsl:apply-templates select= \"node()|@*\" /> </xsl:copy> </xsl:template> </xsl:stylesheet> It is good practice to create XSL documents only once per application run. We have a static utility method XSLDocument.make() for this: final class Foo { private static final XSL STYLESHEET = XSLDocument . make ( Foo . class . getResourceAsStream ( \"stylesheet.xsl\" ) ); public XML style ( final XML xml ) { return Foo . STYLESHEET . transform ( xml ); } } Pay attention to the fact we're using XSLT 2.0. Built-in Java implementation of XSLT doesn't support version 2.0, and in order to make it work, we're using these two Maven Saxon dependencies: <dependency> <groupId> net.sourceforge.saxon </groupId> <artifactId> saxon </artifactId> <version> 9.1.0.8 </version> <scope> runtime </scope> </dependency> <dependency> <groupId> net.sourceforge.saxon </groupId> <artifactId> saxon </artifactId> <version> 9.1.0.8 </version> <classifier> xpath </classifier> <scope> runtime </scope> </dependency> All you need to do to start using jcabi-xml for XSL transformations is add this dependency to your pom.xml : <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-xml </artifactId> </dependency> If you have any problems or suggestions, don't hesitate to submit an issue to the Github issue tracker . "},{"title":"Don't Repeat Yourself in Maven POMs; Use Jcabi-Parent","url":"/2015/02/05/jcabi-parent-maven-pom.html","tags":["java","maven","jcabi"],"date":"2015-02-05 00:00:00 +0000","categories":["jcg"],"body":" Maven is a build automation tool mostly for Java projects. It's a great tool, but it has one important drawback that has motivated the creation of similar tools, like Gradle and SBT. That weakness is its verbosity of configuration. Maven gets all project build parameters from pom.xml , an XML file that can get very long. I've seen POM files of 3,000-plus lines. Taking into account 1) recent DSL buzz and 2) fear of XML, it's only logical that many people don't like Maven because of its pom.xml verbosity. But even if you're an XML fan who enjoys its strictness and elegance (like myself), you won't like the necessity to repeat yourself in pom.xml for every project. If you're working on multiple projects, code duplication will be enormous. An average Java web app uses a few dozen standard Maven plugins and almost the same number of pretty common dependencies, like JUnit, Apache Commons, Log4J, Mockito, etc. All of them have their versions and configurations, which have to be specified if you want to keep the project stable and avoid Maven warnings. Thus, once a new version of a plugin is released, you have to go through all pom.xml files in the projects you're working on and update it there. You obviously understand what code duplication means. It's a disaster. However, there is a solution. jcabi-parent is a very simple Maven dependency with nothing inside it except a large pom.xml with multiple pre-configured dependencies, profiles, and plugins. All you need to do in order to reuse them all in your project is define com.jcabi:parent as your parent POM: <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <parent> <groupId> com.jcabi </groupId> <artifactId> parent </artifactId> <!-- check the latest version at http://parent.jcabi.com --> <version> 0.32.1 </version> </parent> [...] </project> That's all you need. Now you can remove most of your custom configurations from pom.xml and rely on defaults provided by jcabi-parent. Its pom.xml is rather large and properly configured. Multiple projects depend on it, so you can be confident that you're using the best possible configuration of all standard plugins. Here are a few examples of pom.xml from projects that are using jcabi-parent (you can see how compact they are): Xembly ReXSL jcabi-http Qulice "},{"title":"Four NOs of a Serious Code Reviewer","url":"/2015/02/09/serious-code-reviewer.html","tags":["mgmt","agile"],"date":"2015-02-09 00:00:00 +0000","categories":[],"body":"Code reviews (a.k.a. peer reviews) must be a mandatory practice for every serious software development team. I hope there is no debate about this. Some do pre-merge code reviews, protecting their master/development branch from accidental mistakes. Others do post-merge regular reviews to discover bugs and inconsistencies after they are introduced by their authors. Some even do both, reviewing before merges and regularly after. Code reviews are very similar to a white-box testing technique where a tester looks for defects with full access to the sources of the software. In either case, a code review is a great instrument to increase quality and boost team motivation. However, it's not so simple to do them right. I would even say it's very easy and comfortable to do them wrong. Most code reviews and reviewers I've seen make similar mistakes. That's why I decided to summarize the four basic principles of a good reviewer as I see them. Hopefully you find them helpful. Kim Jong-un with colleagues (2014) No Fear There are a few different fears a serious code reviewer should renounce. The first and most popular is the fear of offending an author of the code. \" I'd better close my eyes and pretend I didn't see her bugs today so tomorrow she will ignore my mistakes \" — This is the kind of attitude this fear produces. Needless to say, it's counterproductive and degrades code quality and team morale. Here is my advice: be direct, honest, and straight-forward. If you don't like the code, you don't like it. You shouldn't care how your opinion will be taken by the author of the code. If you do have such feelings toward your colleagues, there is something wrong with the management model. You're afraid of being rejected by the team for \"not being a team player\", which is a label attached to you by the weakest members of the team, not by the project sponsor. The sponsor pays you to produce high-quality software. The sponsor doesn't care how much your intention to increase quality offends others, who care less. The sponsor wants his money to produce deliverables that can be sold to customers and returned back in profit. The sponsor is not paying you to make friends in the office. The next type of fear sounds like this: \" If I reject this code, I will delay the release \" Again, it goes without saying that such an attitude does a significant disservice to the entire project. You will accept the code and close your eyes to what you don't like in it. The code will go into the next release, and we'll ship it sooner. You won't be a bottleneck, and nobody will say that because of that dogmatic code reviewer, we delayed the release and lost some cash. You will be a good team player, right? Wrong! As I've mentioned before , a professional team player understands his or her personal role in a project and doesn't cover anyone's ass. If the rejection of bad code delays the release, that's the fault of its author. Your professional responsibility is to make this fault visible. That's how you help the team learn and improve. I think it's obvious that the education and improvement of a team first requires getting rid of bad programmers and promoting good ones. Honest and fearless code reviewers help the team learn and improve. Yet another fear is expressed like this: \" I may be wrong and they will laugh me out \" Even worse, they may spot my lack of knowledge. They may see that I don't know what I'm doing. It would be better to stay quiet and pretend there are no bugs in the code. At least then I wouldn't embarass myself with stupid comments. You know that it's much easier to look smart if you keep your mouth shut, right? Wrong! The project is not paying you to look good. You're getting your paychecks not because the team loves you but because you produce deliverables on a daily basis. Your professional responsibility is to do what's best for the project and ignore everyone's opinions, including the opinion of your boss. Similar to \"A Happy Boss Is a False Objective\" , I would say that the respect of the team is a false goal. Don't try to earn respect. Instead, create clean code and respect will come automatically. Let me reiterate: Don't be afraid to embarass yourself by making incorrect and stupid comments about someone's code. Be loyal to the project, not to the expectations of people around you. They expect you to be smart and bright, but the project expects you to say what you think about the code. So screw their opinions; do the right thing and say what you really think. No Compromise Okay, you've fearlessly said what you thought about the code and simply rejected it. The branch you were reviewing is not good, and you explained why. You asked its author to refactor here and re-write there. What's next? He or she will try to make a deal with you. It's natural and it's happening in almost every branch I'm seeing in our teams. The author of the code is also a professional developer, and he also has no fear. So he insists that his implementation approach is right and your ideas are wrong. What should a professional code reviewer do in this case? The worst thing (as in any conflict resolution) is a compromise. This is what ruins quality faster than bad code. A compromise is a conflict resolution technique for which both parties agree not to get what they wanted just for the sake of ceasing the conflict. In other words, \" Let's make peace just to stop fighting \" It's the worst approach ever. Instead of a lousy compromise, there are three professional exits from a fight over a piece of code: \" You're right; I take my comments back! \" This may happen, and it should happen very often. You should be ready to admit your mistakes. You didn't like the code, but its author explained to you its benefits, and you accepted the logic — not because you want to stop fighting with him but because you really understood the logic and accepted it. Willingness to say, \"I'm wrong\", is the first sign of a mature and serious developer. \" I will never accept this, period! \" Some code deserves that, and there is nothing wrong with resolving a conflict this way. The opponent may accept the situation and re-write everything. And learn something too. \" Let's do what the architect says! \" In every project, there is a software architect who makes final decisions. Appeal to his opinion and get his final decision. Invite him into the discussion, and ask him to take one side in the conflict. Once he tells you that you're wrong, accept the decision and try to learn from it. Thus, either stand strong on your position and fight for it or admit that you're wrong. One way or the other. But don't make a compromise! Don't get me wrong; it's not about being stubborn and holding your cards no matter how bad they are. Be flexible and learn on the spot. Your position may and should change during the negotiation, but don't accept anything that you don't like. You can exit the conflict either by being fully convinced that the opponent is right or when the architect says so. Nothing in between. No Bullshit Again, you fearlessly said that a method should be designed differently. Your opponent, the author of the code, replies that he doesn't think so. You take a look again and decide to stand behind your position. You still think you're right, and you're not going to make a compromise. Now what? It's too early to call an architect, so try to convince your opponent. In most cases, convincing is teaching. You know something that he doesn't know. That's why he created that method the way you don't like. One of you needs some additional education. Here is an opportunity for you to be a teacher of your colleague. To be an effective teacher, you need to show proof. You need to ground your logic and make sure he understands and accepts it. Be ready to show links, articles, books, reports, examples, etc. Just \" I know this because I've been writing Java for 15 years \" is not enough. Moreover, this type of argument only makes you less convincing. If you don't have enough convincing proof, think again — maybe you are wrong. Also, remember that it's your job to prove that the code you're reviewing is bad. The author of the code should not prove anything. His code is great by default. The job of the reviewer is to show why and how that's actually not the case. In other words, you're the plaintiff and he is the defender. Not the other way around. No Offense This is the last and most difficult principle to follow. No matter how bad the code is and how stubborn your opponent is, you must remain professional. To be honest, I find this very difficult sometimes. At teamed.io , we're working in distributed teams and hire a few new people every week. Some of them, despite our screening criteria , appear to be rather stupid difficult to deal with. I encountered a funny situation about a year ago when a new guy was supposed to create a small (20 to 30 lines of code) new feature in an existing Java library. He sent me a pull request (I was a code reviewer) after he put in a few hundred lines of code. That code was absolute garbage and obviously not written by him. I immediately understood that he found it somewhere and copied it. But what could I do? How could I reject it without saying his attitude was unacceptable for a professional developer? I had to spend some time objectively blaming his code for its style, its design, etc. I had to make many serious comments in order to show him that to fix it all, he should delete the garbage and re-write it from scratch. I never saw him again after that task. My point is that it's easy to be professional when you're dealing with professionals. Unfortunately, that's not always the case. But no matter how bad the code in front of you is, be patient and convincing. "},{"title":"Code For the User, Not for Yourself","url":"/2015/02/12/top-down-design.html","tags":["mgmt","agile"],"date":"2015-02-12 00:00:00 +0000","categories":["jcg"],"body":"First, no matter what the methodology is, we all write software for our users (a.k.a. customers, project sponsors, end users, or clients). Second, no matter what the methodology is, we write incrementally, releasing features and bug fixes one by one. Maybe I'm saying something absolutely obvious here, but it's important to remember that each new version should first of all satisfy the needs of the user, not of us programmers. In other words, the way we decompose a big task into smaller pieces should be user-targeted, and that's why you always work top down . Let's see what I mean through a practical example. Delicatessen (1991) by Jean-Pierre Jeunet Say I'm contracted by a friend of mine to create a word-counting command line tool very similar to wc . He promised to pay me $200 for this work, and I promised him I'd deliver the product in two increments — an alpha and beta version. I promised him I'd release the alpha version on Saturday and the beta version on Sunday. He is going to pay me $100 after the first release and the rest after the second release. I'll write in C, and he will pay in cash. The tool is very primitive, and it only took me a few minutes to write. Take a look at it: #include <stdio.h> #include <unistd.h> int main () { char ch ; int count = 0 ; while ( 1 ) { if ( read ( STDIN_FILENO , & ch , 1 ) <= 0 ) { break ; } if ( ch == ' ' ) { ++ count ; } } if ( count > 0 ) { ++ count ; } printf ( \"%d \\n \" , count ); return 0 ; } But let's be professional and not forget about build automation and unit testing. Here is a simple Makefile that does them both: all : wc test wc : wc . c gcc -o wc wc.c test : wc echo '' | ./wc | grep '0' echo 'Hello, world! How are you?' | ./wc | grep '5' Now I run make from a command line and get this output: $ make echo '' | ./wc | grep '0' 0 echo 'Hello, world! How are you?' | ./wc | grep '5' 5 All clean! I'm ready to get my $200. Wait, the deal was to deliver two versions and get cash in two installments. Let's back up a little and think — how can we break this small tool into two parts? On first thought, let's release the tool itself first and build automation and testing next. Is that a good idea? Can we deliver any software without running it first with a test? How can I be sure that it works if I don't ship tests together with it? What will my friend think about me releasing anything without tests? This would be a total embarassment. Okay, let's release Makefile first and wc.c next. But what will my friend do with a couple of tests and no product in hand? This first release will be absolutely pointless, and I won't get my $100. Now we're getting to the point of this article. What I'm trying to say is that every new increment must add some value to the product as it is perceived by the customer, not by us programmers. The Makefile is definitely a valuable artifact, but it provides no value to my friend. He doesn't need it, but I need it. Here is what I'm going to do. I'll release a skeleton of the tool, backed by the tests but with an absolutely dummy implementation. Look at it: #include <stdio.h> int main () { printf ( \"5 \\n \" ); return 0 ; } And I will modify the Makefile accordingly. I will disable the first test to make sure the build passes. Does my tool work? Yes, it does. Does it count words? Yes, it does for some inputs. Does it have value to my friend. Obviously! He can run it from the command line, and he can pass a file as an input. He will always get number \"5\" as a result of counting, though. That's a bummer, but it's an alpha version. He doesn't expect it to work perfectly. However, it works, it is backed by tests, and it is properly packaged. What I just did is a top-down approach to design. First of all, I created something that provides value to my customer. I made sure it also satisfies my technical objectives, like proper unit test coverage and build automation. But the most important goal for me was to make sure my friend received something ... and paid me. "},{"title":"It's Not a School!","url":"/2015/02/16/it-is-not-a-school.html","tags":["mgmt","agile"],"date":"2015-02-16 00:00:00 +0000","categories":[],"body":"At teamed.io , we work in distributed teams and keep all our communications in tickets . Besides that, we encourage every developer on every project to report bugs whenever he or she finds them. We even pay for each bug found. Once in a while, I see bugs reported along these lines: \" Can someone explain to me how to design this module? \" or even \" I haven't used this library before; please help me get started .\" My usual answer is, \" This is not a school; nobody is going to teach you here! \" I realize this sounds rather harsh to most developers who are just starting to work with us, so here I'll try to illustrate why such an attitude makes sense and is beneficial to both the programmers and the project. Disclaimer: I'm talking about software projects here, which PMBOK defines as \" temporary endeavors undertaken to create unique products, services, or results\". If your team is engaged in continuous development or maintenance of software, this concept may not be relevant. G.I. Jane (1997) by Ridley Scott No matter how unpleasant this could be, let's face the reality: each software project is a business, and we, the developers, are its resources . Just like concrete, wood, and glass are the resources required to build a house, which is also a business activity. No matter how much we think about ourselves as a family having fun together and writing code because we enjoy it, each business looks at it completely differently. The project needs us to produce classes, lines of code, methods, functions, files, and features. Then, the project can convert them into happy customers, which will give us something back — usually cash. Finally, the project will share that cash among us, investors, and the government. A properly planned and managed project acquires the best resources its budget can afford and then relies on their quality. A programmer who doesn't have adequate skills or knowledge is an unreliable resource. Obviously, no project would acquire such a resource from the start. However, this weakness may be revealed in the middle of the project. Say you're building a house and you contracted a plumber to install a drainage system. When it's time to mount the equipment, he tells you that he's never worked with it and doesn't know how to install it. It was a risk, and it occurred. A good project manager always has a fallback plan or even a few of them. Obviously, the best option would be to contract another plumber. The worst option would be to train the original one on the spot. Wait, why is that so obvious? The plumber is a great guy. Yes, he doesn't know how to work with this equipment, but that doesn't mean we should fire him immediately. Let's pay for his training, send him to some courses, buy him some books, let him experiment with the equipment for some time, and then he will be able to install it in our house. Great plan, isn't it? The plumber will be happy. But the project won't. The goal of the project is to build a house, not to train a plumber. The project doesn't even have funds to train the bloody plumber! If we train and teach all our workers, we won't ever build a house. We're not running a school here. We're building a house! While working on a software project, a good project manager has a staffing management plan that describes how skills will be recruited, tested, applied, and discharged when necessary. Such a plan may include training, but it would be as small an amount as possible — mostly because a plumber trained by us costs much more than one trained by someone else but does exactly the same, or worse, work. Thus, a smart project manager acquires project members who are already capable of performing their duties and falls back on trainings only in exceptional situations. Now, a logical question: What should we, as programmers, do? We want to learn, and we don't want to spend our own money on it. We don't want to sit home for a few years learning all possible technologies before entering the job market as experts, ready to be hired. We want to learn on the job. We basically want to use project budgets for our own personal educational needs. Moreover, a smart programmer exits every project with some new knowledge, new skills, and new technologies in his or her portfolio. However, if you make your projects spend their money on your education, that's theft. And a good project manager should stop you, saying \"This is not a school!\" What Is the Solution? I believe that in the software business, there is only one workaround — blame the project for your own deficiencies in education, skills, or information. I'm being absolutely serious. Let's discuss a few practical situations. Say you have a module to work with, and it has to be written in Python. You have no experience in Python; you're a Java developer. Who is at fault here? You could think of it as your problem and ask your project manager to teach you, but he should tell you he's not running a school and get rid of you. That's a bad scenario for both of you. Instead, blame the project manager. He hired you. He put you into this situation. He planned all project activities, so he probably knows what he is doing. This means that the project documentation should be detailed enough for a Java developer to create that Python module. However, it is not detailed enough. So report this issue and wait for its resolution. Explain in your bug report that you honestly started to work with the module and realized that its documentation is not complete enough for a Java developer to understand. Ask the project manager to fix this. If the project decides to invest its money into the documentation, you have the chance to read it and learn. Thus, the project's money spent on your education will also contribute to the project. It's a win-win. Here is another example: Say you have to fix a Java module and you're a Java developer. You understand Java, but you don't understand how this algorithm works. You could blame yourself for not reading Knuth in school and ask the project to train you on it. A good and strong project manager should tell you that it's not a school and get rid of you. Again, a bad scenario for both of you. Instead, blame the project. The code is not self-descriptive and is difficult to understand. The algorithm implementation is not obvious and is poorly documented. Ask for better documentation. If the project invests its money into the documentation, you will learn the algorithm from it. The source code will be improved, and you will have improved your skills. Win-win. One more example: Say you are tasked to implement a WebSockets back-end in an existing web app. You know how WebSockets work but can't understand how to connect this new back-end to the existing persistence layer. You are rather new to the project and don't understand what would be the right design. You could ask for the project to provide training to explain how the code works and how it can be extended with features like this one. A project manager should tell you that you're not in school and are supposed to understand the software if the project is paying you a software developer salary. And he will be right. But it's a bad scenario for both of you. Instead, blame the project for incomplete design documentation. Good software should properly document its architecture and design. Ask for the project to provide such documentation. If it invests its time and money into better documentation, you will learn from it and find all the necessary answers. Another win-win. There are a few other examples in my How to Cut Corners and Stay Cool post. In conclusion, I would recommend you remember that software projects are, first and foremost, business activities where we, the developers, are resources. In order to obtain something for ourselves in terms of education and training, we should align our goals with project objectives. Instead of asking for help and information, we should blame the project for its lack of documentation. By fixing this flaw, the project will improve its artifacts and, at the same time, provide valuable knowledge to us, its participants. "},{"title":"Utility Classes Have Nothing to Do With Functional Programming","url":"/2015/02/20/utility-classes-vs-functional-programming.html","tags":["oop"],"date":"2015-02-20 00:00:00 +0000","categories":["jcg"],"body":"I was recently accused of being against functional programming because I call utility classes an anti-pattern . That's absolutely wrong! Well, I do consider them a terrible anti-pattern, but they have nothing to do with functional programming. I believe there are two basic reasons why. First, functional programming is declarative, while utility class methods are imperative. Second, functional programming is based on lambda calculus, where a function can be assigned to a variable. Utility class methods are not functions in this sense. I'll decode these statements in a minute. In Java, there are basically two valid alternatives to these ugly utility classes aggressively promoted by Guava , Apache Commons , and others. The first one is the use of traditional classes, and the second one is Java 8 lambda . Now let's see why utility classes are not even close to functional programming and where this misconception is coming from. Color Me Kubrick (2005) by Brian W. Cook Here is a typical example of a utility class Math from Java 1.0: public class Math { public static double abs ( double a ); // a few dozens of other methods of the same style } Here is how you would use it when you want to calculate an absolute value of a floating point number: double x = Math . abs ( 3.1415926d ); What's wrong with it? We need a function, and we get it from class Math . The class has many useful functions inside it that can be used for many typical mathematical operations, like calculating maximum, minimum, sine, cosine, etc. It is a very popular concept; just look at any commercial or open source product. These utility classes are used everywhere since Java was invented (this Math class was introduced in Java's first version). Well, technically there is nothing wrong. The code will work. But it is not object-oriented programming. Instead, it is imperative and procedural. Do we care? Well, it's up to you to decide. Let's see what the difference is. There are basically two different approaches: declarative and imperative. Imperative programming is focused on describing how a program operates in terms of statements that change a program state. We just saw an example of imperative programming above. Here is another (this is pure imperative/procedural programming that has nothing to do with OOP): public class MyMath { public double f ( double a , double b ) { double max = Math . max ( a , b ); double x = Math . abs ( max ); return x ; } } Declarative programming focuses on what the program should accomplish without prescribing how to do it in terms of sequences of actions to be taken. This is how the same code would look in Lisp, a functional programming language: ( defun f ( a b ) ( abs ( max a b ))) What's the catch? Just a difference in syntax? Not really. There are many definitions of the difference between imperative and declarative styles, but I will try to give my own. There are basically three roles interacting in the scenario with this f function/method: a buyer , a packager of the result, and a consumer of the result. Let's say I call this function like this: public void foo () { double x = this . calc ( 5 , - 7 ); System . out . println ( \"max+abs equals to \" + x ); } private double calc ( double a , double b ) { double x = Math . f ( a , b ); return x ; } Here, method calc() is a buyer, method Math.f() is a packager of the result, and method foo() is a consumer. No matter which programming style is used, there are always these three guys participating in the process: the buyer, the packager, and the consumer. Imagine you're a buyer and want to purchase a gift for your (girl|boy)friend. The first option is to visit a shop, pay $50, let them package that perfume for you, and then deliver it to the friend (and get a kiss in return). This is an imperative style. The second option is to visit a shop, pay $50, and get a gift card. You then present this card to the friend (and get a kiss in return). When he or she decides to convert it to perfume, he or she will visit the shop and get it. This is a declarative style. See the difference? In the first case, which is imperative, you force the packager (a beauty shop) to find that perfume in stock, package it, and present it to you as a ready-to-be-used product. In the second scenario, which is declarative, you're just getting a promise from the shop that eventually, when it's necessary, the staff will find the perfume in stock, package it, and provide it to those who need it. If your friend never visits the shop with that gift card, the perfume will remain in stock. Moreover, your friend can use that gift card as a product itself, never visiting the shop. He or she may instead present it to somebody else as a gift or just exchange it for another card or product. The gift card itself becomes a product! So the difference is what the consumer is getting — either a product ready to be used (imperative) or a voucher for the product, which can later be converted into a real product (declarative). Utility classes, like Math from JDK or StringUtils from Apache Commons, return products ready to be used immediately, while functions in Lisp and other functional languages return \"vouchers\". For example, if you call the max function in Lisp, the actual maximum between two numbers will only be calculated when you actually start using it: ( let ( x ( max 1 5 )) ( print \"X equals to \" x )) Until this print actually starts to output characters to the screen, the function max won't be called. This x is a \"voucher\" returned to you when you attempted to \"buy\" a maximum between 1 and 5 . Note, however, that nesting Java static functions one into another doesn't make them declarative. The code is still imperative, because its execution delivers the result here and now: public class MyMath { public double f ( double a , double b ) { return Math . abs ( Math . max ( a , b )); } } \"Okay,\" you may say, \"I got it, but why is declarative style better than imperative? What's the big deal?\" I'm getting to it. Let me first show the difference between functions in functional programming and static methods in OOP. As mentioned above, this is the second big difference between utility classes and functional programming. In any functional programming language, you can do this: ( defun foo ( x ) ( x 5 )) Then, later, you can call that x : ( defun bar ( x ) ( + x 1 )) // defining function bar ( print ( foo bar )) // passing bar as an argument to foo Static methods in Java are not functions in terms of functional programming. You can't do anything like this with a static method. You can't pass a static method as an argument to another method. Basically, static methods are procedures or, simply put, Java statements grouped under a unique name. The only way to access them is to call a procedure and pass all necessary arguments to it. The procedure will calculate something and return a result that is immediately ready for usage. And now we're getting to the final question I can hear you asking: \"Okay, utility classes are not functional programming, but they look like functional programming, they work very fast, and they are very easy to use. Why not use them? Why aim for perfection when 20 years of Java history proves that utility classes are the main instrument of each Java developer?\" Besides OOP fundamentalism, which I'm very often accused of, there are a few very practical reasons (BTW, I am an OOP fundamentalist): Testability . Calls to static methods in utility classes are hard-coded dependencies that can never be broken for testing purposes. If your class is calling FileUtils.readFile() , I will never be able to test it without using a real file on disk. Efficiency . Utility classes, due to their imperative nature, are much less efficient than their declarative alternatives . They simply do all calculations right here and now, taking processor resources even when it's not yet necessary. Instead of returning a promise to break down a string into chunks, StringUtils.split() breaks it down right now. And it breaks it down into all possible chunks, even if only the first one is required by the \"buyer\". Readability . Utility classes tend to be huge (try to read the source code of StringUtils or FileUtils from Apache Commons). The entire idea of separation of concerns, which makes OOP so beautiful, is absent in utility classes. They just put all possible procedures into one huge .java file, which becomes absolutely unmaintainable when it surpasses a dozen static methods. To conclude, let me reiterate: Utility classes have nothing to do with functional programming. They are simply bags of static methods, which are imperative procedures. Try to stay as far as possible away from them and use solid, cohesive objects no matter how many of them you have to declare and how small they are. "},{"title":"A Haircut","url":"/2015/02/23/haircut.html","tags":["oop","mgmt"],"date":"2015-02-23 00:00:00 +0000","categories":["jcg"],"body":"I received a haircut today, and the niceness of my hairdresser led him to fill the appointment with courteous questions about how I wanted my hair cut, what size of clipper he should use, how long the sides should be, and how much should be removed from the front. He also offered me many types of shampoo and a cup of tea. All this reminded me of the work we do as programmers, and I decided to write a short post about it. I've already mentioned before that trying to make a customer happy is a false objective. This hairdresser was a perfect illustrative example of this very mistake. By the way, in the end, I wasn't happy, and he got no tip. How could this happen if he was so friendly and nice? The Man Who Wasn't There (2001) by Coen Brothers I'm not a hairdresser, and I have very little understanding of how to deal with hair. I came to him because I assumed he knew more about this business than I did. I chose him through the assistance of Yelp. I wanted him to tell me how long the hair on the sides should be and how much should be removed on the top. I expected him to give me his professional judgement and stand by it. Instead of asking me how much I wanted removed on the sides, he should have told me there should be less on the sides. This is what a true professional would do. A true professional would give me his vision of the haircut that best suits me and would try to convince me that it was the best choice. A true professional would not ask me but would tell me instead, because he would understand that my goal was not to boss him around and make him do my hair the way I wanted it. My goal was to get the best out of his professional skill. Unfortunately, the guy was either weak or immature. He didn't argue with me and didn't try to convince me. He tried to please me. In the end, he lost. Exactly the same thing happens when we ask our customers about the technologies they want us to use. I hear this question very often: What language do you want us to use (meaning Java or Ruby or something else)? Or what database should we use? Or how do you want us to design this? Don't do that. Don't lose like that hairdresser. Don't ask your clients what they want. Instead, learn their business requirements and then suggest the solution you think is the best for them. Then, insist and argue if they don't agree. Convince them. Even if they fire you in the end for your stubbornness, it's better than being that hairdresser who is doomed to please every single client without getting anywhere further. Remember, the client is not the king; his hairs are. "},{"title":"Composable Decorators vs. Imperative Utility Methods","url":"/2015/02/26/composable-decorators.html","tags":["oop"],"date":"2015-02-26 00:00:00 +0000","categories":["jcg"],"body":"The decorator pattern is my favorite among all other patterns I'm aware of. It is a very simple and yet very powerful mechanism to make your code highly cohesive and loosely coupled . However, I believe decorators are not used often enough. They should be everywhere, but they are not. The biggest advantage we get from decorators is that they make our code composable . That's why the title of this post is composable decorators. Unfortunately, instead of decorators, we often use imperative utility methods, which make our code procedural rather than object-oriented. Матрёшка First, a practical example. Here is an interface for an object that is supposed to read a text somewhere and return it: interface Text { String read (); } Here is an implementation that reads the text from a file: final class TextInFile implements Text { private final File file ; public TextInFile ( final File src ) { this . file = src ; } @Override public String read () { return new String ( Files . readAllBytes (), \"UTF-8\" ); } } And now the decorator, which is another implementation of Text that removes all unprintable characters from the text: final class PrintableText implements Text { private final Text origin ; public PrintableText ( final Text text ) { this . origin = text ; } @Override public String read () { return this . origin . read () . replaceAll ( \"[^\\p{Print}]\" , \"\" ); } } Here is how I'm using it: final Text text = new PrintableText ( new TextInFile ( new File ( \"/tmp/a.txt\" )) ); String content = text . read (); As you can see, the PrintableText doesn't read the text from the file. It doesn't really care where the text is coming from. It delegates text reading to the encapsulated instance of Text . How this encapsulated object will deal with the text and where it will get it doesn't concern PrintableText . Let's continue and try to create an implemetation of Text that will capitalize all letters in the text: final class AllCapsText implements Text { private final Text origin ; public AllCapsText ( final Text text ) { this . origin = text ; } @Override public String read () { return this . origin . read (). toUpperCase ( Locale . ENGLISH ); } } How about a Text that trims the input: final class TrimmedText implements Text { private final Text origin ; public TrimmedText ( final Text text ) { this . origin = text ; } @Override public String read () { return this . origin . read (). trim (); } } I can go on and on with these decorators. I can create many of them, suitable for their own individual use cases. But let's see how they all can play together. Let's say I want to read the text from the file, capitalize it, trim it, and remove all unprintable characters. And I want to be declarative . Here is what I do: final Text text = new AllCapsText ( new TrimmedText ( new PrintableText ( new TextInFile ( new File ( \"/tmp/a.txt\" )) ) ) ); String content = text . read (); First, I create an instance of Text , composing multiple decorators into a single object. I declaratively define the behavior of text without actually executing anything. Until method read() is called, the file is not touched and the processing of the text is not started. The object text is just a composition of decorators, not an executable procedure . Check out this article about declarative and imperative styles of programming: Utility Classes Have Nothing to Do With Functional Programming . This design is much more flexible and reusable than a more traditional one, where the Text object is smart enough to perform all said operations. For example, class String from Java is a good example of a bad design. It has more than 20 utility methods that should have been provided as decorators instead: trim() , toUpperCase() , substring() , split() , and many others, for example. When I want to trim my string, uppercase it, and then split it into pieces, here is what my code will look like: final String txt = \"hello, world!\" ; final String [] parts = txt . trim (). toUpperCase (). split ( \" \" ); This is imperative and procedural programming. Composable decorators, on the other hand, would make this code object-oriented and declarative. Something like this would be great to have in Java instead (pseudo-code): final String [] parts = new String . Split ( new String . UpperCased ( new String . Trimmed ( \"hello, world!\" ) ) ); To conclude, I recommend you think twice every time you add a new utility method to the interface/class. Try to avoid utility methods as much as possible, and use decorators instead. An ideal interface should contain only methods that you absolutely cannot remove. Everything else should be done through composable decorators. "},{"title":"Team Morale: Myths and Reality","url":"/2015/03/02/team-morale-myths-and-reality.html","tags":["mgmt","agile"],"date":"2015-03-02 00:00:00 +0000","categories":[],"body":"There are plenty of books, articles, and blog posts about team morale . They will all suggest you do things like regular celebrations, team events, free lunches, pet-friendly offices, coffee machines, birthday presents, etc. All of these are instruments of concealed enslaving . These traditional techniques turn employees into speechless monkeys, programming under the influence of Prozac. Their existence and popularity is our big misfortune. Let me present my own vision of how team morale can be boosted on a software team — a team that has a strong leader and a good project manager. Apocalypto (2006) by Mel Gibson Fire Fast . The first and most important quality of a good leader is his or her ability to separate bad apples from good ones as soon as possible. Nothing will earn you more disrespect from your team than tolerance of underperforming team members. Your job as a leader is to help the best players play better, and they can't play better if they see that management doesn't understand the difference between excellence and mediocrity. It's a severe demotivating factor. Be Honest About Problems and Risks . Your team is following you and expecting you to be a smart leader. While they are writing Java, you're talking to investors and customers. They want to be sure you know what you're doing. The best way to show them you have no idea where the team is going is to tell them that the future is bright and cloudless. Everybody understands that's either a lie and you are trying to hide risks or you're stupid enough to not see them. In either case, the best people would attempt to quit before it's too late. Thus, to keep morale up, regularly inform your people about problems you're facing and risks you're trying to prevent. They will appreciate it and respect you. A strong, professional manager deals with risks instead of ignoring them. Failures Are Yours; Success Is Theirs . Always remember that when someone on your team makes a mistake, it is first of all your personal mistake. You hired that person, you trained him or her, you delegated the responsibility, and you controlled and monitored the job. Then he made a mistake, and the project lost money, disappointed a customer, or damaged the firm's reputation. Of course you need to take necessary disciplinary actions and maybe fire the troublemaker. But first of all, you have to admit in front of everyone that it was your personal mistake. You didn't control enough, you didn't plan well, or you didn't take preventive actions. This is what the team expects from you. Also, your people expect you to explain to them how you're going to learn from this mistake in order to prevent a similar one from happening in the future. A strong leader isn't afraid to look stupid in front of the team. A weak leader does look stupid when he or she tries to hide mistakes that have been made. Responsibility Is Always Personal . The most demotivating word used in task descriptions is \"together\". Don't use it. Each task has to be personally and individually assigned (no matter what the Agile Manifesto says). Everybody is responsible for his or her own success or failure. How their results join together and lead to a mutual success or failure — that's your business. Whether you succeed or fail, we all will see. Once you say we all have to succeed together, the team understands that you're trying to shift responsibility from your own shoulders to theirs. It's a sign of weakness, and you lose respect. Make tasks and goals strictly personal, and be prepared to be responsible for the group's success. You, as a manager, break down an entire project into parts and delegate them to your people. If you do this job properly, we all will succeed. But don't try to blame us if the parts fall apart. Don't Mention Steve Jobs . Try to avoid global slogans and world domination speeches in the office and in front of the team. They demotivate. If we're doing so good, why are our salaries not reflecting this success yet? If your vision is so global, why is it not yet implemented in reality? Don't promise to become the next Steve Jobs. Instead, become the next good manager of a highly paid team that is solving interesting problems for real people. Your practical achievements, no matter how small and down-to-earth they are, will give you much more respect than many-hour-long speeches about our fantastic future. Don't Say a Word About Agile . Even though Agile is a great attitude-changing and mind-shifting concept, it is absolutely inapplicable in practice, mostly because it is too abstract. When you're proclaiming in the office that we should value \"working software over comprehensive documentation\", it sounds like you don't know what you're doing. The team doesn't need such abstract slogans from you. It needs specific instructions and rules in order to follow them and produce results, money, and satisfaction. Agile is a set of abstract principles that you should understand and digest. But then, after you chew them properly, convert them to specific and very unambitious rules of work. Don't talk about Agile; be agile. Don't Close the Door . Responsibility is personal, money is personal, and results are personal. But their discussions should be open to everybody. Don't close the door to that meeting room when you're talking about problems or appraising someone's results. You want your team to work together? Give everybody an assurance that none of them will be terminated behind a closed door. These pompous speeches about \"us working together\" usually turn into mush once the team sees that someone gets fired after a private conversation with a manager. Are we together, or is it you against us? To keep team morale up, you, as a leader, have to establish ground rules of work that will define who gets what when we succeed and who goes home first when we fail. These rules should be open to everybody. These rules should rule the team, not your personal decisions made behind a closed door. Celebrate Achievements Instead of Birthdays . Team-building events are a great tool to boost team morale, but only when they are built around personal or team achievements instead of calendar events. A project team is not a group of friends or family members, even though some teams may feel like that. No matter how it feels, a team is here for one reason — to create the product and make money for its sponsor. This is the direction we're going. Our goal is not to build a community and live together til the end of our days. Our goal is to achieve the business success of the product we're developing, or in other words, complete the project. When the only events we're celebrating are our birthdays, that's a sign to us that our leaders are trying to lie to us. They are pretending that we're here to make a community of friends while in reality they are using us to build their business. It's unhealthy and ruins team morale. Instead, celebrate achievements on your real path — to the success of the product under development. This will show everybody that you, as a leader, are honest with your people and ready to show them that their true role on the team is to develop a product and earn money for its investors. Honesty is the best team morale booster. Don't Rule; Make Rules and Plans . Nothing demotivates more than an upredictable manager. For the team, you are an abstraction of the entire world around the team. They see the reality through the prism of your personality. What you tell them about the reality is what they perceive. If you are unpredictable, the reality is unpredictable and scary for them. To avoid that, stop making decisions that are based on your personal and momentary judgement. Instead, make decisions that are based on the rules you've defined upfront and plans you've drawn beforehand. First, create a plan for team growth and announce it to everybody. The plan should include risks and their mitigations. The plan should say who will be fired first when or if the project goes down. The plan should give a predictable and measurable picture of the reality around your office. It should be a map of terrain you're going to cross with your team. When it's time to make a decision, everybody will understand why it's made and will respect you as a leader who predicted the situation and managed it professionally. Put Money on the Table . Discuss money openly and freely, right in the office, right in front of everybody. This advice is for true professionals. If you can't do what is said above, don't try this one. But if you consider yourself a real pro in management and leadership, you should put money on the table and let everybody know who is getting what, when, why, and why not. Everybody should know everybody's salaries, bonuses, benefits, and the rationale behind them. Each programmer should know what he or she should do in order to get a $5,000 raise to their annual salary. Also, he or she should know why a colleague is called \"senior developer\" while his or her title is still \"junior\". This information should be public and printed on the wall right behind your chair. Why don't most managers do this? Because they don't have any rationale behind their monetary decisions. Instead of managing the money, they let money manage them. That's a subject for another post :) "},{"title":"Don't Create Objects That End With -ER","url":"/2015/03/09/objects-end-with-er.html","tags":["oop"],"date":"2015-03-09 00:00:00 +0000","categories":["best"],"body":"Manager. Controller. Helper. Handler. Writer. Reader. Converter. Validator. Router. Dispatcher. Observer. Listener. Sorter. Encoder. Decoder. This is the class names hall of shame . Have you seen them in your code? In open source libraries you're using? In pattern books? They are all wrong. What do they have in common? They all end in \"-er\". And what's wrong with that? They are not classes, and the objects they instantiate are not objects. Instead, they are collections of procedures pretending to be classes. Fight Club (1999) by David Fincher Peter Coad used to say: Challenge any class name that ends in \"-er\". There are a few good articles about this subject, including Your Coding Conventions Are Hurting You by Carlo Pescio, One of the Best Bits of Programming Advice I Ever Got by Travis Griggs, and Naming Objects – Don’t Use ER in Your Object Names by Ben Hall. The main argument against this \"-er\" suffix is that \"when you need a manager, it's often a sign that the managed are just plain old data structures and that the manager is the smart procedure doing the real work\". I totally agree but would like to add a few words to this. I mentioned already in Seven Virtues of a Good Object that a good object name is not a job title, but I didn't explain why I think so. Besides that, in Utility Classes Have Nothing to Do With Functional Programming , I tried to explain the difference between declarative and imperative programming paradigms. Now it's time to put these two pieces together. Let's say I'm an object and you're my client. You give me a bucket of apples and ask me to sort them by size. If I'm living in the world of imperative programming, you will get them sorted immediately, and we will never interact again. I will do my job just as requested, without even thinking why you need them sorted. I would be a sorter who doesn't really care about your real intention: List < Apple > sorted = new Sorter (). sort ( apples ); Apple biggest = sorted . get ( 0 ); As you see here, the real intention is to find the biggest apple in the bucket. This is not what you would expect from a good business partner who can help you work with a bucket of apples. Instead, if I lived in the world of declarative programming, I would tell you: \" Consider them sorted; what do you want to do next? \". You, in turn, would tell me that you need the biggest apple now. And I would say, \" No problem; here it is \". In order to return the biggest one, I would not sort them all. I would just go through them all one by one and select the biggest. This operation is much faster than sorting first and then selecting the first in the list. In other words, I would silently not follow your instructions but would try to do my business my way. I would be a much smarter partner of yours than that imperative sorter. And I would become a real object that behaves like a sorted list of apples instead of a procedure that sorts: List < Apple > sorted = new Sorted ( apples ); Apple biggest = sorted . get ( 0 ); See the difference? Pay special attention to the difference between the sorter and sorted names. Let's get back to class names. When you add the \"-er\" suffix to your class name, you're immediately turning it into a dumb imperative executor of your will. You do not allow it to think and improvise. You expect it to do exactly what you want — sort, manage, control, print, write, combine, concatenate, etc. An object is a living organism that doesn't want to be told what to do. It wants to be an equal partner with other objects, exposing behavior according to its contract(s), a.k.a. interfaces in Java and C# or protocols in Swift. Philosophically speaking, the \"-er\" suffix is a sign of disrespect toward the poor object. "},{"title":"Worst Technical Specifications Have No Glossaries","url":"/2015/03/16/technical-glossaries.html","tags":["mgmt","agile"],"date":"2015-03-16 00:00:00 +0000","categories":[],"body":"I read a few technical specifications every week from our current and potential clients, and there's one thing I can't take anymore; I have to write about it: 99 percent of the documents I'm reading don't have glossaries, and because of that, they are very difficult to read and digest. Even when they do have glossaries, their definitions of terms are very vague and ambiguous. Why is this happening? Don't we understand the importance of a common vocabulary for any software project? I'm not sure what the causes are, but this is what a software architect should do when he or she starts a project — create a glossary. Pulp Fiction (1994) by Quentin Tarantino I'm trying to write something unique about this subject, but everything I can say is so obvious that I doubt anyone would be interested in reading it. Anyway, I will try. A glossary (a.k.a. vocabulary) is a list of terms used by the project that is usually included at the beginning of the technical specification document. Ideally, every technical term used in the document should be briefly explained in the glossary. The existence of a glossary helps everyone who works with the document quickly understand each other and avoid misconceptions. On top of this, a detailed and accurate glossary saves a reader a lot of time. So why are glossaries not written? I see a few possible causes (usually, they are combined): We're Smarter Than This . Some people think glossaries are for newbies. After all, why would I explain what a PDU is? Any serious network engineer should understand that it stands for \"protocol data unit\". If you don't understand it, do your homework and then come back to work with us. Our team only works with well-educated engineers. You're supposed to understand what PDU, ADC, TxR, IPv6, DPI, FIFO, and USSR (joking!) stand for. Otherwise, you're not talented enough to be with us. Needless to say, this attitude can only come from those who have no idea what they are doing. A good engineer always remembers that if the receiver doesn't understand a message, it's the sender's fault. We Don't Need These Formalities . Seriously, why would we spend time on writing a glossary if everybody understands all our terms without it? We've been working as a team for a few years, so we all know what DPI and FIFO are, and we know what \"record\" and \"timing data\" are. Why bother with the glossary, which will provide no additional business value for us? I've seen many technical meetings of very mature and \"well-organized\" teams burn hours of time on pointless discussions simply because of different understandings the same term. A glossary is not a formality; it's a key instrument of a software architect and all other team members. We Prefer Working Software Over Comprehensive Documentation . This is what the Agile Manifesto says, and it's true. But the key word here is \"comprehensive\". We don't need comprehensive documentation, but we need a glossary. It's a key element in any project, and it simply can't be replaced by working software. No working software can help us understand what \"header\" and \"data signal\" are unless there is a simple and clear statement about it. We Don't Have Time . We're developing too fast and brainstorming every day, so the concept is frequently changing. We simply don't have time to document our thoughts. We all understand each other, and that is the beauty of being agile. No, that is not a beauty. Instead, it is a lack of discipline and elementary management skill. A lack of a glossary is a personal fault of the software architect, and there are no excuses for it. All Our Terms Are Well-Known to Everyone . Seriously, do we need to document what TCP/IP and FIFO are? That's what they teach us in school. Everyone understands that, don't they? Yes, some of the terms are well-known. But what is the problem of adding them to the glossary with a few words and a link to a Wikipedia article? This will only take a few minutes of an architect's time, but it will make life easier for everybody in the project, both now and in a few years from now. To conclude, there is no excuse for the absence of a glossary in any software project. And it is the personal responsibility of a software architect to keep this document (or a chapter) up to date. Hope I wasn't too obvious above :) "},{"title":"Java Web App Architecture In Takes Framework","url":"/2015/03/22/takes-java-web-framework.html","tags":["java"],"date":"2015-03-22 00:00:00 +0000","categories":["jcg"],"body":"I used to utilize Servlets, JSP, JAX-RS, Spring Framework, Play Framework, JSF with Facelets, and a bit of Spark Framework. All of these solutions, in my humble opinion, are very far from being object-oriented and elegant. They all are full of static methods, untestable data structures, and dirty hacks. So about a month ago, I decided to create my own Java web framework. I put a few basic principles into its foundation: 1) No NULLs, 2) no public static methods, 3) no mutable classes, and 4) no class casting, reflection, and instanceof operators. These four basic principles should guarantee clean code and transparent architecture. That's how the Takes framework was born. Let's see what was created and how it works. Making of The Godfather (1972) by Francis Ford Coppola Java Web Architecture in a Nutshell This is how I understand a web application architecture and its components, in simple terms. First, to create a web server, we should create a new network socket , that accepts connections on a certain TCP port . Usually it is 80, but I'm going to use 8080 for testing purposes. This is done in Java with the ServerSocket class: import java.net.ServerSocket ; public class Foo { public static void main ( final String ... args ) throws Exception { final ServerSocket server = new ServerSocket ( 8080 ); while ( true ); } } That's enough to start a web server. Now, the socket is ready and listening on port 8080. When someone opens http://localhost:8080 in their browser, the connection will be established and the browser will spin its waiting wheel forever. Compile this snippet and try. We just built a simple web server without the use of any frameworks. We're not doing anything with incoming connections yet, but we're not rejecting them either. All of them are being lined up inside that server object. It's being done in a background thread; that's why we need to put that while(true) in afterward. Without this endless pause, the app will finish its execution immediately and the server socket will shut down. The next step is to accept the incoming connections. In Java, that's done through a blocking call to the accept() method: final Socket socket = server . accept (); The method is blocking its thread and waiting until a new connection arrives. As soon as that happens, it returns an instance of Socket . In order to accept the next connection, we should call accept() again. So basically, our web server should work like this: public class Foo { public static void main ( final String ... args ) throws Exception { final ServerSocket server = new ServerSocket ( 8080 ); while ( true ) { final Socket socket = server . accept (); // 1. Read HTTP request from the socket // 2. Prepare an HTTP response // 3. Send HTTP response to the socket // 4. Close the socket } } } It's an endless cycle that accepts a new connection, understands it, creates a response, returns the response, and accepts a new connection again. HTTP protocol is stateless, which means the server should not remember what happened in any previous connection. All it cares about is the incoming HTTP request in this particular connection. The HTTP request is coming from the input stream of the socket and looks like a multi-line block of text. This is what you would see if you read an input stream of the socket: final BufferedReader reader = new BufferedReader ( new InputStreamReader ( socket . getInputStream ()) ); while ( true ) { final String line = reader . readLine (); if ( line . isEmpty ()) { break ; } System . out . println ( line ); } You will see something like this: GET / HTTP/1.1 Host: localhost:8080 Connection: keep-alive Cache-Control: max-age=0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.89 Safari/537.36 Accept-Encoding: gzip, deflate, sdch Accept-Language: en-US,en;q=0.8,ru;q=0.6,uk;q=0.4 The client (the Google Chrome browser, for example) passes this text into the connection established. It connects to port 8080 at localhost , and as soon as the connection is ready, it immediately sends this text into it, then waits for a response. Our job is to create an HTTP response using the information we get in the request. If our server is very primitive, we can basically ignore all the information in the request and just return \"Hello, world!\" to all requests (I'm using IOUtils for simplicity): import java.net.Socket ; import java.net.ServerSocket ; import org.apache.commons.io.IOUtils ; public class Foo { public static void main ( final String ... args ) throws Exception { final ServerSocket server = new ServerSocket ( 8080 ); while ( true ) { try ( final Socket socket = server . accept ()) { IOUtils . copy ( IOUtils . toInputStream ( \"HTTP/1.1 200 OK\\r\\n\\r\\nHello, world!\" ), socket . getOutputStream () ); } } } } That's it. The server is ready. Try to compile and run it. Point your browser to http://localhost:8080 , and you will see Hello, world! : $ javac -cp commons-io.jar Foo.java $ java -cp commons-io.jar:. Foo & $ curl http://localhost:8080 -v * Rebuilt URL to: http://localhost:8080/ * Connected to localhost ( ::1 ) port 8080 ( #0) > GET / HTTP/1.1 > User-Agent: curl/7.37.1 > Host: localhost:8080 > Accept: */* > < HTTP/1.1 200 OK * no chunk, no close, no size. Assume close to signal end < * Closing connection 0 Hello, world! That's all you need to build a web server. Now let's discuss how to make it object-oriented and composable. Let's try to see how the Takes framework was built. Routing/Dispatching Routing/dispatching is combined with response printing in Takes. All you need to do to create a working web application is to create a single class that implements Take interface: import org.takes.Request ; import org.takes.Take ; public final class TkFoo implements Take { @Override public Response route ( final Request request ) { return new RsText ( \"Hello, world!\" ); } } And now it's time to start a server: import org.takes.http.Exit ; import org.takes.http.FtBasic ; public class Foo { public static void main ( final String ... args ) throws Exception { new FtBasic ( new TkFoo (), 8080 ). start ( Exit . NEVER ); } } This FtBasic class does the exact same socket manipulations explained above. It starts a server socket on port 8080 and dispatches all incoming connections through an instance of TkFoo that we are giving to its constructor. It does this dispatching in an endless cycle, checking every second whether it's time to stop with an instance of Exit . Obviously, Exit.NEVER always responds with, \"Don't stop, please\". HTTP Request Now let's see what's inside the HTTP request arriving at TsFoo and what we can get out of it. This is how the Request interface is defined in Takes : public interface Request { Iterable < String > head () throws IOException ; InputStream body () throws IOException ; } The request is divided into two parts: the head and the body. The head contains all lines that go before the empty line that starts a body, according to HTTP specification in RFC 2616 . There are many useful decorators for Request in the framework. For example, RqMethod will help you get the method name from the first line of the header: final String method = new RqMethod ( request ). method (); RqHref will help extract the query part and parse it. For example, this is the request: GET / user ? id = 123 HTTP / 1.1 Host: www . example . com This code will extract that 123 : final int id = Integer . parseInt ( new RqHref ( request ). href (). param ( \"id\" ). get ( 0 ) ); RqPrint can get the entire request or its body printed as a String : final String body = new RqPrint ( request ). printBody (); The idea here is to keep the Request interface simple and provide this request parsing functionality to its decorators. This approach helps the framework keep classes small and cohesive. Each decorator is very small and solid, doing exactly one thing. All of these decorators are in the org.takes.rq package. As you already probably understand, the Rq prefix stands for Request . First Real Web App Let's create our first real web application, which will do something useful. I would recommend starting with an Entry class, which is required by Java to start an app from the command line: import org.takes.http.Exit ; import org.takes.http.FtCLI ; public final class Entry { public static void main ( final String ... args ) throws Exception { new FtCLI ( new TkApp (), args ). start ( Exit . NEVER ); } } This class contains just a single main() static method that will be called by JVM when the app starts from the command line. As you see, it instantiates FtCLI , giving it an instance of class TkApp and command line arguments. We'll create the TkApp class in a second. FtCLI (translates to \"front-end with command line interface\") makes an instance of the same FtBasic , wrapping it into a few useful decorators and configuring it according to command line arguments. For example, --port=8080 will be converted into a 8080 port number and passed as a second argument of the FtBasic constructor. The web application itself is called TkApp and extends TsWrap : import org.takes.Take ; import org.takes.facets.fork.FkRegex ; import org.takes.facets.fork.TkFork ; import org.takes.tk.TkWrap ; import org.takes.tk.TkClasspath ; final class TkApp extends TkWrap { TkApp () { super ( TkApp . make ()); } private static Take make () { return new TkFork ( new FkRegex ( \"/robots.txt\" , \"\" ), new FkRegex ( \"/css/.*\" , new TkClasspath ()), new FkRegex ( \"/\" , new TkIndex ()) ); } } We'll discuss this TkFork class in a minute. If you're using Maven, this is the pom.xml you should start with: <? xml version = \"1.0\" ?> < project xmlns = \"http://maven.apache.org/POM/4.0.0\" xmlns: xsi = \"http://www.w3.org/2001/XMLSchema-instance\" xsi: schemaLocation = \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > < modelVersion > 4.0 . 0 </ modelVersion > < groupId > foo </ groupId > < artifactId > foo </ artifactId > < version > 1.0 - SNAPSHOT </ version > < dependencies > < dependency > < groupId > org . takes </ groupId > < artifactId > takes </ artifactId > < version > 0.9 </ version > <!-- check the latest in Maven Central --> </ dependency > </ dependencies > < build > < finalName > foo </ finalName > < plugins > < plugin > < artifactId > maven - dependency - plugin </ artifactId > < executions > < execution > < goals > < goal > copy - dependencies </ goal > </ goals > < configuration > < outputDirectory > $ { project . build . directory }/ deps </ outputDirectory > </ configuration > </ execution > </ executions > </ plugin > </ plugins > </ build > </ project > Running mvn clean package should build a foo.jar file in target directory and a collection of all JAR dependencies in target/deps . Now you can run the app from the command line: $ mvn clean package $ java -Dfile.encoding = UTF-8 -cp ./target/foo.jar:./target/deps/* foo.Entry --port = 8080 The application is ready, and you can deploy it to, say, Heroku. Just create a Procfile file in the root of the repository and push the repo to Heroku. This is what Procfile should look like: web: java -Dfile.encoding=UTF-8 -cp target/foo.jar:target/deps/* foo.Entry --port=${PORT} TkFork This TkFork class seems to be one of the core elements of the framework. It helps route an incoming HTTP request to the right take . Its logic is very simple, and there are just a few lines of code inside it. It encapsulates a collection of \"forks\", which are instances of the Fork interface: public interface Fork { Iterator < Response > route ( Request req ) throws IOException ; } Its only route() method either returns an empty iterator or an iterator with a single Response . TkFork goes through all forks, calling their route() methods until one of them returns a response. Once that happens, TkFork returns this response to the caller, which is FtBasic . Let's create a simple fork ourselves now. For example, we want to show the status of the application when the /status URL is requested. Here is the code: final class TkApp extends TkWrap { private static Take make () { return new TkFork ( new Fork () { @Override public Iterator < Response > route ( Request req ) { final Collection < Response > responses = new ArrayList <>( 1 ); if ( new RqHref ( req ). href (). path (). equals ( \"/status\" )) { responses . add ( new TkStatus ()); } return responses . iterator (); } } ); } } I believe the logic here is clear. We either return an empty iterator or an iterator with an instance of TkStatus inside. If an empty iterator is returned, TkFork will try to find another fork in the collection that actually gets an instance of Response . By the way, if nothing is found and all forks return empty iterators, TkFork will throw a \"Page not found\" exception. This exact logic is implemented by an out-of-the-box fork called FkRegex , which attempts to match a request URI path with the regular expression provided: final class TkApp extends TkWrap { private static Take make () { return new TkFork ( new FkRegex ( \"/status\" , new TkStatus ()) ); } } We can compose a multi-level structure of TkFork classes; for example: final class TkApp extends TsWrap { private static Take make () { return new TkFork ( new FkRegex ( \"/status\" , new TkFork ( new FkParams ( \"f\" , \"json\" , new TkStatusJSON ()), new FkParams ( \"f\" , \"xml\" , new TkStatusXML ()) ) ) ); } } Again, I believe it's obvious. The instance of FkRegex will ask an encapsulated instance of TkFork to return a response, and it will try to fetch it from one that FkParams encapsulated. If the HTTP query is /status?f=xml , an instance of TkStatusXML will be returned. HTTP Response Now let's discuss the structure of the HTTP response and its object-oriented abstraction, Response . This is how the interface looks: public interface Response { Iterable < String > head () throws IOException ; InputStream body () throws IOException ; } Looks very similar to the Request , doesn't it? Well, it's identical, mostly because the structure of the HTTP request and response is almost identical. The only difference is the first line. There is a collection of useful decorators that help in response building. They are composable , which makes them very convenient. For example, if you want to build a response that contains an HTML page, you compose them like this: final class TkIndex implements Take { @Override public Response act () { return new RsWithStatus ( new RsWithType ( new RsWithBody ( \"<html>Hello, world!</html>\" ), \"text/html\" ), 200 ); } } In this example, the decorator RsWithBody creates a response with a body but with no headers at all. Then, RsWithType adds the header Content-Type: text/html to it. Then, RsWithStatus makes sure the first line of the response contains HTTP/1.1 200 OK . You can create your own decorators that can reuse existing ones. Take a look at how it's done in RsPage from rultor.com. How About Templates? Returning simple \"Hello, world\" pages is not a big problem, as we can see. But what about more complex output like HTML pages, XML documents, JSON data sets, etc? There are a few convenient Response decorators that enable all of that. Let's start with Velocity , a simple templating engine. Well, it's not that simple. It's rather powerful, but I would suggest to use it in simple situations only. Here is how it works: final class TkIndex implements Take { @Override public Response act () { return new RsVelocity ( \"Hello, ${name}\" ) . with ( \"name\" , \"Jeffrey\" ); } } The RsVelocity constructor accepts a single argument that has to be a Velocity template. Then, you call the with() method, injecting data into the Velocity context. When it's time to render the HTTP response, RsVelocity will \"evaluate\" the template against the context configured. Again, I would recommend you use this templating approach only for simple outputs. For more complex HTML documents, I would recommend you use XML/XSLT in combination with Xembly. I explained this idea in a few previous posts: XML+XSLT in a Browser and RESTful API and a Web Site in the Same URL . It is simple and powerful — Java generates XML output and the XSLT processor transforms it into HTML documents. This is how we separate representation from data. The XSL stylesheet is a \"view\" and TkIndex is a \"controller\", in terms of MVC . I'll write a separate article about templating with Xembly and XSL very soon. In the meantime, we'll create decorators for JSF/Facelets and JSP rendering in Takes. If you're interested in helping, please fork the framework and submit your pull requests. What About Persistence? Now, a question that comes up is what to do with persistent entities, like databases, in-memory structures, network connections, etc. My suggestion is to initialize them inside the Entry class and pass them as arguments into the TkApp constructor. Then, the TkApp will pass them into the constructors of custom takes . For example, we have a PostgreSQL database that contains some table data that we need to render. Here is how I would initialize a connection to it in the Entry class (I'm using a BoneCP connection pool): public final class Entry { public static void main ( final String ... args ) throws Exception { new FtCLI ( new TkApp ( Entry . postgres ()), args ). start ( Exit . NEVER ); } private static Source postgres () { final BoneCPDataSource src = new BoneCPDataSource (); src . setDriverClass ( \"org.postgresql.Driver\" ); src . setJdbcUrl ( \"jdbc:postgresql://localhost/db\" ); src . setUser ( \"root\" ); src . setPassword ( \"super-secret-password\" ); return src ; } } Now, the constructor of TkApp must accept a single argument of type java.sql.Source : final class TkApp extends TkWrap { TkApp ( final Source source ) { super ( TkApp . make ( source )); } private static Take make ( final Source source ) { return new TkFork ( new FkRegex ( \"/\" , new TkIndex ( source )) ); } } Class TkIndex also accepts a single argument of class Source . I believe you know what to do with it inside TkIndex in order to fetch the SQL table data and convert it into HTML. The point here is that the dependency must be injected into the application (instance of class TkApp ) at the moment of its instantiation. This is a pure and clean dependency injection mechanism, which is absolutely container-free. Read more about it in \"Dependency Injection Containers Are Code Polluters\" . Unit Testing Since every class is immutable and all dependencies are injected only through constructors, unit testing is extremely easy. Let's say we want to test TkStatus , which is supposed to return an HTML response (I'm using JUnit 4 and Hamcrest ): import org.junit.Test ; import org.hamcrest.MatcherAssert ; import org.hamcrest.Matchers ; public final class TkIndexTest { @Test public void returnsHtmlPage () throws Exception { MatcherAssert . assertThat ( new RsPrint ( new TkStatus (). act ( new RqFake ()) ). printBody (), Matchers . equalsTo ( \"<html>Hello, world!</html>\" ) ); } } Also, we can start the entire application or any individual take in a test HTTP server and test its behavior via a real TCP socket; for example (I'm using jcabi-http to make an HTTP request and check the output): public final class TkIndexTest { @Test public void returnsHtmlPage () throws Exception { new FtRemote ( new TkIndex ()). exec ( new FtRemote . Script () { @Override public void exec ( final URI home ) throws IOException { new JdkRequest ( home ) . fetch () . as ( RestResponse . class ) . assertStatus ( HttpURLConnection . HTTP_OK ) . assertBody ( Matchers . containsString ( \"Hello, world!\" )); } } ); } } FtRemote starts a test web server at a random TCP port and calls the exec() method at the provided instance of FtRemote.Script . The first argument of this method is a URI of the just-started web server homepage. The architecture of Takes framework is very modular and composable. Any individual take can be tested as a standalone component, absolutely independent from the framework and other takes . Why the Name? That's the question I've been hearing rather often. The idea is simple, and it originates from the movie business. When a movie is made, the crew shoots many takes in order to capture the reality and put it on film. Each capture is called a take . In other words, a take is like a snapshot of the reality. The same applies to this framework. Each instance of Take represents a reality at one particular moment in time. This reality is then sent to the user in the form of a Response . "},{"title":"JAXB Is Doing It Wrong; Try Xembly","url":"/2015/03/26/jaxb-vs-xembly.html","tags":["oop","java","xml"],"date":"2015-03-26 00:00:00 +0000","categories":["jcg"],"body":" JAXB is a 10-year-old Java technology that allows us to convert a Java object into an XML document (marshalling) and back (unmarshalling). This technology is based on setters and getters and, in my opinion, violates key principles of object-oriented programming by turning objects into passive data structures . I would recommend you use Xembly instead for marshalling Java objects into XML documents. This is how JAXB marshalling works. Say you have a Book class that needs to be marshalled into an XML document. You have to create getters and annotate them: import javax.xml.bind.annotation.XmlElement ; import javax.xml.bind.annotation.XmlRootElement ; @XmlRootElement public class Book { private final String isbn ; private final String title ; public Book ( final String isbn , final String title ) { this . isbn = isbn ; this . title = title ; } @XmlElement public String getIsbn () { return this . isbn ; } @XmlElement public String getTitle () { return this . title ; } } Then you create a marshaller and ask it to convert an instance of class Book into XML: final Book book = new Book ( \"0132350882\" , \"Clean Code\" ); final JAXBContext context = JAXBContext . newInstance ( Book . class ); final Marshaller marshaller = jaxbContext . createMarshaller (); marshaller . marshal ( book , System . out ); You should be expecting something like this in the output: <?xml version=\"1.0\"?> <book> <isbn> 0132350882 </isbn> <title> Clean Code </title> </book> So what's wrong with it? Pretty much the same thing that's wrong with object-relational mapping, which is explained in ORM Is an Offensive Anti-Pattern . JAXB is treating an object as a bag of data, extracting the data and converting it into XML the way JAXB wants. The object has no control over this process. Therefore an object is not an object anymore but rather a passive bag of data. An ideal approach would be to redesign our class Book this way: public class Book { private final String isbn ; private final String title ; public Book ( final String isbn , final String title ) { this . isbn = isbn ; this . title = title ; } public String toXML () { // create XML document and return } } However, there are a few problems with this approach. First of all, there's massive code duplication. Building an XML document is a rather verbose process in Java. If every class had to re-implement it in its toXML() method, we would have a big problem with duplicate code. The second problem is that we don't know exactly what type of wrapping our XML document should be delivered in. It may be a String or an InputStream or maybe an instance of org.w3c.dom.Document . Making many toXML() methods in each object would definitely be a disaster. Xembly provides a solution. As I've mentioned before , it is an imperative language for XML constructions and manipulations. Here is how we can implement our Book object with the help of Xembly: import org.xembly.Directive ; public class Book { private final String isbn ; private final String title ; public Book ( final String isbn , final String title ) { this . isbn = isbn ; this . title = title ; } public Iterable < Directive > toXembly () { return new Directives () . add ( \"book\" ) . add ( \"isbn\" ). set ( this . isbn ). up () . add ( \"title\" ). set ( this . title ). up () . up (); } } Now, in order to build an XML document, we should use this code outside the object: final Book book = new Book ( \"0132350882\" , \"Clean Code\" ); final String xml = new Xembler ( book . toXembly ()). xml (); This Xembler class will convert Xembly directives into an XML document. The beauty of this solution is that the internals of the object are not exposed via getters and the object is fully in charge of the XML marshalling process. In addition, the compexity of these directives may be very high — much higher than the rather cumbersome annotations of JAXB. Xembly is an open-source project, so feel free to submit your questions or corrections to Github . "},{"title":"How AppVeyor Helps Me to Validate Pull Requests Before Rultor Merges Them","url":"/2015/03/29/rultor-with-appveyor.html","tags":["devops","rultor"],"date":"2015-03-29 00:00:00 +0000","categories":[],"body":" AppVeyor is a great cloud continuous integration service that builds Windows projects. Rultor is a DevOps assistant, which automates release, merge and deploy operations, using Docker containers. These posts explain how Rultor works and what it's for: Rultor.com, a Merging Bot and Master Branch Must Be Read-Only . The problem is that Rultor is running all scripts inside Docker containers and Docker can't build Windows projects. The only and the best logical solution is to trigger AppVeyor before running all other scripts in Docker. If AppVeyor gives a green light, we continue with our usual in-Docker script. Otherwise, we fail the entire build. Below I explain how this automation was configured in Takes framework . First, I got a token from my AppVeyor account (at the time of writing it was here ). I created a text file curl-appveyor.cfg with this content (it's not my real token inside, just an example): --silent --header \"Authorization: Bearer 1hdmsfbs7xccb9x6g1y4\" --header \"Content-Type: application/json\" --header \"Accept: application/json\" Then, I encrypted this file, using rultor command line tool: $ rultor encrypt -p yegor256/takes curl-appveyor.cfg The file I creaeted was called curl-appveyor.cfg.asc . I committed and pushed into yegor256/takes Github repository. $ git add curl-appveyor.cfg.asc $ git commit -am 'CURL config for Appveyor' $ git push origin master Then, I configured AppVeyor \"pinging\" from Docker script. This is what I did in .rultor.yml : decrypt : curl-appveyor.cfg : \"repo/curl-appveyor.cfg.asc\" merge : script : |- version=$(curl -K ../curl-appveyor.cfg --data \"{accountName: 'yegor256', projectSlug: 'takes', pullRequestId: '${pull_id}'}\" https://ci.appveyor.com/api/builds | jq -r '.version') while true do status=$(curl -K ../curl-appveyor.cfg https://ci.appveyor.com/api/projects/yegor256/takes/build/${version} | jq -r '.build.status') if [ \"${status}\" == \"success\" ]; then break; fi if [ \"${status}\" == \"failed\" ]; then echo \"see https://ci.appveyor.com/project/yegor256/takes/build/${version}\"; exit 1; fi echo \"waiting for AppVeyor build ${version}: ${status}\" sleep 5s done mvn clean install There is no magic here, it's very simple. First, I start a new build using /api/builds end-point of AppVeyor REST API . ${pull_id} is an environment variable that is coming from Rultor, it contains an integer number of current pull request. I'm using jq in order to parse AppVeyor JSON output. Once the build is started, I'm getting its unique version and start looping to check its status. I'm waiting for success or failed . Anything else will mean that the build is still in progress and I should keep looping. You can see how it works in this pull request, for example: yegor256/takes#93 . "},{"title":"Class Casting Is a Discriminating Anti-Pattern","url":"/2015/04/02/class-casting-is-anti-pattern.html","tags":["oop"],"date":"2015-04-02 00:00:00 +0000","categories":[],"body":"Type casting is a very useful technique when there is no time or desire to think and design objects properly. Type casting (or class casting) helps us work with provided objects differently, based on the class they belong to or the interface they implement. Class casting helps us discriminate against the poor objects and segregate them by their race, gender, and religion. Can this be a good practice? Гадкий утенок (1956) by Владимир Дегтярёв This is a very typical example of type casting (Google Guava is full of it, for example Iterables.size() : public final class Foo { public int sizeOf ( Iterable items ) { int size = 0 ; if ( items instanceof Collection ) { size = Collection . class . cast ( items ). size (); } else { for ( Object item : items ) { ++ size ; } } return size ; } } This sizeOf() method calculates the size of an iterable. However, it is smart enough to understand that if items are also instances of Collection , there is no need to actually iterate them. It would be much faster to cast them to Collection and then call method size() . Looks logical, but what's wrong with this approach? I see two practical problems. First, there is a hidden coupling of sizeOf() and Collection . This coupling is not visible to the clients of sizeOf() . They don't know that method sizeOf() relies on interface Collection . If tomorrow we decide to change it, sizeOf() won't work. And we'll be very surprised, since its signature says nothing about this dependency. This won't happen with Collection , obviously, since it is part of the Java SDK, but with custom classes, this may and will happen. The second problem is an inevitably growing complexity of method sizeOf() . The more special types it has to treat differently, the more complex it will become. This if/then forking is inevitable, since it has to check all possible types and give them special treatment. Such complexity is a result of a violation of the single responsibility principle. The method is not only calculating the size of Iterable but is also performing type casting and forking based on that casting. What is the alternative? There are a few, but the most obvious is method overloading (not available in semi-OOP languages like Ruby or PHP): public final class Foo { public int sizeOf ( Iterable items ) { int size = 0 ; for ( Object item : items ) { ++ size ; } return size ; } public int sizeOf ( Collection items ) { return items . size (); } } Isn't that more elegant? Philosophically speaking, type casting is discrimination against the object that comes into the method. The object complies with the contract provided by the method signature. It implements the Iterable interface, which is a contract , and it expects equal treatment with all other objects that come into the same method. But the method discriminates objects by their types. The method is basically asking the object about its ... race. Black objects go right while white objects go left. That's what this instanceof is doing, and that's what discrimination is all about. By using instanceof , the method is segregating incoming objects by the certain group they belong to. In this case, there are two groups: collections and everybody else. If you are a collection, you get special treatment. Even though you abide by the Iterable contract, we still treat some objects specially because they belong to an \"elite\" group called Collection . You may say that Collection is just another contract that an object may comply with. That's true, but in this case, there should be another door through which those who work by that contract should enter. You announced that sizeOf() accepts everybody who works on the Iterable contract. I am an object, and I do what the contract says. I enter the method and expect equal treatment with everybody else who comes into the same method. But, apparently, once inside the method, I realize that some objects have some special privileges. Isn't that discrimination? To conclude, I would consider instanceof and class casting to be anti-patterns and code smells. Once you see a need to use them, start thinking about refactoring. "},{"title":"Tacit, a CSS Framework Without Classes","url":"/2015/04/13/tacit-css-framework-for-dummies.html","tags":["css","html"],"date":"2015-04-13 00:00:00 +0000","categories":[],"body":"I've been using Bootstrap for more than two years in multiple projects, and my frustration has been building. First of all, it's too massive for a small web app. Second, it is not fully self-sufficient; no matter how much you follow its principles of design, you end up with your own CSS styles anyway. Third, and most importantly, its internal design is messy. Having all this in mind, I created tacit , my own CSS framework, which immediately received positive feedback on Hacker News . Tacit , according to Google, means \"understood or implied without being stated\". That's exactly the idea of the framework . It doesn't have a single CSS class and can be applied to any valid HTML5 document. For example, you have an HTML document: <!DOCTYPE html> <html> <head> <title> Subscribe </title> </head> <body> <section> <p> Are you interested in learning more? </p> <form> <label> Email: </label> <input name= \"email\" /> <button type= \"submit\" > Subscribe </button> </form> </section> </body> </html> This is how it looks in Safari: Now, I add tacit.min.css to it: <!DOCTYPE html> <html> <head> <title> Subscribe </title> <link rel= \"stylesheet\" type= \"text/css\" href= \"http://yegor256.github.io/tacit/tacit.min.css\" /> </head> <body> <section> <p> Are you interested in learning more? </p> <form> <label> Email: </label> <input name= \"email\" type= \"text\" /> <button type= \"submit\" > Subscribe </button> </form> </section> </body> </html> This is how it looks in the same Safari browser: I hope you got the idea. The HTML itself wasn't changed at all. All CSS styles are applied to standard HTML elements. Unlike many other CSS frameworks, in Tacit you don't have to mention CSS classes in the HTML document. The HTML stays clean and only exposes the data in a pure HTML5 way. The HTML document is still readable and usable, but it doesn't have the good-looking-graphics component. Tacit adds that component in a non-intrusive manner. Of course, in many projects, the default layout features of Tacit won't be enough. In most cases, I still have to add my own CSS classes and inline styles. But Tacit gives me an adequate foundation to start from. It solves most of the problems associated with responsiveness of forms, appearance of form controls, tables, fonts, and colors. Tacit allows me to focus on functionality from the first day of a project. And the functionality immediately looks attractive. I have tried many other frameworks, including Bootstrap , Kube , and Pure . None of them are designed with this concept in mind. They all put CSS in front of HTML. In all of them, CSS is the most important element of web design, while HTML is something that assists. Tacit takes a different approach. In Tacit, HTML is king while CSS is a supportive element that only makes data look better. Enjoy :) "},{"title":"Software Quality Award","url":"/award.html","tags":["mgmt","oop"],"date":"2015-04-16 00:00:00 +0000","categories":["jcg"],"body":"I'm a big fan of rules and discipline in software development; as an example, see Are You a Hacker or a Designer? . Also, I'm a big fan of object-oriented programming in its purest form; for example, see Seven Virtues of a Good Object . I'm also a co-founder and the CTO of Teamed.io , a software development company through which I put my admiration of discipline and clean design into practice. I want to encourage you to share my passion — not just by reading this blog but through making real open source software in a disciplined way. This award is for those who are brave enough to swim against the current and value quality above everything else. Send me your own project for review and participate in the contest. Rules: One person can submit up to three projects. Submissions are accepted until the September 1, 2015 . Submissions must be sent via email to me@yegor256.com . All I need is your Github login and repository name; I will check the commit history to make sure you're the main contributor to the project. I reserve the right to reject any submission without explanation. All submissions will be published on this page (including rejected ones). Results will be announced October 15 on this page and by email. The best project will receive $4,096 . Final decisions will be made by me and are not negotiable (although I may invite other people to help me make the right decision). Each project must be: Open source. At least 5,000 lines of code. At least one year old. Object-oriented (that's the only thing I understand). The best projects will feature: Strict and visible principles of design. Continuous delivery. Traceability of changes. Self-documented source code. Strict rules of code formatting. What doesn't matter: Popularity. Even if nobody is using your product, it is still eligible for this award. I don't care about popularity; quality is the key. Programming language. I believe that any language, used correctly, can be applied to design a high-quality product. Buzz and trends. Even if your project is yet another parser of command line arguments, it's still eligible for the award. I don't care about your marketing position; quality is all. By the way, if you want to sponsor this award and increase the bonus, email me . 84 rojects submitted so far (in order of submission): mdbs99/AWS FagnerMartinsBrack/WebStories robertop/triumph4php blambeau/wlang alf-tool/alf-core by @blambeau siom79/japicmp tunabrain/tungsten AdamsLair/duality openra/openra by @chrisforbes ApiGen/ApiGen by @TomasVotruba Pacmanfan/UVDLPSlicerController openfurther/further-open-core by @rahuofu Bertram25/ValyriaTear odoepner/typepad vladmihalcea/flexy-pool js-cookie/js-cookie by @FagnerMartinsBrack jOOQ/jOOQ by @lukaseder raphw/byte-buddy uniVocity/univocity-parsers by @jbax arnaudroger/SimpleFlatMapper elBukkit/MagicPlugin by @NathanWolf trade-manager/trade-manager ParaPenguin/morphix tzaeschke/zoodb tzaeschke/critbit praveendath92/MDroid DeqingSun/ESP8266-Dash-Button javamonkey/beetl2.0 81813780/HandyTabBar xionghuiCoder/clearpool android-cjj/ComicReader Dreampie/icedog hujiaweibujidao/TinyWeibo hujiaweibujidao/XingShan hujiaweibujidao/WeChat4j beartung/tclip-android beartung/insta-filter wewoor/ZBLOG gulpjs/gulp by @contra joedayz/titanic-javaee7 TheCricket/Chisel-2 ddasilva/scheme-droid chenjishi/SlideActivity yazeed44/MultiImagePicker wbotelhos/raty miku-nyan/Overchan-Android chylex/Hardcore-Ender-Expansion lionsoul2014/jcseg yfpeng/pengyifan-bioc daimor/NBStudio sytolk/TaxiAndroidOpen yaylas/AndroidFaceRecognizer omgware/fluid-simulator-v2 wendykierp/JTransforms zqq90/webit-script markdenihan/owaspSecurityShepherd subchen/jetbrick-template-2x subchen/snack-string restorer/gloomy-dungeons-2 restorer/Gloomy-Dungeons-3D restorer/zame-haxe-particles mrzl/LeapMotionP5 relu91/niftyeditor bparmentier/OpenBikeSharing graphhopper/graphhopper by @karussell t-oster/VisiCut arnaudroger/SimpleFlatMapper Floens/Clove chrisshayan/TechLooper bonigarcia/dualsub blundell/WoodyFaceDetection blundell/QuickSand blundell/ArrowLogger pushtorefresh/storio by @artem-zinnatullin mangstadt/ez-vcard jmyrland/DriSMo pmd/pmd by @adangel javaslang/javaslang thothbot/parallax jasonycw/MemeCreator jasonycw/TyphoonTycoon ysc/QuestionAnsweringSystem ysc/word andot/hrpose guikeller/jetty-runner "},{"title":"My Favorite Software Books","url":"/2015/04/22/favorite-software-books.html","tags":["book-review"],"date":"2015-04-22 00:00:00 +0000","categories":[],"body":"There are plenty of books about software engineering, but only a few of them rank among my favorites. I read all of those that do over and over again, and I might just update this post in the future when I stumble upon something else that's decent. Note that I tried to put the most important books at the top of the list. Object Thinking by David West. This is the best book I've read about object-oriented programming, and it totally changed my understanding of it. I would recommend you read it a few times. But before reading, try to forget everything you've heard about programming in the past. Try to start from scratch. Maybe it will work for you too :) PMP Exam Prep, Eighth Edition: Rita's Course in a Book for Passing the PMP Exam by Rita Mulcahy. This book is my favorite for project management. Even though it's about the PMI approach and PMBOK in particular, it is a must-read for everyone who is interested in management. Ignore the PMBOK specifics and focus on the philosophy of project management and the role of project manager in it. The Art of Software Testing by Glenford J. Myers et al. You can read my short review of this book here . The book perfectly explains the philosophy of testing and destroys many typical myths and stereotypes. No matter what your job description is, if you're working in the software industry, you should understand testing and its fundamental principles. This is the only book you need in order to get that understanding. Growing Object-Oriented Software, Guided by Tests by Steve Freeman and Nat Pryce. All you need to know about your unit testing is in this book. I'm fully aware that I didn't include famous software engineer Kent Beck's book in this list because I don't like it at all. You definitely should read it, just to know what's going on, but it won't help you write good tests. Read this one instead, and read it many times. Working Effectively With Legacy Code by Michael Feathers. This is awesome reading about modern software development, its pitfalls, and typical failures. Most of the code we're working on now is legacy (a.k.a. open source). I read this book as a novel. Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation by Jez Humble and David Farley. This is a perfect book about software delivery, continuous integration, testing, packaging, versioning, and many other techniques involved in programming. It's definitely a must-read for anyone who is serious about software engineering. XML in a Nutshell, Third Edition by Elliotte Rusty Harold and W. Scott Means. XML is my favorite standard. And I hated it before I read this book. I didn't understand all the strange prefixes, namespaces, XPath expressions, and schemas. Just this one book changed everything, and ever since reading it, I've used XML everywhere. It is very well written and easy to read. It's a must for everybody. Java Concurrency in Practice by Brian Goetz et al. This is a very practical book about Java multi-threading, and at the same time, it provides a lot of theoretical knowledge about concurrency in general. I highly recommend you read it at least once. Effective Modern C++: 42 Specific Ways to Improve Your Use of C++11 and C++14 by Scott Meyers. No matter what language you're using, this book is very interesting and very useful. It makes many important suggestions about better C++ coding. If you understand most of them, your Java/Ruby/Python/Scala coding skills will improve significantly. Code Complete: A Practical Handbook of Software Construction, Second Edition by Steve McConnell. Consider this the bible of clean coding. Read it a few times and use it as a reference manual in debates with your colleagues. It mentions the most terrible anti-patterns and worst practices you'll see in modern programming. To be a good programmer, you must know all of them. Software Estimation: Demystifying the Black Art by Steve McConnell. This one's an interesting read about software engineering and its most tricky part — estimations. At the least, read it to be aware of the problem and possible solutions. Writing Effective Use Cases by Alistair Cockburn. An old and very good book, you won't actually use anything from this in your real projects, but you will pick up the philosophy of use cases, which will redirect your mind in the right direction. Don't take this book as something practical; these use cases are hardly used anywhere today, but the idea of scoping functionality this way is absolutely right. Software Requirements, Third Edition by Karl Wiegers (author) and Joy Beatty. A superb book about requirements analysis, the first and most important activity in any software project. Even if you're not an analyst, this book is a must-read. Version Control With Git: Powerful Tools and Techniques for Collaborative Software Development by Jon Loeliger and Matthew McCullough. This title serves as a practical guide for Git, a version control system. Read it from cover to cover and you will save many hours of your time in the future. Git is a de-facto standard in version control, and every programmer must know its fundamental principles — not from a cheat sheet but from an original source. JavaScript: The Definitive Guide: Activate Your Web Pages by David Flanagan. JavaScript is a language of the modern Web, and this book explains it very well. No matter what kind of software you develop, you must know JavaScript. Don't read it as a practical guide (even though it's called a guide) but rather as food for thought. JavaScript offers a lot to learn for Java/Ruby/Python developers. CSS: The Definitive Guide by Eric A. Meyer. CSS is not just about colors and shadows, and it's not only for graphic designers. CSS is a key language of the modern Web. Every developer must know it, whether you're working with a back-end, front-end, or desktop application in C++. "},{"title":"How to Implement an Iterating Adapter","url":"/2015/04/30/iterating-adapter.html","tags":["java"],"date":"2015-04-30 00:00:00 +0000","categories":["jcg"],"body":"Iterator is one of the fundamental Java interfaces, introduced in Java 1.2. It is supposed to be very simple; however, in my experience, many Java developers don't understand how to implement a custom one, which should iterate a stream of data coming from some other source. In other words, it becomes an adapter of another source of data, in the form of an iterator. I hope this example will help. Let's say we have an object of this class: final class Data { byte [] read (); } When we call read() , it returns a new array of bytes that were retrieved from somewhere. If there is nothing to retrieve, the array will be empty. Now, we want to create an adapter that would consume the bytes and let us iterate them: final class FluentData implements Iterator < Byte > { boolean hasNext () { /* ... */ } Byte next () { /* ... */ } void remove () { /* ... */ } } Here is how it should look (it is not thread-safe!): final class FluentData implements Iterator < Byte > { private final Data data ; private final Queue < Byte > buffer = new LinkedList <>(); public FluentData ( final Data dat ) { this . data = dat ; } public boolean hasNext () { if ( this . buffer . isEmpty ()) { for ( final byte item : this . data . read ()) { this . buffer . add ( item ); } } return ! this . buffer . isEmpty (); } public Byte next () { if (! this . hasNext ()) { throw new NoSuchElementException ( \"Nothing left\" ); } return this . buffer . poll (); } public void remove () { throw new UnsupportedOperationException ( \"It is read-only\" ); } } There is no way to make it thread-safe because the iterating process is outside the scope of the iterator. Even if we declare our methods as synchronized , this won't guarantee that two threads won't conflict when they both call hasNext() and next() . So don't bother with it and just document the iterator as not thread-safe, then let its users synchronize one level higher when necessary. "},{"title":"How to Protect a Business Idea While Outsourcing","url":"/2015/05/04/how-to-protect-business-idea.html","tags":["mgmt"],"date":"2015-05-04 00:00:00 +0000","categories":[],"body":"When you hire a programmer or a team of programmers to implement your business idea, there is a significant risk of theft and accidental loss. They may implement your idea (or its elements) without you using their own resources. Also, they may disclose it to their friends, and those friends may disclose it even further, until it is eventually implemented by someone you don't even know. This happens a lot — and everywhere. I've been on both sides. Here is my experience and a few recommendations. There Will Be Blood (2007) by Paul Thomas Anderson There are basically a few levels of protection you can obtain, and they are listed below, from the simplest and least secure to the most expensive and most secure. Very often, a software team you outsource programming to is located offshore in a developing country where people care about laws much less than in, say, the United States; corruption makes it possible for them to get away with ignoring almost any violation. Let's not forget this. We're not discussing here the risk of losing a product. This also happens very often — your programmers start working for you, you pay them, they show you something, and then things go south. You find yourself in conflict with them, and they don't deliver you anything; they ask for extra money instead. You end up with nothing or something that is broken and can't be put on the market. This is yet another risk, which I'll try to describe in another article soon. In this article, we're talking specifically about a situation where your programmers are using your idea to implement something similar on their own. Let's say you want to create a new search engine that would be smarter and more accurate than Google. You disclose your alrogithms to a group of talented programmers and they just implement it on their own. You already gave them a multi-billion-dollar idea; why would they work for you on the payroll if they can create their own startup? This is the question you, as a savvy entrepreneur, must be prepared to answer for yourself. By the way, I'm not a lawyer; I'm speaking here from practical experience only. Non-Disclosure Agreement (NDA) The first and easiest step is a so-called NDA. Here is a very simple and useful example of one, from NOLO . You put the name of your programmers into the document and ask them to sign it. They won't object, and you get a piece of paper with a signature; what's next? If I'm a programmer, the document basically states that whatever you disclose to me, I should keep in secret and never \"use for my own benefit, publish, copy, or otherwise disclose to others.\" If you then give me your Google-killer idea and I create my own product using its key principles, what will be your next steps? In a court of law in your country, you will have to prove that: 1) you disclosed your idea to me, 2) I used it to create the product, 3) I didn't know about this idea beforehand, and 4) I didn't invent it myself. Until all of these criteria are proven, I'm innocent and my product is online, working and attracting customers. Will you be able to prove that you disclosed the idea to me? Probably, if you sent me some documents. If you discussed it with me over the phone, you won't prove anything. Will you be able to prove that your exact idea was used in my product? How will it technically be possible if I don't disclose the source code? Can you prove that I didn't know about something similar before meeting you? Or maybe I was thinking about it on my own. Or I read about it somewhere else. What if I disclose your idea to my friends and they create a product? Will you be able to prove the fact of that disclosure? There are many such questions. My point is that a signed NDA is a very weak protection. It's more like a lock on the bathroom door — anyone can break it with a kick from their leg, but for those with good manners, it's a sign that the restroom is occupied. Non-Compete Clause (NCC) The next level of protection is an agreement with a Non-Compete Clause , which explicitly prohibits me from engaging in the business that your idea is about. For example, it may sound like this: \"The developer is not allowed to participate in any businesses related to online searching for five years.\" Will I sign this agreement? It depends. But if I show any reluctance in signing it, you should think twice about my real intentions. Will this NCC protect you if I disclose your idea to my friends and they implement it? No, it won't. Also, I would recommend you put some explicit liability numbers into the agreement. For example, it may sound like \"a minimum penalty for a proven breach of the non-compete clause is $50,000\". In my experience, such explicit statements make contracts much more valuable and help prevent them from being violated. Copyright Copyright is where the government starts to protect you, but you have to pay us for it (by \"us\" I mean all of us, the society). Well, at least in the United States. In the United Kingdom, it's free, for example. In the U.S., you go to copyright.gov , click \"register a copyright\", fill in an online form, post the description of your business idea in a plain text file, pay $35, and you're done. In a few months, you will receive a confirmation that your \"record\" is registered. What does it give you? In a court of law, you can claim that this idea came to your mind on that specific date. Everyone who later made something similar probably stole it from you, including me, your programmer. Will you be able to prove that my product is actually based on your idea and steals it? Maybe. It's not a very strong protection either, but I would recommend you do it together with an NDA and NCC. Patent A patent is the best you can get to protect your idea. A patent is basically a guarantee of safety that you buy from the government. To get that guarantee, you have to do three things: 1) explain what exactly will be protected, 2) prove that it doesn't belong to someone else, and 3) pay your dues regularly. It's very similar to what gangsters do when they \"protect\" you, but here we're dealing with intellectual property and there is only one \"gangster\" per country :) The concept is pretty much the same. First, you describe your idea in the format that patents are written. It is not difficult at all, but it would help if you read one of those \"how to file a patent\" books. I recommend Patent It Yourself: Your Step-by-Step Guide to Filing at the U.S. Patent Office by David Pressman and Thomas J. Tuytschaevers. Your application will likely amount to about 10 pages and should take a few days of your time if you know your idea well. No need to hire any attorneys; that's a waste of money. Then you should do some research to make sure something similar already exists — but not exactly the same. For example, you can find Google patents for searching algorithms and mention them in your patent in the list of references. Finally, you pay $425 (if you're a small company) and submit it to the USPTO . There is also an option to file a \"provisional\" patent, but I would recommend you not do this, as it's just an extra hassle. Simply file a normal one. Once you've paid, your protection starts immediately. If I create a product that uses the idea described in your patent, you can bring me to court and ask me to share my profit with you. You will claim that I was making money by using your brilliant idea, and now it's time to share that success. Legally speaking, you will accuse me of patent infringement . First, I will try to invalidate your patent, claiming that something similar already existed before you filed a patent, called prior art . If I succeed, the USPTO will invalidate your patent without a refund, and I'll walk away, paying you nothing. If I fail, I'll try to prove that I didn't infringe on your patent. I will say that my product is not using your ideas but rather is designed with something else in mind, just like Samsung did . Maybe I'll win, but my chances will be low. By the way, in a few years, you will receive a patent from the USPTO and put it on the shelf. You will then have to pay $480 more. Also, at the end of three years, you will have to pay $800 just to keep your guarantee alive. That escalates to $1,800 in seven years and $3,700 in 11 years (see the fee schedule ). Told you; just like gangsters :) To summarize, getting a patent is the best instrument available at the moment in developed countries that can protect your business idea. However, as Apple vs. Samsung lawsuit demonstrates, it is not a 100 percent guarantee either. "},{"title":"Constructors Must Be Code-Free","url":"/2015/05/07/ctors-must-be-code-free.html","tags":["java","oop"],"date":"2015-05-07 00:00:00 +0000","categories":["jcg"],"body":"How much work should be done within a constructor? It seems reasonable to do some computations inside a constructor and then encapsulate results. That way, when the results are required by object methods, we'll have them ready. Sounds like a good approach? No, it's not. It's a bad idea for one reason: It prevents composition of objects and makes them unextendable. Kill Bill: Vol. 2 (2004) by Quentin Tarantino Let's say we're making an interface that would represent a name of a person: interface Name { String first (); } Pretty easy, right? Now, let's try to implement it: public final class EnglishName implements Name { private final String name ; public EnglishName ( final CharSequence text ) { this . parts = text . toString (). split ( \" \" , 2 )[ 0 ]; } @Override public String first () { return this . name ; } } What's wrong with this? It's faster, right? It splits the name into parts only once and encapsulates them. Then, no matter how many times we call the first() method, it will return the same value and won't need to do the splitting again. However, this is flawed thinking! Let me show you the right way and explain: public final class EnglishName implements Name { private final CharSequence text ; public EnglishName ( final CharSequence txt ) { this . text = txt ; } @Override public String first () { return this . text . toString (). split ( \"\" , 2 )[ 0 ]; } } This is the right design. I can see you smiling, so let me prove my point. Before I start proving, though, let me ask you to read this article: Composable Decorators vs. Imperative Utility Methods . It explains the difference between a static method and composable decorators. The first snippet above is very close to an imperative utility method, even though it looks like an object. The second example is a true object. In the first example, we are abusing the new operator and turning it into a static method, which does all calculations for us right here and now . This is what imperative programming is about. In imperative programming, we do all calculations right now and return fully ready results. In declarative programming, we are instead trying to delay calculations for as long as possible. Let's try to use our EnglishName class: final Name name = new EnglishName ( new NameInPostgreSQL ( /*...*/ ) ); if ( /* something goes wrong */ ) { throw new IllegalStateException ( String . format ( \"Hi, %s, we can't proceed with your application\" , name . first () ) ); } In the first line of this snippet, we are just making an instance of an object and labeling it name . We don't want to go to the database yet and fetch the full name from there, split it into parts, and encapsulate them inside name . We just want to create an instance of an object. Such a parsing behavior would be a side effect for us and, in this case, will slow down the application. As you see, we may only need name.first() if something goes wrong and we need to construct an exception object. My point is that having any computations done inside a constructor is a bad practice and must be avoided because they are side effects and are not requested by the object owner. What about performance during the re-use of name , you may ask. If we make an instance of EnglishName and then call name.first() five times, we'll end up with five calls to the String.split() method. To solve that, we create another class, a composable decorator , which will help us solve this \"re-use\" problem: public final class CachedName implements Name { private final Name origin ; public CachedName ( final Name name ) { this . origin = name ; } @Override @Cacheable ( forever = true ) public String first () { return this . origin . first (); } } I'm using the Cacheable annotation from jcabi-aspects , but you can use any other caching tools available in Java (or other languages), like Guava Cache : public final class CachedName implements Name { private final Cache < Long , String > cache = CacheBuilder . newBuilder (). build (); private final Name origin ; public CachedName ( final Name name ) { this . origin = name ; } @Override public String first () { return this . cache . get ( 1L , new Callable < String >() { @Override public String call () { return CachedName . this . origin . first (); } } ); } } But please don't make CachedName mutable and lazily loaded — it's an anti-pattern, which I've discussed before in Objects Should Be Immutable . This is how our code will look now: final Name name = new CachedName ( new EnglishName ( new NameInPostgreSQL ( /*...*/ ) ) ); It's a very primitive example, but I hope you get the idea. In this design, we're basically splitting the object into two parts. The first one knows how to get the first name from the English name. The second one knows how to cache the results of this calculation in memory. And now it's my decision, as a user of these classes, how exactly to use them. I will decide whether I need caching or not. This is what object composition is all about. Let me reiterate that the only allowed statement inside a constructor is an assignment. If you need to put something else there, start thinking about refactoring — your class definitely needs a redesign. "},{"title":"Three Things I Expect From a Software Architect","url":"/2015/05/11/software-architect-responsibilities.html","tags":["mgmt","agile","architect"],"date":"2015-05-11 00:00:00 +0000","categories":[],"body":"A software architect is a key person in a software project, which I explained in my What Does a Software Architect Do? post a few months ago. The architect is personally responsible for the technical quality of the product we're developing. No matter how good the team is, how complex the technology is, how messy the requirements are, or how chaotic the project sponsor is, we blame the architect and noone else. Of course, we also reward the architect if we succeed. Here is what I, as project manager, expect from a good architect. Dr. Strangelove (1964) by Stanley Kubrick In all projects we run at Teamed.io , I expect regular reports from software architects a few times a week. Each report includes three mandatory parts: 1) scope status, 2) issues, and 3) risks. Scope Status The first and most important type of information I'm looking for is the scope status, which should be presented in Product Breakdown Structure (PBS) format. No matter how complex or how small the product is, a good architect should be able to create a PBS of four to eight items . For example: 1. MySQL persistence [done] 2. OAuth login [done] 3. Input parsing in XML [75%] 4. S3 data storage [none] 5. UI cross-platform testing [none] That's the size of the report I'm expecting to receive from a good architect every few days. The main goal for the architect here is to make sure that nothing is missed. No matter how big the project is, all its technical components must fit into this PBS. The architect is personally responsible for not missing the information in the PBS and making it as accurate as possible. If something is missed or the report is delayed, that becomes a good reason to change the architect. The percentages of progress are also important here. Even though individual tasks are managed with the \"0/100 completion principle\" in mind, the architect must compile those percentages and make sure that compilation is accurate. Again, a mistake here is unforgivable. Issues The second important part of a regular report from an architect is a list of current issues the development team is facing. An issue is something that has already happened and we're suffering from it. Here are a few practical examples: 1. MySQL is too slow for our performance requirements 2. Java 1.6 doesn't allow us to use library X 3. We don't have a replacement for a Ruby guy who left us 4. Integration tests are not predictable Again, the list must include four to eight items (no more and no less), and the architect should mention the most critical issues there. Risks Now, the risks. A risk is something that hasn't happened yet but may happen soon, and if it happens, we'll be in trouble. The architect is responsible for keeping an eye on all potential risks and regularly reporting the most critical ones to the project manager. Here is an example of a brief risk report: 1. Deployment platform may not support Java 8 [3/8] 2. Library X may take more than the two weeks planned [7/3] 3. We may lose a good Ruby developer soon [5/6] 4. Integration tests may not be safe enough [7/2] 5. We may fail to find an open source library [3/8] A project manager may require additional information about each risk, but that's another story. What is most important is to keep the project manager informed about the top of the list. Each risk has two numbers associated with it: probability and impact , from 0 to 9. In the list above, the first risk has a probability of 3 and impact of 8. This means the architect believes that most likely this won't happen, but if it does happen, we'll be in big trouble. Pay attention, as the key word in each risk description is may . A risk is something that hasn't happened yet. That's the biggest difference between a risk and an issue. An issue is a risk that has already occurred. PS. Here is how an architect can enforce the principles of design and architecture: Two Instruments of a Software Architect "},{"title":"Two Instruments of a Software Architect","url":"/2015/05/13/two-instruments-of-software-architect.html","tags":["mgmt","agile","architect"],"date":"2015-05-13 00:00:00 +0000","categories":[],"body":"A software architect is a key person in any software project, no matter how big or small it is. An architect is personally responsible for the technical outcome of the entire team. A good architect knows what needs to be done and how it's going to be done, both architecturally and design-wise. In order to enforce this idea in practice, an architect uses two instruments: bugs and reviews . Rear Window (1954) by Alfred Hitchcock At Teamed.io , we discourage any communication between developers unless they are formally attached to the tickets or tasks we're working on. Read more details about this approach in this post . The same principle applies to an architect. We don't use meetings, stand-ups, Skype calls, IRC channels, or any other tools where information flies in the air and stays in our heads. Instead, we put everything in writing and talk only when we're being explicitly asked to and paid to — in tickets. Bugs With this in mind, a reasonable question may be asked: How can a software architect enforce his or her technical vision for the team if he can't communicate with the team? Here is our answer: the architect must use bugs . A bug is a ticket that has a reporter, a problem, and a resolver, just like this post explains. Say an architect reviews an existing technical solution and finds something that contradicts his vision. When such a contradiction is found, it is a good candidate for a bug. Sometimes there is just not enough information in the code yet, and this is also a good candidate for a bug. Thus, bugs reported by an architect serve as communication channels between him and the team. An architect doesn't explain what needs to be done but asks the team to fix the product in a way he thinks is right. If the ticket resolver, a member of the team, disagrees with that approach, a discussion starts right in the ticket. Sometimes an architect has doubts and needs to discuss a few possible solutions with the team or simply collect opinions. Again, we use bugs for that. But these bugs don't report problems in the source code; instead, they complain about incomplete documentation. For example, say an architect doesn't know which database to use, MongoDB or Cassandra, and needs more information about their pros and cons. A bug will sound like \"our design documentation doesn't have a detailed comparison of existing NoSQL databases; please fix it\". Whoever is assigned to this ticket will perform the comparison and update the documentation. Bugs are a proactive tool for an architect. Through reporting bugs, an architect influences the project and \"dictates his will\". Reviews In our projects, every ticket is implemented in its own branch. When implementation is done, all tickets pass mandatory code peer review. In other words, developers review each others' code. An architect is not involved in this process. But when peer review is done, each ticket goes to an architect and he has to give a final \"OK\" before the code goes to the master branch through Rultor, our merge bot . This is an architect's opportunity for control. This is where he can prevent his vision from being destroyed. When the code created by a developer violates project design principles or any part of the entire technical idea, the architect says \"No\" and the branch is rejected. Reviews are a reactive instrument for an architect. Through strict and non-compromising code reviews, an architect enforces his design and architectural principles. PS. Here is how an architect is supposed to report to the project manager: Three Things I Expect From a Software Architect "},{"title":"How Cookie-Based Authentication Works in the Takes Framework","url":"/2015/05/18/cookie-based-authentication.html","tags":["java"],"date":"2015-05-18 00:00:00 +0000","categories":[],"body":" When you enter your email and password into the Facebook login page, you get into your account. Then, wherever you go in the site, you always see your photo at the top right corner of the page. Facebook remembers you and doesn't ask for the password again and again. This works thanks to HTTP cookies and is called cookie-based authentication . Even though this mechanism often causes some security problems, it is very popular and simple. Here is how Takes makes it possible in a few lines of code. First, let's see how it works. Moreover, let's see how I believe it should work. Step one: The user enters an email and password and clicks \"submit\". The server receives a POST request with this information inside: POST / HTTP/1.1 Host: www.facebook.com Content-Type: application/x-www-form-urlencoded email=me@yegor256.com&password=itisasecret The server matches the provided information with its records and decides what to do. If the information is invalid, it returns the same login page, asking you to enter it all again. If the information is valid, the server returns something like this: HTTP/1.1 303 See Other Location: www.facebook.com Set-Cookie: user=me@yegor256.com Since the response status code is 303, the browser goes to the page specified in the Location header and opens the front page of the site. This is what it sends to the server: GET / HTTP/1.1 Host: www.facebook.com Cookie: user=me@yegor256.com The server gets my email from the Cookie header and understands that it's me again! No need to ask for the password once more. The server trusts the informatiom from the cookie. That's it. That's what cookie-based authentication is all about. Wait ... What About Security? Right, what about security? If the server trusts any browser request with a user email in the Cookie header, anyone would be able to send my email from another place and get access to my account. The first step to prevent this is to encrypt the email with a secret encryption key, known only to the server. Nobody except the server itself will be able to encrypt it the same way the server needs to decrypt it. The response would look like this, using an example of encryption by XOR cipher with bamboo as a secret key: HTTP/1.1 303 See Other Location: www.facebook.com Set-Cookie: user=b1ccafd92c568515100f5c4d104671003cfa39 This is not the best encryption mechanism, though; for proper encryption, it's better to use something stronger like DES . This all sounds good, but what if someone hijacks the traffic between the server and the browser and gets a hold of a properly encrypted email cookie? In this case, the thief would be able to use the same cookie for authentication even without knowing its content. The server would trust the information and let the person into my account. This type of attack is called man-in-the-middle (MITM). To prevent this from happening, we should use HTTPS and inform the browser that the cookie is sensitive and should never be returned to the server without SSL encryption. That's done by an extra flag in the Set-Cookie header: HTTP/1.1 303 See Other Location: www.facebook.com Set-Cookie: user=me@yegor256.com; Secure There is yet another type of attack associated with cookie-based authentication, based on a browser's ability to expose all cookies associated with a web page to JavaScript executed inside it. An attacker may inject some malicious JavaScript code into the page (Don't ask me how ... this will happen only if your entire HTML rendering is done wrong), and this code will gain access to the cookie. Then, the code will send the cookie somewhere else so the attacker can collect it. This type of attack is called cross-site scripting (XSS). To prevent this, there is another flag for the Set-Cookie header, called HttpOnly : HTTP/1.1 303 See Other Location: www.facebook.com Set-Cookie: user=me@yegor256.com; Secure; HttpOnly The presence of this flag will tell the browser that this particular cookie can be transferred back to the server only through HTTP requests. JavaScript won't have access to it. How It's Done in Takes Here is how this cookie-based authentication mechanism is designed in the Takes framework. The entire framework consists of takes , which receive requests and produce responses ( this article explains the framework in more detail). When the request comes in, we should find the authentication cookie in the Cookie header and translate it to the user credentials. When the response goes out, we should add the Set-Cookie header to it with the encrypted user credentials. That's it. Just these two steps. Let's say we have an account page that is supposed to show the current user's balance: final class TkAccount implements Take { private final Balances balances ; @Override public Response act ( final Request request ) { final Identity user = // get it from request return RsHTML ( String . format ( \"<html>Your balance is %s</html>\" , this . balances . retrieve ( user ) ) ); } } Right after the request comes in, we should retrieve the identity of the user, encoded inside an authenticating cookie. To make this mechanism reusable, we have the TkAuth decorator, which wraps an existing take , decodes an incoming cookie, and adds a new TkAuth header to the request with the user's identification information: final Codec codec = new CcHex ( new CcXOR ( new CcPlain ())); final Pass pass = new PsCookie ( codec ); new TkAuth ( new TkAccount (), pass ); Again, when TkAuth receives a request with an authenticating cookie inside, it asks pass to decode the cookie and return either a valid Identity or Identity.ANONYMOUS . Then, when the response goes back to the browser, TkAuth asks pass to encode the indentity back into a string and adds Set-Cookie to the response. PsCookie uses an instance of Codec in order to do these backward and forward encoding operations. When our TkAccount take wants to retrieve a currently authenticated user identity from the request, it can use RqAuth , a utility decorator of Request : final class TkAccount implements Take { @Override public Response act ( final Request request ) { final Identity user = new RqAuth ( request ). identity (); // other manipulations with the user } } The RqAuth decorator uses the header, added by PsCookie , in order to authenticate the user and create an Identity object. How Is It Composable? This mechanism is indeed very extendable and \"composable\". Let's say we want to skip authentication during integration testing. Here is how: new TkAuth ( take , // original application \"take\" new PsChain ( new PsFake ( /* if running integration tests */ ), new PsCookie ( new CcHex ( new CcXOR ( new CcPlain ())) ) ) ); PsChain implements Pass and attempts to authenticate the user by asking all encapsulated passes, one by one. The first one in the chain is PsFake . Using a single boolean argument in its constructor, it makes a decision whether to return a fake identity or return nothing. With just a single boolean trigger, we can switch off the entire authentication mechanism in the app. Let's say you want to authenticate users through Facebook OAuth. Here is how: new TkAuth ( take , // original application \"take\" new PsChain ( new PsByFlag ( new PsByFlag . Pair ( PsFacebook . class . getSimpleName (), new PsFacebook ( \"... Facebook API key ...\" , \"... Facebook API secret ...\" ) ) ), new PsCookie ( new CcHex ( new CcXOR ( new CcPlain ())) ) ) ); When a user clicks on the login link on your site, the browser goes to facebook.com , where his or her identity is verified. Then, Facebook returns a 302 redirection response with a Location header set to the URL we provide in the login link. The link must include something like this: ?PsByFlag=PsFacebook . This will tell PsByFlag that this request authenticates a user. PsByFlag will iterate through all encapsulated \"pairs\" and try to find the right one. PsFacebook will be the first and the right one. It will connect to the Facebook API using the provided credentials and will retrieve all possible information about the user. Here is how we can implement a logout mechanism: new TkAuth ( take , // original application \"take\" new PsChain ( new PsByFlag ( new PsByFlag . Pair ( PsFacebook . class . getSimpleName (), new PsFacebook ( \"... Facebook API key ...\" , \"... Facebook API secret ...\" ) ), new PsByFlag . Pair ( PsLogout . class . getSimpleName (), new PsLogout () ) ), new PsCookie ( new CcHex ( new CcXOR ( new CcPlain ())) ) ) ); Now, we can add ?PsByFlag=PsLogout to any link on the site and it will log the current user out. You can see how all this works in a real application by checking out the TkAppAuth class in Rultor . "},{"title":"How to Avoid a Software Outsourcing Disaster","url":"/2015/05/21/avoid-software-outsourcing-disaster.html","tags":["outsourcing","mgmt"],"date":"2015-05-21 00:00:00 +0000","categories":[],"body":"Software outsourcing is a disaster waiting to happen; we all know that. First, you find a company that promises you everything you could wish for in a product — on-time and in-budget delivery, highest quality, beautiful user interface, cutting-edge technologies, and hassle-free lifetime support. So you send the first payment and your journey starts. The team hardly understands your needs, the quality is terrible, all your time and budget expectations are severely violated, and the level of frustration is skyrocketing. And the \"best\" part is that you can't get away or else all the money you've spent so far will go down the drain and you will have to start from scratch. You have to stay \"married\" to this team because you can't afford a \"divorce\". Is there a way to do software outsourcing right? The Evil Cult (1993) by Jing Wong Yes, it is possible to do it right and truly hassle-free, but you have to be ready to twist your management philosophy. The basic fundamental principle here is that 1) you should openly and frequently communicate your concerns with the outsourcing team, and 2) they should openly and frequently communicate risks and issues with you. These are two major success factors in software outsourcing that are very often neglected. I learned this principle from Wei Liao Zi . He said, according to Military Strategy Classics of Ancient China , p.239: When information from below reaches up high, and the concerns of up high penetrate to below, this is the most ideal situation Let me demonstrate a few practical examples of software outsourcing disasters and explain how they can be avoided if you follow said 2,500-year-old principle. It Takes Forever and I'm Over Budget! It's always 95 percent ready, and you always have something that is not implemented or is broken. They've done a lot of work, you've paid a lot of money, but a market-ready product is not yet there. It takes week after week and month after month; the backlog always has something, and you simply can't finish this. You're starting to see this project in your nightmares, and Prozac doesn't help anymore. How does this sound? Familiar? I hope you do realize that no matter what kind of contract you signed with your software outsourcing partner, how many schedules you've baselined, or how many promises were made, they want to keep you as a client forever. Well, for as long as you have something in your bank account. You want your business to succeed and flourish, right? They want the same for their business. Your success means a product that is finished and launched to end users. Their success means a neverending process of writing software for you. These two goals have very little in common. I would even say they contradict each other — when you succeed, they fail. Of course, they will tell you they want to finish this product for you and get new contracts in the future. They will say their primary motiviation is to make you happy and obtain a good reference. They will assure you that customer satisfaction is more important than money. However, I'm suggesting you be strong enough to face the reality — it's all lies. The majority of software outsourcing projects fail. The vast majority (see the latest CHAOS report ). Software developers realize this better than you, mostly because they see how it happens every day. And your project is not an exception. Thus, let's forget about these beautiful promises and focus on the ugly reality — you're on your own. Keeping in mind the principle I mentioned above, here is my recommendation: Make sure the team understands 1) your real time and scope constraints and 2) the consequences of their violation. This is about the first part of the principle — you should openly and frequently communicate your concerns . What usually happens is that the outsourcing team remains unaware of a real business situation and only hears \"I need this ASAP\" every second day. \"ASAP\" is not a deadline. Moreover, it's a very de-motivating subsitute for a realistic milestone. When the team doesn't know when exactly you need the product, what exactly has to be ready by that date, and why , it starts to work against you. The emphasis here is on \"why\". For most business owners, it's difficult to answer this question. Why do you need the product to be ready by the first of June? Just because you are sick of waiting? This is not a reasonable answer. You're sick of it but you still have money in your bank account. They will keep invoicing you, and they won't respect you. They won't treat you as a strong and goal-oriented business person. You either aren't smart enough to identify your time constraints or you're hiding them from the team. In either case, they won't appreciate that behavior. Here is how a properly defined time and cost constraint may sound: Features A, B, and D must be ready before the first of June, because our marketing campaign starts on the fifth of June. If we don't have them ready, I will lose $25,000 in marketing costs. If this happens, I will have to cut the monthly development budget in half. When the software outsourcing company, your partner, hears this definition of a deadline, it becomes a real partner of yours. Now its goals are aligned with yours. If the milestone is missed, you will suffer and they understand exactly how. Besides that, they see how your suffering will be transferred to their shoulders too. Stop asking them to finish everything ASAP. Stop calling them twice a day and yelling for an hour about their poor performance. Stop using language in business emails. Stop making all this noise. It doesn't help you anyway. Moreover, it only makes the situation worse, because you're losing respect and they're starting to treat you like a cash cow — a rather stupid and emotional one. Instead, do your homework and define your realistic milestones. Think about your real time, scope, and budget limitations. Write them down in very short and concise sentences. Make sure your constraints are realistic and their descriptions answer the main question — why. Why do you need this by the first of June? Why do you want to spend less than $50,000? Why do you need all five features to be in version 1.0? Why do you want your web app to be ready to handle 1K concurrent sessions? Why do you need a mobile app in the first release? Answer for yourself and make sure your answers are understood by the outsourcing company. Don't hide this information. The Product Is So Clumsy You want your web app to look like Pinterest, react fast, be easy to use, and make you proud when you show it to your friends. But the product they created for you is clumsy, slow, and to be honest, ugly. You're asking them to do something about it, and they keep giving you promises. The project keeps consuming your money and its budget grows, but the look and feel is not getting any better. It is far from Pinterest, very far. The frustration is growing, and you don't see any reasonable way out of this. The only advice you're getting from your friends is to re-do it all from scratch with a new web development team. How does this sound? I bet it's familiar. I believe the root cause of this dead-end situation is a fear of conflict. At early stages in the project, you try to do everything you can to keep a good relationship with the outsourcing company and not to offend anyone. You don't want to control anyone's work because they may take it as an insult. You don't want to express your quality concerns because they may de-motivate the team. You just hope they will improve the product in the future, but when the future comes, it's too late. Again, keeping the age-old principle in mind, I would recommend that from the first day of the project, you establish a routine procedure of checking their results and expressing your concerns. In our projects at Teamed.io , we ask our customers to be present in GitHub, review our releases frequently, and report any inconsistencies found as GitHub issues. We encourage project sponsors to be as pessimistic and negative about our quality from the beginning of the project. We realize this is how we can minimize the risk of a \"piled-up frustration\". Try to do the same in your project that is outsourced to an offshore developer. Don't be afraid to offend them. Iterative and incremental criticism is a much healthier approach than feedback-free peace that ends in war. Find a way to keep your outsourcing team aware of your opinion about its results on a regular basis. Don't try to be nice to save a project. You're doing yourself a bad favor. Instead, be open about your concerns. Remember the first part of the principle above — you should openly and frequently communicate your concerns. This is how you stabilize the project and minimize risks. Also, it's a very good practice, from time to time, to invite technical reviewers to generate independent opinions about the product under development. Read my other post about this subject: You Do Need Independent Technical Reviews! . I Can't Rely on Their Promises You call them, make plans, declare milestones, define features, set priorities, agree about quality, and then hang up. In a few days, you realize it was a total waste of time. They don't keep their promises because there is always something new happening. Someone is sick, some server is broken, some piece of software appears to be malfunctional, some code is no longer working, etc. You call again, express your frustration, make strong accusations, restructure milestones, redefine features, reset priorities, and in a few days start over. Been there, done that? Sound familiar? In my experience, this unpredictability and unreliability of a software outsourcing team is in most cases caused by a project sponsor himself or herself. This happens when you don't listen to them or they are afraid to tell you the truth, which is usually the same thing. Some call this \"fear-driven development\". The team is afraid of you, and in order to keep you on board as a paying customer, has to lie to you. Basically, they are telling you what you want to hear — that the end of the project is close, that currently open bugs are easy to fix, that performance problems are minor, that the quality of the architecture is outstanding, and that the team is very motivated to work with you. When you hear any of the above, question yourself — Do you encourage them to tell the truth? Do you reward them for bringing you bad but honest news? Once again citing the fundamental principle mentioned above, I would recommend you make sure your reasoning for awards and punishments is transparent to your software outsourcing partner and is based on project objectives, not your personal emotions. In one of my previous posts , I wrote that a happy customer is a false objective for a software development team. A customer who is promoting this objective is a terrible customer who is doomed to fail the project. If you reward your team when they make you happy with good news, you are training them to lie to you. If you expect them to deliver good news, you are discouraging them from telling you the truth and from doing what is good for the project, not for you personally. You're discouraging them from arguing with you. In other words, you're throttling the channel of information that is supposed to come to you from the people working for you. You're isolating yourself, and the team is starting to work against you, not with you. Here is a practical recommendation. First, regularly announce your reasonable objectives and constraints, like I explained above. Make sure the team understands your business plans and the \"why\" reasoning behind them. Second, regularly ask team members about risks and issues. Ask them why they think project objectives may be compromised. Even better, let them document risks regularly and report them back to you. Reward them for being honest in this list of risks. Try it and you will be surprised by how many interesting things that risk list will contain. "},{"title":"A Few Thoughts on Unit Test Scaffolding","url":"/2015/05/25/unit-test-scaffolding.html","tags":["oop","java"],"date":"2015-05-25 00:00:00 +0000","categories":["jcg"],"body":"When I start to repeat myself in unit test methods by creating the same objects and preparing the data to run the test, I feel disapointed in my design. Long test methods with a lot of code duplication just don't look right. To simplify and shorten them, there are basically two options, at least in Java: 1) private properties initialized through @Before and @BeforeClass , and 2) private static methods. They both look anti-OOP to me, and I think there is an alternative. Let me explain. Léon: The Professional by Luc Besson JUnit officially suggests a test fixture : public final class MetricsTest { private File temp ; private Folder folder ; @Before public void prepare () { this . temp = Files . createTempDirectory ( \"test\" ); this . folder = new DiscFolder ( this . temp ); this . folder . save ( \"first.txt\" , \"Hello, world!\" ); this . folder . save ( \"second.txt\" , \"Goodbye!\" ); } @After public void clean () { FileUtils . deleteDirectory ( this . temp ); } @Test public void calculatesTotalSize () { assertEquals ( 22 , new Metrics ( this . folder ). size ()); } @Test public void countsWordsInFiles () { assertEquals ( 4 , new Metrics ( this . folder ). wc ()); } } I think it's obvious what this test is doing. First, in prepare() , it creates a \"test fixture\" of type Folder . That is used in all three tests as an argument for the Metrics constructor. The real class being tested here is Metrics while this.folder is something we need in order to test it. What's wrong with this test? There is one serious issue: coupling between test methods. Test methods (and all tests in general) must be perfectly isolated from each other. This means that changing one test must not affect any others. In this example, that is not the case. When I want to change the countsWords() test, I have to change the internals of before() , which will affect the other method in the test \"class\". With all due respect to JUnit, the idea of creating test fixtures in @Before and @After is wrong, mostly because it encourages developers to couple test methods. Here is how we can improve our test and isolate test methods: public final class MetricsTest { @Test public void calculatesTotalSize () { final File dir = Files . createTempDirectory ( \"test-1\" ); final Folder folder = MetricsTest . folder ( dir , \"first.txt:Hello, world!\" , \"second.txt:Goodbye!\" ); try { assertEquals ( 22 , new Metrics ( folder ). size ()); } finally { FileUtils . deleteDirectory ( dir ); } } @Test public void countsWordsInFiles () { final File dir = Files . createTempDirectory ( \"test-2\" ); final Folder folder = MetricsTest . folder ( dir , \"alpha.txt:Three words here\" , \"beta.txt:two words\" \"gamma.txt:one!\" ); try { assertEquals ( 6 , new Metrics ( folder ). wc ()); } finally { FileUtils . deleteDirectory ( dir ); } } private static Folder folder ( File dir , String ... parts ) { Folder folder = new DiscFolder ( dir ); for ( final String part : parts ) { final String [] pair = part . split ( \":\" , 2 ); this . folder . save ( pair [ 0 ], pair [ 1 ]); } return folder ; } } Does it look better now? We're not there yet, but now our test methods are perfectly isolated. If I want to change one of them, I'm not going to affect the others because I pass all configuration parameters to a private static utility (!) method folder() . A utility method, huh? Yes, it smells . The main issue with this design, even though it is way better than the previous one, is that it doesn't prevent code duplication between test \"classes\". If I need a similar test fixture of type Folder in another test case, I will have to move this static method there. Or even worse, I will have to create a utility class. Yes, there is nothing worse in object-oriented programming than utility classes. A much better design would be to use \"fake\" objects instead of private static utilities. Here is how. First, we create a fake class and place it into src/main/java . This class can be used in tests and also in production code, if necessary ( Fk for \"fake\"): public final class FkFolder implements Folder , Closeable { private final File dir ; private final String [] parts ; public FkFolder ( String ... prts ) { this ( Files . createTempDirectory ( \"test-1\" ), parts ); } public FkFolder ( File file , String ... prts ) { this . dir = file ; this . parts = parts ; } @Override public Iterable < File > files () { final Folder folder = new DiscFolder ( this . dir ); for ( final String part : this . parts ) { final String [] pair = part . split ( \":\" , 2 ); folder . save ( pair [ 0 ], pair [ 1 ]); } return folder . files (); } @Override public void close () { FileUtils . deleteDirectory ( this . dir ); } } Here is how our test will look now: public final class MetricsTest { @Test public void calculatesTotalSize () { final String [] parts = { \"first.txt:Hello, world!\" , \"second.txt:Goodbye!\" }; try ( final Folder folder = new FkFolder ( parts )) { assertEquals ( 22 , new Metrics ( folder ). size ()); } } @Test public void countsWordsInFiles () { final String [] parts = { \"alpha.txt:Three words here\" , \"beta.txt:two words\" \"gamma.txt:one!\" }; try ( final Folder folder = new FkFolder ( parts )) { assertEquals ( 6 , new Metrics ( folder ). wc ()); } } } What do you think? Isn't it better than what JUnit offers? Isn't it more reusable and extendable than utility methods? To summarize, I believe scaffolding in unit testing must be done through fake objects that are shipped together with production code. "}]}