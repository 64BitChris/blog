{"entries":[{"title":"About Programming","url":"/index.html","tags":[],"date":null,"categories":[],"body":" page of "},{"title":"Unsubscribe","url":"/unsubscribe.html","tags":[],"date":null,"categories":[],"body":"I'm sorry to see you leaving :( Your email Unsubscribe You can always subscribe again. "},{"title":"Puzzle Driven Development","url":"/2009/03/04/pdd.html","tags":["pdd"],"date":"2009-03-04 00:00:00 +0000","categories":[],"body":"PDD, or Puzzle Driven Development, is a method used to break down programming tasks into smaller ones and enable their implementation in parallel. The PDD method is used widely in XDSD methodology. The method is pending a USPTO patent (application no. 12/840,306 ). Let's review the method with an example. Say, for instance, you are a programmer and have been tasked to design and implement a Java class. This is the formal task description: \"class DBWriter has to extend java.io.Writer abstract class and save all incoming data into the database\". You have one hour to implement this task. It is obvious to you that one hour is not enough because the problem is much bigger and requires more work than the slotted time allows. Additionally, there are a numerous unknowns: What information do we need to save, and in what format? What is the DB schema? Is it an SQL or NoSQL database? How to connect to the DB? JDBC? JPA? DAO? How to handle exceptions? Let's keep all these unknowns in mind as we try to solve the problem on the highest level of abstraction. Of course, we start with a test: import org.junit.* ; import static org . mockito . Mockito .*; public class DBWriterTest { @Test void testSavesDataIntoDatabase () throws Exception { DataBridge mapper = mock ( DataBridge . class ); Writer writer = new DBWriter ( mapper ); try { writer . write ( \"hello, world!\" ); } finally { writer . close (); } verify ( mapper ). insert ( \"hello, world!\" ); } } In the above test, we define the expected behavior of the class. The test fails to compile because there are two missing classes: DataBridge and DBWriter . Let's implement the bridge first: import java.io.IOException ; public interface DataBridge { void insert ( String text ) throws IOException ; } Next, the writer itself: import java.io.IOException ; import java.io.Writer ; import java.utils.Arrays ; public class DBWriter implements Writer { private DataBridge bridge ; public DBWriter ( DataBridge brdg ) { this . bridge = brdg ; } @Override void flush () throws IOException { } @Override void close () throws IOException { } @Override void write ( char [] cbuf , int off , int len ) throws IOException { String data = new String ( Arrays . copyOfRange ( cbuf , off , off + len )); this . bridge . insert ( data ); } } Using the above code, we solve the problem. In the example, we successfully designed, implemented and tested the required DBWriter class. Subsequently, the class can now immediately can be used \"as is\" by other classes. Of course, the implementation is not finished, since we are not writing anything to the database. Furthermore, we still aren't answering the majority of questions asked in the sample scneario. For instance, we still don't know exactly how the database needs to be connected, its type (SQL or NoSQL,) the correct data format and so on. However, we've already made a number of important architectural assumptions, which allowed us to implement the class and make it usable by other classes. Now it's time to identify the unknowns in our code and mark them with puzzles. Every puzzle is a request for refinement. We want to ask someone else to help us refine and correct our assumptions. Here is the first puzzle we need to add: public interface DataBridge { /** * @todo #123 I assumed that a simple insert() method will be * enough to insert data into the database. Maybe it's * not true, and some sort of transaction support may be * required. We should implement this interface and create * an integration test with a database. */ void insert ( String text ) throws IOException ; } The puzzle has three elements: @todo tag, #123 locator and a comment. Locator displays the following: \"The puzzle was created while working with ticket #123\". Let’s add one more puzzle: void write ( char [] cbuf , int off , int len ) throws IOException { // @todo #123 I assumed that the data should be sent to the database // as its received by the writer. Maybe this assumption // is wrong and we should aggregate data into blocks/chunks // and then send them to the data bridge. String data = new String ( Arrays . copyOfRange ( cbuf , off , off + len )); this . bridge . insert ( data ); } This puzzle indicates one of our concerns because we are not sure that the architectural decision is right. Actually, the design is very primitive at the moment and very likely to be incorrect. To refine it and refactor, we require more information from the task specifier. The task is finished. Now, you can reintegrate your branch into master and return the ticket to whoever assigned it to you. His task now is to find other people who will be able to resolve the puzzles we just created. Every puzzle created now will produce other puzzles, which will be resolved by other people. Consequtly, our simple one-hour task can potentially generate hundreds of other tasks, which may take days or even years to complete. Nevertheless, your goal of working with your specific task is to finish it as soon as possible and reintegrate your branch into master . Best Practices There are a few simple rules that help you to place puzzles correctly. First, you should put your @todo annotations at the point where your code hits a stub. For example, in a unit test. You're implementing a test and it fails because the class has not yet been implemented. You skip the test with the @Ignore annotation and add a @todo puzzle to its Javadoc. Second, your puzzle should remain as near as possible to the code element that is hitting the stub. Say thay you have a unit test that has three test methods. All of them fail now because the class has not been implemented. The best approach would be to ignore every one of them and create three (!) puzzles. Each one of the puzzles should explain what you expect from the class and how it should be implemented. Third, be as descriptive as possible. Your puzzle will soon be a task definition for someone else. So, explain clearly what you expect the next person to implement, how to do it, which documentation to use and so on and so forth. There should be enough information present that the next person assigned to the puzzles is ables implement your required classes without additional input from you! BTW, puzzle collection process can be automated by means of our PDD Ruby gem . "},{"title":"D29, a prototype","url":"/2013/12/29/proto.html","tags":["programming"],"date":"2013-12-29 00:00:00 +0000","categories":[],"body":"D29 is a prototype of a new programming language and a development platform. Well, actually, not a prototype yet, but just an idea. As it looks to me, the languages we have now (even the most modern ones) are still close to COBOL/C and far from being truely elegant and modern. Would be great if we can design a language/platform, which will be a mix of object oriented programming and functional programming, and will have all the features listed below, out-of-the-box. Key principles: everything is an object byte and bytes are the only built-in types strict compile-time static analysis Native support of: UML and OCL build automation aspect oriented programming continuous integration and delivery deployment , incl. staged one collective code ownership revision control test driven development integration testing mocking serialization (to XML, JSON, binary) documentation preferably with CNL requirements traceability components repository (a la Sonatype Nexus) RAII tracing (aka logging) internationalization and localization generic programming software licenses multithreading deep objects immutability manual testing performance testing (and others) assertions versioning class invariants design patterns (e.g. Adapter , Bridge and Decorator ) object cloning object persistence authentication and authorization ACID transactions distributed objects and their horizontal scalability asynchronous methods backward compatibility of runtime platforms object metadata, like life time, ownership, etc. Maybe native support of: cloud computing Features: no mutable objects ( why? ) no public/protected object properties no static properties/methods ( why? ) no global variables no pointers no NULL ( why? ) no scalar types, like int , float , etc. no unchecked exceptions no interface-less classes multiple inheritance (!) no operator overloading all methods are either final or abstract no mutability of method arguments no reflection no instanceof operator no root class (like, for example, Object in Java) instant object destruction instead of garbage collection Maybe: native support of Java classes/libraries compilation into Java byte code If interested to contribute, email me. Maybe we'll do something together :) "},{"title":"First Post","url":"/2014/04/06/introduction.html","tags":[],"date":"2014-04-06 00:00:00 +0000","categories":[],"body":"This is the first post on my new blog. Therefore, it's not about anything in particular - just an introduction and my way of saying hello. This blog will be primarily about software development ideas. As my About Me page says, I'm passionate about software quality, and will write solely about my ideas and views on it. Anyway, welcome to my new blog. Together, let's see how this works out! :) ps. BTW, I purchased the Cambria font just for this new blog. It cost, €98. Nevertheless, I think its a good investment for this new venture. "},{"title":"Movies for Thanasis","url":"/2014/04/06/movies-for-thanasis.html","tags":["movies"],"date":"2014-04-06 00:00:00 +0000","categories":[],"body":"Sometime ago, I recommended a list of movies to a friend of mine after he told me was losing all interest in \"Hollywood.\" Level C titles are supposed to be impossible to understand unless you've seen (and understood) their prequels -- listed in sections A and B. So, start browsing the lists in sections A and post your comments if you have any. :) Level A True Romance (1993) Pulp Fiction (1994) Kill Bill (2003) Doberman (1997) La fille sur le pont (1999) Irreversible (2002) Fear and Loathing in Las Vegas (1998) Perdita Durango (1997) Golden Balls (1993) The Million Dollar Hotel (2000) Y Tu Mamá También (2001) Reservoir Dogs (1992) Trainspotting (1996) Fight Club (1999) Arizona Dream (1992) Black Cat, White Cat (1998) Buffalo '66 (1998) Jamon Jamon (1992) Natural Born Killers (1994) Thursday (1998) Bullet (1996) Level B Delicatessen (1991) Bernie (1996) Hardmen (1996) Revolver (2005) Science of Sleep (2006) Cashback (2006) El Crimen Perfecto (2004) El día de la bestia (1995) La comunidad (2000) The Happiness of the Katakuris (2001) 99 francs (2007) Combien tu m'aimes? (2005) Ying xiong (2002) Brutti, sporchi e cattivi (1976) Level C What Just Happened (2008) Happiness (1998) Before the Devil Knows You Are Dead (2007) No Country for Old Men (2007) A Serious Man (2009) "},{"title":"PhantomJS as an HTML Validator","url":"/2014/04/06/phandom.html","tags":["phandom","phantomjs","testing"],"date":"2014-04-06 00:00:00 +0000","categories":[],"body":" I created phandom.org a few months ago, but yesterday finally found the time to make some needed changes to it. So, now is a good time to explain how I'm using Phandom in some of my unit tests. Before I get started, though, I should say a few words about phantomjs , which is a JavaScript interface for WebKit. WebKit, on the other hand, is a web browser without a user interface. WebKit is a C++ library that enables manipulation of HTML content, through DOM calls. For example, this is a simple JavaScript located code in example.js : 1 2 3 4 5 6 7 8 var page = require ( 'webpage' ). create (); page . open ( 'http://google.com' , function () { console . log ( 'loaded!' ); phantom . exit ( 0 ); } ); We run phantomjs from the command line with the following code: $ phantomjs example.js Phantomjs creates a page object (provided by webpage module inside phantomjs), and then asks it to open() a Web page. The object communicates with WebKit and converts this call into DOM instructions. After which, the page loads. The phantomjs engine then terminates on line 6. WebKit renders a web page with all neccessary components such as CSS, JavaScript, ActionScript, etc, just as any standard Web browser would. So far so good, and this is the traditional way of using phantomjs. Now, on to giving you an idea of how Phandom (which stands for \"PhantomJS DOM\") works inside Java unit tests: To test this, let's give phantomjs an HTML page and ask him to render it. When the page is ready, we'll ask phantomjs to show us how this HTML looks in WebKit. If we see the elements we need and desire, — we're good to go. Let's use the following example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import com.rexsl.test.XhtmlMatchers ; import org.hamcrest.MatcherAssert ; import org.phandom.Phandom ; public class DocumentTest { @Test public void rendersValidHtml () { Document doc = new Document (); // This is the method we're testing. It is supposed // to return a valid HTML without broken JavaScript // and with all required HTML elements. String html = doc . html (); MatcherAssert . assertThat ( XhtmlMatchers . xhtml ( new Phandom ( html ). dom ()), XhtmlMatchers . hasXPath ( \"//p[.='Hello, world!']\" ) ); } } When we use the above code, here is what happens. First, we get HTML html as a String from doc object, and then pass it to Phandom as an argument. Then, on line 13, we call the Phandom.dom() method to get an instance of the class org.w3c.dom.Document . If our HTML contains any broken JavaScript code, method dom() produces a runtime exception and the unit test faila. If HTML is clean and WebKit is able to render it without problems, the test passes. I'm using this mechanism in a few different projects,and it works quite well. Therefore, I highly recommend it. Of course, you shouldn't forget that you must have phantomjs installed on your build machine. In order to avoid unit test failures when phantomjs is not available or present, I've created the following supplementary method: public class DocumentTest { @Test public void rendersValidHtml () { Assume . assumeTrue ( Phandom . installed ()); // the rest of the unit test method body... } } Enjoy and feel free to report any bugs or problems you encounter to: Github issues :) "},{"title":"Xembly, an Assembly for XML","url":"/2014/04/09/xembly-intro.html","tags":["xembly","xml","xsl","xsd"],"date":"2014-04-09 00:00:00 +0000","categories":[],"body":" I use XML in almost every one of my projects. And, despite all the fuss about JSON/YAML, I honestly believe that XML is one of the greatest languages ever invented. Also, I believe that the beauty of XML reveals itself when used in combination with related technologies. For example, you can expose your data in XML and render it for the end-user using XSL stylesheet . Another example would be when you validate the same data, before rendering, to ensure that the structure is correct. You can do this with the XSD schema. Alternatively, you can pick specific data elements from the entire document by using XPath queries. Essentially, these three technologies, XSL, XSD schema and XPath, are what makes XML so powerful. However, there can be times when XML falls short. For instance, imagine you have an existing document that needs to be modified just slightly. For example, let's use the following: <accounts> [...] <acc id= '34' > <name> Jeffrey </name> <balance> 305 </balance> </acc> <acc id= '35' > <name> Walter </name> <balance> 50090 </balance> </acc> [...] </accounts> The above code represents a list of accounts. Each account has its own id and several child elements. In our example, we need to find the account belonging to Jeffrey and increase its balance by 500 . How would we do this? Well, there are a few possible solutions: SAX-parse the document, change the balance and save the stream; DOM-parse it, find the element with XPath, change the value and then print it; apply a parametrized XSL stylesheet; apply XQuery small script to make changes All of these methods have their own drawbacks. However, all of them have one particular problem in common — they are very verbose. With each of the above methods, you need at least a page of code to perform this rather simple operation. Furthermore, if the logic of the operation becomes more complex, the amount of needed code grows much faster than you may expect. Simply put, XML lacks a tool for primitive data manipulations within a document. Perhaps, it is this shortcoming that makes XML unpopular with some. Anyway, here is a tool I created a few month ago: Xembly . It is an imperative language with a few simple directives and resembles Assembly in style. Thus, the name - Xembly. With Xembly, there are no loops, conditions or variables - just a sequence of directives with arguments. Let's create a simple example. Say, for instance, we want to add a new account number 36 to our list document. The code would look like: 1 2 3 4 5 6 7 8 XPATH '/ accounts '; ADD ' account '; ATTR ' id ' , ' 36 '; ADD ' name '; SET ' Donny '; UP ; ADD ' balance '; SET ' 3400 '; The above should be intuitively clear, but I'll explain just in case. First, the XPATH directive points us to the element found by the \"/accounts\" XPath query. This will be our root element. We assume here that it exists in the document. Therefore, if it is absent, our Xembly script will fail with a runtime exception. Next, the ADD directive on line 2 creates a new XML element without any children or attributes. Then, the ATTR directive sets an attribute for this element. The code then adds the new child element name and sets its text value to \"Donny\" using the SET directive. Finally, we move our pointer back to account element using UP , add the balance child element and set its value to \"3400\" . Our balance changing task can be expressed in Xembly with the following code: XPATH '/ accounts / account [ name =\" Jeffrey \"]/ balance '; XSET ' . + 500 '; The XSET directive sets the element text value, similar to SET , but calculates it beforehand using the provided XPath expression . + 500 . Xembly performs all manipulations through DOM. Consequently, Xembly can be implemented inside any language that has a built-in DOM implementation. In the meantime, there is only one implementation of Xembly language — in Java. Here is how it works: 1 2 3 4 5 6 7 Iterable < Directive > directives = new Directives () . xpath ( \"/accounts\" ) . add ( \"account\" ) . attr ( \"id\" , \"36\" ) . add ( \"name\" ). set ( \"Donny\" ). up () . add ( \"balance\" ). set ( \"3400\" ); new Xembler ( directives ). apply ( document ); In this snippet, I'm using a supplementary script builder, Directives , which enables generation of directives in a fluent way. Then, I use Xembler class, which is similar to \"assembler\", to apply all specified directives to the document object of class org.w3c.dom.Document . Additionally, Xembly can be used to build XML documents from scratch and as a replacement for traditional DOM building. A quick example: System . out . println ( new Xembler ( new Directives (). add ( \"html\" ) . add ( \"head\" ) . add ( \"title\" ) . set ( \"Hello, world!\" ) ). xml () ); The above snippet produces the following output: <html> <head> <title> Hello, world! </title> </head> </html> For me, this appears to be more simple and compact. As usual, your bug reports and suggestions are always welcomed. Please send to Github issues :) "},{"title":"How much do you pay per line of code?","url":"/2014/04/11/cost-of-loc.html","tags":["xdsd","mgmt"],"date":"2014-04-11 00:00:00 +0000","categories":["jcg"],"body":" Yes, I know, \"line of code\" (LoC) is a very wrong metric . There are tons of articles written about it, as well as famous books. However, I want to compare two projects in which I have participated recently and discuss some very interesting numbers. Project #1: Traditionally Co-located The first project I was apart of was performed by a traditionally co-located group of programmers. There were about 20 of them (I'm not counting managers, analysts, product owners, SCRUM masters, etc.) The project was a web auctioning site with pretty high traffic numbers (over two million page views per day). The code base size was about 200k lines, of which 150k was PHP, 35k JavaScript and the remainder CSS, XML, Ruby, and something else. I'm counting only non-empty and non-comment lines of code, using cloc.pl . It was a commercial project, so I can't disclose its name. Brazil (1985) by Terry Gilliam The team was co-located in one office in Europe where everybody was working \"from nine 'til five\". We had meetings, lunches, desk-to-desk chats and lots of other informal communications. All tasks were tracked in JIRA. Project #2: Extremely Distributed The second project was an open source Java product, developed by an extremely distributed team of about 15 developers. We didn't have any chats or any other informal communications. We discussed everything in Github issues. The code base was significantly smaller with only about 30k lines, of which about 90% was Java and the rest in XML. Shaolin Temple (1982) by Chang Hsin Yen Maturity of Development Both projects hosted their code bases on Github. Both teams were developing in feature branches - even for small fixes. Both teams used build automation, continuous integration, pre-flight builds, static analysis and code reviews. This indicates the maturity of the project teams. Both projects satisfied the requirements of their users. I'm mentioning this to emphasize that both projects produced valuable and useful lines of code. There was no garbage and almost no code duplication . Show Me the Money In both projects, my role was called that of lead architect, and I know their economics and financials. Besides that, I had access to both Git repositories, so I can measure how many new lines (or changed lines) were introduced by both teams in, say, a three-month period. Now, let's see the numbers. The first project (the one that was co-located) was paying approximately €50,000 annually to a good developer, which was about $5,600 per month or $35 per hour. The second one (the extremely distributed project) was paying $20-35 per hour, for completed tasks only according to one of the principles of XDSD . The first one, in three months, produced 59k new lines and removed 29k in changes in the master branch, which in totals 88k lines of code. The project resulted in about 10,000 man hours to produce these lines (20 programmers, three months, 170 working hours per month) — which equates to about $350k. Therefore, the project cost a whopping $3.98 per line The second project, in the same three month period, produced 45k new lines and removed 9k, which comes to 54k in all. To complete this work, we spent only $7k (approximately 350 working hours in 650 tasks). Thus, the project costs merely: ¢13 per line This also means that programmers were writing approximately 150 lines per hour or over a thousand per day. The Mythical Man-Month talks about 10 lines per day, which is a hundred times less than we saw in our project. $350k vs $7k, $3.98 vs ¢13? What do you think? How to Validate the Numbers? If you're curios, I'm using hoc to get the numbers from Git (it is explained in Hits-of-Code Instead of SLoC ). You can validate the numbers for the second project here on Github: jcabi/jcabi-github . Conclusion What I'm trying to express with these numbers is that distributed programming is much more effective, money-wise, than a co-located team. Again, I can hear you saying that \"line of code\" is not a proper metric. But, come on, $0.13 vs. $3.98? Thirty times more expensive? The Big Lebowski (1998) by Joel Coen It's not about metrics any more. It's about preventing wasteful man hours and the huge waste of money that comes with them? Can We Do the Same? Of course, the same results can't be achieved by just telling your programmers to work from home and never come to the office. XDSD is not about that. XDSD is about strict quality principles, which should be followed by the entire team. And when these principles are in place — you pay thirty times less. By the way, this is what people say about their projects: $12–103: crazyontap.com $15–40: betterembsw.blogspot.nl over $5: joelonsoftware.com What are your numbers? Please post your comments below. "},{"title":"Fluent Java HTTP Client","url":"/2014/04/11/jcabi-http-intro.html","tags":["jcabi","http","java"],"date":"2014-04-11 00:00:00 +0000","categories":[],"body":" In the world of Java, there are plenty of HTTP clients from which to choose. Nevertheless, I decided to create a new one because none of the other clients satisfied fully all of my requirements. Maybe, I'm too demanding. Still, this is how my jcabi-http client interacts when you make an HTTP request and expect a successful HTML page in return: 1 2 3 4 5 6 7 8 String html = new JdkRequest ( \"https://www.google.com\" ) . uri (). path ( \"/users\" ). queryParam ( \"id\" , 333 ). back () . method ( Request . GET ) . header ( \"Accept\" , \"text/html\" ) . fetch () . as ( RestResponse . class ) . assertStatus ( HttpURLConnection . HTTP_OK ) . body (); I designed this new client with the following requirements in mind: Simplicity For me, this was the most important requirement. The client must be simple and easy to use. In most cases, I need only to make an HTTP request and parse the JSON response to return a value. For example, this is how I use the new client to return a current EUR rate: 1 2 3 4 5 6 String rate = new JdkRequest ( \"http://www.getexchangerates.com/api/latest.json\" ) . header ( \"Accept\" , \"application/json\" ) . fetch () . as ( JsonResponse . class ) . json (). readArray (). getJsonObject ( 0 ) . getString ( \"EUR\" ); I assume that the above is easy to understand and maintain. Fluent Interface The new client has to be fluent, which means that the entire server interaction fits into one Java statement. Why is this important? I think that fluent interface is the most compact and expressive way to perform multiple imperative calls. To my knowledge, none of the existing libraries enable this type of fluency. Testable and Extendable I'm a big fan of interfaces, mostly because they make your designs both cleaner and highly extendable at the same time. In jcabi-http , there are five interfaces extended by 20 classes. Request is an interface, as well as Response , RequestURI , and RequestBody exposed by it. Use of interfaces makes the library highly extendable. For example, we have JdkRequest and ApacheRequest , which make actual HTTP calls to the server using two completely different technologies: (JDK HttpURLConnection and Apache Http Client, respectively). In the future, it will be possible to introduce new implementations without breaking existing code. Say, for instance, I want to fetch a page and then do something with it. These two calls perform the task differently, but the end results are the same: String uri = \"http://www.google.com\" ; Response page ; page = new JdkRequest ( uri ). fetch (); page = new ApacheRequest ( uri ). fetch (); XML and JSON Out-of-the-Box There are two common standards that I wanted the library to support right out of the box. In most cases, the response retrieved from a server is in either XML or JSON format. It has always been a hassle, and extra work, for me to parse the output to take care of formatting issues. jcabi-http client supports them both out of the box, and it's possible to add more formats in the future as needed. For example, you can fetch XML and retrieve a string value from its element: String name = new JdkRequest ( \"http://my-api.example.com\" ) . header ( \"Accept\" , \"text/xml\" ) . fetch () . as ( XmlResponse . class ) . xml (). xpath ( \"/root/name/text()\" ). get ( 0 ); Basically, the response produced by fetch() is decorated by XmlResponse . This then exposes the xml() method that returns an instance of the XML interface. The same can be done with JSON through the Java JSON API ( JSR-353 ). None of the libraries that I'm aware of or worked with offer this feature. Immutable The last requirement, but certainly not the least important, is that I need all interfaces of the library to be annotated with @Immutable . This is important because I need to be able to encapsulate an instance of Request in other immutable classes. ps. A short summary of this article was published at JavaLobby "},{"title":"PDD by Roles","url":"/2014/04/12/puzzle-driven-development-by-roles.html","tags":["xdsd","pdd","mgmt"],"date":"2014-04-12 00:00:00 +0000","categories":[],"body":" In this post, I'll try to walk you through a project managed with the spirit of Puzzle Driven Development (PDD). As I do this, I will attempt to convey typical points of view of various project members. Basically, there are six key roles in any software team: Project Manager — assigns tasks and pays on completion System Analyst — documents the product owner's ideas Architect — defines how system components interact Designer — implements most complex components Programmer — implements all components Tester — finds and reports bugs Everybody, except the project manager, affects the project in two ways: they fix it and they break it at the same time. Let me explain this with a simple example. Fix and Break Let's assume, for the sake of simplicity, that a project is a simple software tool written by me for a close friend. I created the first draft version 0.0.1 and delivered it to him. For me, the project is done. I've completed the work, and hopefully will never have to return to it again. However, the reality of the project is very different. In just a few hours, I receive a call from my friend saying that a he's found a few bugs in the tool. He is asking me to fix them. Now, I can see that the project is not done. In fact, it's broken. It has a few bugs in it, which means a few tasks to complete. I'm going to fix the project, by removing the bugs. I implement a new version of the software, name it 0.0.2 and ship it to my friend. Again, I believe my project is finished. It is fixed and should be closed. This scenario repeats itself again and again until my friend stops calling me. In other words, until he stops breaking my project. It is obvious that the more my friend breaks my project, the higher the quality of the software delivered ultimately at the end. Version 0.0.1 was just a very preliminary version, although I considered it final at the time I released it. In a few months, after I learn of and fix hundreds of bugs, version 3.5.17 will be much more mature and stable. This is the result of this \"fix and break\" approach. The diagram shows the relation between time and mess in the project. The bugs my friend is reporting to me are breaking the project, increasing its instability (or simply its messiness). New versions I release resolve the bugs and are fixing the project. Your Github commit dynamics should resemble this graph, for example: When the project starts, its messiness is rather low, and then it starts to grow. The messiness then reaches its peak and starts to go down. Project Manager The job of a project manager is to do as much as possible to fix the project. He has to use the sponsor's time and money in order to remove all bugs and inconsistencies and return the project back to a \"fixed\" state. Pulp Fiction (1994) by Quentin Tarantino When I say \"bugs,\" I mean more than just software errors but also: unclear or ambiguous requirements features not yet implemented functional and non-functional bugs lack of test coverage unresolved @todo markers lack of risk analysis etc. The project manager gives me tasks that he wants done in order to fix and stabilize the project to return it back to a bug-free state. My job, as a member of a software team, is to help him perform the needed fixes and, at the same time, do my best to break the project! In the example with my friend, he was breaking the project constantly by reporting bugs to me. This is how he helped both of us increase the final quality of the product. I should do the same and always try to report new bugs when I'm working on some feature. I should fix and break at the same time. Now let's take a closer look at project roles. System Analyst A product owner submits an informal feature request, which usually starts with \"it would be nice to have...\" I'm a system analyst and my job is to translate owner's English into formal specifications in the SRS, understandable both by programmers and myself. It's not my responsibility to implement the feature. Arizona Dream (1992) by Emir Kusturica My task is complete when a new version of the SRS is signed by the Change Control Board. I'm an interpreter for the product owners, translating from their language to formal language needed in the SRS document. My only customer is the product owner. As soon as she closes the feature request, I'll be paid. Besides feature requests from product owners, I often receive complaints about the quality of the SRS. The document may not be clear enough for some team members. Therefore, it's my job to resolve clarity problems and fix the SRS. These team members are also my customers. When they close their bug reports, I'll be paid. In both cases (a feature request or a bug,) I can make changes to the SRS immediately - if I have enough time. However, it's not always possible. I can submit a bug and wait for its resolution; but, I don't want to keep my customers waiting. This is where puzzle driven development helps me. Instead of submitting bug reports, I add \" TBD \" puzzles in the SRS document. The puzzles are informal replacements of normally very strict formal requirements. They satisfy my customer, since they are in plain English, and are understandable by technical people. Thus, when I don't have time, I don't wait. I change the SRS using TBDs at points where I can't create a proper and formal description of the requirements or simply don't know what to write exactly. Architect Now, I'm the architect, and my task is to implement a requirement, which has been formally specified in the SRS. PM is expecting a working feature from me, which I can deliver only when the architecture is clear and classes have been designed and implemented. The Science of Sleep (2006) by Michel Gondry Being an architect, I'm responsible for assembling all of the components together and making sure they fit. In most cases, I'm not creating them myself, but I'm telling everybody how they should be created. My work flow of artifacts is the following: directed graph digraph G { SRS -> UML; UML -> \"Source code\"; } G SRS SRS UML UML SRS->UML Source code Source code UML->Source code I receive requirements from the SRS, produce UML diagrams and explain to designers how to create source code according to my diagrams. I don't really care how source code is implemented. I'm more concerned with the interaction of components and how well the entire architecture satisfies functional and non-functional (!) requirements. My task will be closed and paid when the system analyst changes its state to \"implemented\" in the SRS. The system analyst is my only customer. I have to sell my solution to him. Project manager will close my task and pay me when system analyst changes the status of the functional requirement from \"specified\" to \"implemented\". The task sounds big, and I have only half an hour. Obviously, puzzle driven development should help me. I will create many tickets and puzzles. For example: SRS doesn't explain requirements properly Non-functional requirements are not clear UML diagrams are not clear enough Components are not implemented Build is not automated Continuous integration is not configured Quality of code is not under control Performance testing is not automated When all of my puzzles are resolved, I can get back to my main task and finish feature implementation. Obviously, this may take a long time - days or even weeks. But, the time cost of the main task is less than an hour. What is the point of all this hard work? Well, it's simple; I'll earn my hours from all the bugs reported. From this small half-an-hour task, I will generate many tickets, and every one of them will give me extra cash. Designer and Programmer The only real differences between designer and programmer are the complexity of their respective tasks and the hourly rates they receive. Designers usually do more complex and higher level implementations, while programmers implement all low-level details. Pulp Fiction (1994) by Quentin Tarantino I'm a programmer and my task is to implement a class or method or to fix some functional bug. In most cases, I have only half an hour available. And, most tasks are bigger and require more time than that. Puzzle driven development helps me break my task into smaller sub-tasks. I always start with a unit test. In the unit test, I'm trying to reproduce a bug or model the feature. When my test fails, I commit it and determine the amount of time I have left. If I still have time to make it pass — I do it, commit the changes and report to the project manager. If I don't have time to implement the fix, I mark pieces of code that don't already have @todo markers, commit them and report to the project manager that I've finished. As you see, I'm fixing the code and breaking it at the same time. I'm fixing it with my new unit test, but breaking it with @todo puzzles. This is how I help to increase the overall quality of the project - by fixing and breaking at the same time. Tester I'm a tester and my primary motivation is to find bugs. This may be contradictory to what you've heard before; but in XDSD , we plan to find a certain amount of bugs at every stage of the project. Fear and Loathing in Las Vegas (1998) by Terry Gilliam As as a tester, I receive tasks from my project manager. These tasks usually resemble \"review feature X and find 10 bugs in it\". The project manager needs a certain number of bugs to be found in order to fix the project. From his point of view, the project is fixed when, say, 200 bugs have been found. That's why he asks me to find more. Thus, to respond to the request, i find bugs to do my part in regard to the \"fixing\" part of the bigger picture. At the same time, though, I can find defects on my own and report them. This is the \"breaking\" part of my mission. "},{"title":"Bugs Are Welcome","url":"/2014/04/13/bugs-are-welcome.html","tags":["testing","xdsd","mgmt"],"date":"2014-04-13 00:00:00 +0000","categories":[],"body":"The traditional understanding of a software defect (aka \"bug\") is that it is something negative and want to avoid in our projects. We want our projects to be \"bug-free.\" Our customers are asking us to develop software that doesn't have bugs. And, we, as users, expect software to work without bugs. Charlie and the Chocolate Factory (2005) by Tim Burton But, let's take a look at bugs from a different angle. In XDSD , we say that \"bugs are welcome.\" This means we encourage all interested parties to find bugs and report them. We want our team to see bugs as something that we need in our projects. Why? Because we understand that there are two categories of bugs: visible and hidden. The more bugs that become visible, the more of them we can fix. More fixed bugs means fewer to annoy our users. By discovering bugs we make them visible. This is the primary job of a software tester — to make bugs visible. Obviously, their visibility affects the quality of the product in a positive way. This is because we can fix them before our users start complaining. In order to motivate all team members to make more bugs visible, we pay for their discovery. In XDSD projects, we are pay 15 minutes for every bug found (no matter who finds them and where.) We Plan Bugs We go even further. At XDSD , we plan for a number of hidden bugs in every project. We do this by using our experience with previous projects and expert judgment. Let's say we're starting to develop a web system, which is similar to the one we worked on last year. We know that in the previous project our users and team together reported 500 bugs. It's logical to assume that the new project will have a similar number of bugs. Thus, our task is to make those 500 bugs visible before they hit the production platform and our users call us to complain about them. Therefore, we're making it one of the project goals: \"discover 500 bugs.\" Of course, our estimate may be wrong. Nevertheless, we have historical records for a few dozen projects, and in all of them the number is close to 500. So, finding 500 bugs in a project is usually a reality — we can use it as a target. What Is a Bug? Let us try to define a bug (or software defect) in a non-ambiguous manner. Something can be reported as a bug and subsequently paid for iff: it is reproducible it refers to functionality already implemented is can be fixed in a reasonable amount of time it doesn't duplicate a bug already reported Reproducibility of a bug is very important. Consequently, it is the responsibility of a bug reporter to make sure the bug is reproducible. Until it is proven that the bug can be reproduced — it's not a bug for which payment can be made. A bug is not a task; it has to refer to an existing functionality. Additionally, an explanation must exist for how and when the existing functionality doesn't work as expected. "},{"title":"No Obligations","url":"/2014/04/13/no-obligations-principle.html","tags":["xdsd","mgmt"],"date":"2014-04-13 00:00:00 +0000","categories":[],"body":" It is a very common problem in project management — how to make team members more responsible and avoid micro management ? We start with creating plans, drawing Gantt charts, announcing milestones, motivating everybody and promising big bonuses on success. Excuses Then everybody begins working and we start hearing excuses: \"The task is not yet ready. I was doing something else\" \"May I take a day off? Tomorrow is my birthday?\" \"May I skip the unit test because I don't know how to fix it?\" \"I don't know how to do it, can someone help me?\" \"I tried, but this doesn't work; what can I do?\" \"This installation requires all of my time. I can't finish the task\" With excuses, team members transfer responsibility back to the project manager. There was a very famous article \"Management Time: Who's Got the Monkey?\" published in the Harvard Business Review about this very subject. I recommend that you read it. Its authors present problems as monkeys sitting on our shoulders. When the project manager assigns a task to a programmer — he moves the monkey from his shoulders to the programmer's shoulders. The programmer usually presents the excuse \"I don't know what to do\". Now the monkey is back on the shoulders of the managers. The goal of the manager is to send the monkey back to make it the programmer's problem again. One of traditional way of transferring responsibility back to team members is to become an aggressive manager. For instance the manager may say, \"You have a birthday tomorrow? I don't care, you still have to meet your deadline\" or \"You don't know how to fix the unit test? Not my problem, it should be fixed by tomorrow,\" etc. We've all seen multiple examples of that type of aggressive management. Personally, I find this management style extremely annoying and destructive for the project. The project environment becomes very unhealthy and good people usually end up leaving. Another traditional management method is micro-management. This results when the project manager checks task statuses every few hours and tells people what to do and how to handle problems. Needless to say, this management style ruins the team and causes good people to leave even faster. However, in order to keep the project on track and meet all milestones, responsibility must be on the shoulders of the team members. They should be responsible for their own tasks and report back to the project manager when they are finished with their jobs. The Big Lebowski (1998) by Joel Coen Implementation problems should be solved by team members on their own. So, how do we accomplish this in XDSD ? I Owe You Nothing In XDSD , there is the first fundamental principle that says everybody should be paid for deliverables. Based on this idea, we can go even further and declare a \"No Obligations\" principle. In essence, for every team member, it says: if you don’t like the task assigned to you, don’t have time or you’re simply not in the mood — don't do it. You have no obligation to do anything. You're free to reject every second task that a project manager gives to you or even all of them. On the other hand, though, the project manager is not obliged to keep a task assigned to you for longer than 10 days (we think that this time frame is logical). If you get a task, and don't deliver within ten days, the project manager can take it away and pay you nothing — no matter how much time you invested in the task already or the reasons for your failure to complete it. Where Are The Monkeys Now? This principle helps us to separate responsibilities between project manager and team members. The manager is responsible for finding the right people and assigning them appropriate tasks. There is a problem with the project manager's management style if he receives too many rejections from the team. On the other hand, his team members are responsible for their tasks and should not provide excuses for non-completion. Well, team members can make excuses, but they won't change anything. No matter what their excuses are, the deliverables will be purchased only from members who manage to complete their tasks on time. How Does This Affect Me? When you're working with XDSD -inspired project, you should always keep the \"No Obligations\" principle in mind. You should start a task only if you're sure that you can finish it in a few days. You should pursue your tasks and control deadlines yourself. The project manager will not ask you for status updates, as usually happens with traditional projects. He will just take the task away from you after ten days if you don’t finish it. To avoid that, you should control your tasks and their deadlines. With every task, try to be as lazy as possible and cut every corner you can. The smaller the amount of work you perform on a task, the easier it will be to deliver it and pass all quality controls. Always remember that your efforts are not appreciated — only the deliverables matter. "},{"title":"Object-Oriented DynamoDB API","url":"/2014/04/14/jcabi-dynamo-java-api-of-aws-dynamodb.html","tags":["dynamodb","aws","java","jcabi"],"date":"2014-04-14 00:00:00 +0000","categories":[],"body":" I'm a big fan of cloud computing in general and of Amazon Web Services in particular. I honestly believe that in a few years big providers will host all, or almost all, computing and storage resources. When this is the case, we won't have to worry too much anymore about downtime, backups and system administrators. DynamoDB is one of the steps towards this future. This looks cool - jcabi-dynamo - a #Java Object layer atop the #DynamoDB SDK - http://t.co/khRFR2joKX #aws — Jeff Barr (@jeffbarr) September 19, 2013 DynamoDB is a NoSQL database accessible through RESTful JSON API. Its design is relatively simple. There are tables, which basically are collections of data structs, or in AWS terminology, \"items.\" Every item has a mandatory \"hash,\" an optional \"range\" and a number of other optional attributes. For instance, take the example table depts : +------+--------+---------------------------+ | dept | worker | Attributes | +------+--------+---------------------------+ | 205 | Jeff | job=\"manager\", sex=\"male\" | | 205 | Bob | age=43, city=\"Chicago\" | | 398 | Alice | age=27, job=\"architect\" | +------+--------+---------------------------+ For Java, Amazon provides an SDK , which mirrors all RESTful calls to Java methods. The SDK works fine, but is designed in a pure procedural style. Let's say we want to add a new item to the table above. RESTful call putItem looks like (in essence): putItem: tableName: depts item: dept: 435 worker: \"William\" job: \"programmer\" This is what the Amazon server needs to know in order to create a new item in the table. This is how you're supposed to make this call through the AWS Java SDK: 1 2 3 4 5 6 7 8 9 10 11 12 13 PutItemRequest request = new PutItemRequest (); request . setTableName ( \"depts\" ); Map < String , AttributeValue > attributes = new HashMap <>(); attributes . put ( \"dept\" , new AttributeValue ( 435 )); attributes . put ( \"worker\" , new AttributeValue ( \"William\" )); attributes . put ( \"job\" , new AttributeValue ( \" programmer )); request . setItem ( attributes ); AmazonDynamoDB aws = // instantiate it with credentials try { aws . putItem ( request ); } finally { aws . shutdown (); } The above script works fine, but there is one major drawback — it is not object oriented. It is a perfect example of an imperative procedural programming . To allow you to compare, let me show what I've done with jcabi-dynamo . Here is my code, which does exactly the same thing, but in an object-oriented way: 1 2 3 4 5 6 7 8 Region region = // instantiate it with credentials Table table = region . table ( \"depts\" ); Item item = table . put ( new Attributes () . with ( \"dept\" , 435 ) . with ( \"worker\" , \"William\" ) . with ( \"job\" , \"programmer\" ) ); My code is not only shorter, but it also employs encapsulation and separates responsibilities of classes. Table class (actually it is an interface internally implemented by a class) encapsulates information about the table, while Item encapsulates item details. We can pass an item as an argument to another method and all DynamoDB related implementation details will be hidden from it. For example, somewhere later in the code: void sayHello ( Item item ) { System . out . println ( \"Hello, \" + item . get ( \"worker\" )); } In this script, we don't know anything about DynamoDB or how to deal with its RESTful API. We interact solely with an instance of Item class. By the way, all public entities in jcabi-dynamo are Java interfaces. Thanks to that, you can test and mock the library completely. Let's consider a more complex example, which would take a page of code if we were to use a bare AWS SDK. Let's say that we want to remove all workers from our table who work as architects: Region region = // instantiate it with credentials Iterator < Item > workers = region . table ( \"depts\" ). frame () . where ( \"job\" , Condition . equalTo ( \"architect\" )); while ( workers . hasNext ()) { workers . remove (); } jcabi-dynamo has saved a lot of code lines in a few of my projects. You can see it in action at rultor-users . The library ships as a JAR dependency in Maven Central (get its latest versions from Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamo </artifactId> </dependency> "},{"title":"Definition Of Done","url":"/2014/04/15/definition-of-done.html","tags":["mgmt","xdsd"],"date":"2014-04-15 00:00:00 +0000","categories":[],"body":" Definition of Done (DoD) is a key definition used in Scrum and the one we also use in XDSD . DoD is an exit criteria of a simple atomic task and answers the question:\"am I done with this task?\" Moreover, DoD answers the question: \"will I be paid for the task?\" In XDSD , the definition of \"done\" is very simple — the task is done iff its author accepts the deliverables. At XDSD , our first and most important principle states that someone is paid only when he provides deliverables. Combining the definition of done and the principle of paying only for deliverables provides us a very important conclusion: we do not pay for un-finished tasks. Every task has its own time budget. Regardless of the number of people who worked on a task previously, only the last one — the one who managed to provide a working deliverable — receives payment. To better understand this principle, you should read: No Obligations Principle . Your goal as a developer working on a task should be to close it and receive payment as soon as possible. To that end, here are few things that can help you complete tasks and receive payments without too much frustration: Don't even start a task unless you're sure you can finish it; Ask any and all questions of the task author in advance (before beginning work); Don't assume anything — ask if you're not sure; Stay after the author to close tasks — be aggressive; Don't expect any help from anyone — you're on your own; Ask about payment if you don’t receive it automatically after an author closes your task(s) It is important to remember that, as a developer, it is your responsibility to ensure that tasks are closed and you receive payment. "},{"title":"Github Guidelines","url":"/2014/04/15/github-guidelines.html","tags":["github","xdsd"],"date":"2014-04-15 00:00:00 +0000","categories":[],"body":"This manual explains the workflow used when working with a XDSD project hosted on Github.com . You start when a Github issue is assigned to you. Next, you will receive a message from a project manager containing the issue number, title, description and its budget in hours (usually 30 minutes). If you don't agree with the budget allotment, don't hesitate to ask for an increase. As soon as you are comfortable with the budget and understand the scope of the work, say so in a reply to the ticket and start working. Be aware that you won't be paid for time spent above and beyond the allotted time budget. 1. Fork Even though you're part of the development team, you don't have write access to the repository in Github. Consequently, to contribute changes, you should fork the repository to your own Github account (create a private copy of it), make needed changes and then submit them for review using \"a pull request.\" After you submit a pull request review, the repository owner approves your changes by merging them into the main repository. This is how we protect the main development stream against accidental damage. This article explains how to fork a repository: fork-a-repo This one explains how to download and install Github on your computer: set-up-git Finally, don't forget to add your private SSH key to Github: generating-ssh-keys 2. Branch Once you have a forked our repository to your account, clone it to your computer, and then check out the master branch. For example: 1 2 git clone git@github.com:yegor256/jcabi.git git checkout master Now, it's time to branch ( 123 is the number of the Github issue you're going to work with, and the name of the branch): 1 git checkout -b 123 By convention, we use the same names for the branch and issue you're working with. 3. Changes All task-related questions should be discussed in the Github issue. For Github issues, we don't use emails, Skype, phone calls or meetings. All questions should be asked directly in the Github issues. Don't hesitate to submit new issues if something is not clear or you need help. It's a very common to receive a task that you may not be able to implement. Don't panic. This usually happens when you first just join a project and don't yet have enough information. If this happens, don't try to figure out a problem or issue by yourself. The rule of thumb for this type of situation is: \"if something is not clear, it is our fault, not yours.\" Therefore, if you don’t understand the project design, it is the fault of the project designer. Submit a bug report requesting an explanation of a design concept. You will be paid for this report, and the information you receive in the reply will be shared between all other developers. Read this article: Bugs Are Welcome . Don't expect anyone to help you. Your only source of help is the source code itself. If the code doesn't explain everything you need to know — it is a bug, which must be reported. 4. Commit and Push Make any needed changes using a text editor or IDE. It's a good practice to commit changes as soon as you make them. Don't accumulate large numbers of changes too long before committing them. 1 2 git commit -am '#123: the description of the changes' git push origin 123 If you have questions about the scope of work, post them in the Github issue and wait for an answer. If you think that the existing code needs improvements, don't hesitate to submit a new issue to Github. Don't try to fix all problems in one branch; let other programmers take care of them. 5. Pull Request Create a pull request in Github using the process in the following article: using-pull-requests Post its number in the original issue and wait for feedback. 6. Code Review After a while, your pull request will be reviewed by someone from the project team. In many cases, you may receive a few negative comments, and you will have to fix any and all issues associated with them. Your pull request won't be merged into master branch , until your changes satisfy the reviewer. Be patient with the reviewer, and listen to him carefully. However, don't think that your reviewer is always right. If you think that your changes are valid, insist that someone else review them. 7. Merge When everything looks good to the reviewer, he will inform our automated merge bot. The automated merge bot will then select your pull request and try to merge it into master branch. For various reasons, this operation fails often. If the merge fails, regardless of the reason, it is your responsibility to make sure that your branch is merged successfully. If you can't merge a branch because of failures in tests not associated with your task, don't try to fix them yourself. Instead, report a problem as a new bug and wait for its resolution. Remember, until your branch is merged, you are not paid. 8. Payment Once your changes are merged, return to the Github issue and ask the author to close it. Once the issue is closed by a project manager, you will receive your payment within a few hours, through oDesk or PayPal. "},{"title":"How XDSD Is Different","url":"/2014/04/17/how-xdsd-is-different.html","tags":["xdsd","mgmt"],"date":"2014-04-17 00:00:00 +0000","categories":["best"],"body":" eXtremely Distributed Software Development, or XDSD for short, is a methodology that differs significantly from working in traditional software development teams. Most XDSD methods are so different (yet critical) that many newcomers get confused. This article should help you bootstrap once you join a project managed with by XDSD principles — either as a developer or a project sponsor. We Pay Only For Closed Tasks Unlike with many other projects, in XDSD , we pay only for closed tasks and the agreed upon time budget. Let me explain by example. Let's say, you are a Ruby programmer and you a get a new task that requires you to fix a broken unit test. The task has a time budget of 30 minutes, as is the case most of the time. Sometimes, though, tasks may have time budgets of fifteen minutes or one hour. In our example, we agree upon a contract rate of $50 per hour. With the broken test, you will receive $25 for completing the task — 30 minute tasked billed at $50 per hour. It does not matter how long it actually takes you to fix the test. Your actual time spent on the project may be five minutes or five hours. Nevertheless, you will receive compensation for 30 minutes of work only. If you fix the broken test in 5 minutes, you receive $25. If the task takes you an hour, or even a month, to complete, you still receive only $25. Furthermore, if you fail to fix the unit test and close the task altogether, you will receive no pay at all for the assignment. You can view more details about this principle in the following articles: No Obligations Principle or Definition of Done . Revolver (2005) by Guy Ritchie As mentioned above, this is one of the most important differences between XDSD and other methods. Many people get confused when they see this principle in action, and some leave our projects because of it. They simply are used to being paid by the end of the month — no matter how much work they actually deliver. In XDSD, we consider this type of approach very unfair. We feel that people who deliver more results should receive more cash. Conversely, those who don't deliver should get less. We Deliver Unfinished Components Since most of our tasks are half an hour in size, we encourage developers to deliver unfinished components. Read more about this concept in the article below: Puzzle Driven Development . No Informal Communications Unlike many other projects or teams you may have worked with, XDSD uses no informal communication channels . To clarify, we never use emails, we never chat on Skype and we don't do any meetings or phone calls. Additionally, XDSD maintains no type mailing list. Our only method of communication is a ticket tracking system (which in most projects consists of Github Issues .) Moreover, we discourage horizontal communications between developers regarding the scope of individual tasks. When assigned a task, your single and only point of contact (and your only customer) is the task author. You communicate with the author in the ticket to clarify task requirements. When the requirements of a task are clear — and you understand them fully — deliver the result to the author and wait for him to close the task. After the author closes the task, the project manager pays you. Goodfellas (1990) by Martin Scorsese We're very strict about this principle — no informal communications. However, it doesn't mean that we are not interested in your opinions and constructive criticism. Rather, we encourage everyone to submit their suggestions and bugs. By the way, we pay for bugs (see the next section for further details about bug reporting and payments.) Since we have no formal communications, members of project teams are not required to work at specific times. Instead, team members work at times convenient for them in their time zones. This includes weekdays and weekends. We Pay For Bugs Unlike many other software teams, XDSD welcomes bug reports in all our projects. Therefore, we ask for bugs openly and expect team members to report them. Review the following article for complete details on XDSD bug reporting: Bugs are welcome We expect everyone involved with a project to report every bug found. Additionally, we encourage team members to make suggestions. In XDSD, we pay team members for every properly reported bug. XDSD makes payments for reported bugs because we believe that the more of them we can find, the higher the quality of the end product. Some new developers are surprised when they receive tasks such as \"you must find 10 bugs in class A.\" Often, the natural reaction is to ask \"what if there are no bugs?\" However, we believe that any software product may have an unlimited amount of bugs; it is just a matter of expending the time and effort needed to discover them. Only Pull Request We never grant team member access to the master branch — no matter how long you work on a project. Consequently, you must always submit your changes through pull requests (most of our projects are done in Github .) We enforce this policy not because we don't trust our developers, but simply because we don't trust anyone :) Read this article: Master Branch Must Be Read-Only . No Compromises About Code Quality Before merge any changes to the master branch, we check the entire code base with unit tests and static analyzers . Unit testing is a very common component in modern software development, and one by which you should not be surprised. However, the strictness of static analysis is something that often frustrates XDSD newcomers, and we understand that. We pay much more attention to the quality and uniformity of our source code than most of our competing software development teams. Even more important is that we never make compromises. If your pull request violates even one rule of the static analyzer, it won't be accepted. And, it doesn't matter how small or innocent that violation may look. This merging process is fully automated and can't be bypassed. "},{"title":"Mocking of HTTP Server in Java","url":"/2014/04/18/jcabi-http-server-mocking.html","tags":["jcabi","http","mocking"],"date":"2014-04-18 00:00:00 +0000","categories":[],"body":" Recently, I explained a fluent Java HTTP client created (mostly) to make HTTP interactions more object-oriented than with other available clients,including: Apache Client , Jersey Client and plain old HttpURLConnection . This client ships in the jcabi-http Maven artifact. However, the client part is not the only benefit of using jcabi-http . Jcabi also includes a server component that can help you in unit and integration testing of your HTTP clients. Let me show you an example first. In the example, I'm using hamcrest for assertions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 MkContainer container = new MkGrizzlyContainer () . next ( new MkAnswer . Simple ( \"hello, world!\" )) . start (); try { new JdkRequest ( container . home ()) . header ( \"User-agent\" , \"Myself\" ) . fetch () . assertBody ( Matchers . containsString ( \"hello\" )); } finally { container . stop (); } MkQuery query = container . take (); MatcherAssert . assertThat ( query . headers (). get ( \"User-agent\" ), Matchers . hasItem ( \"Myself\" ) ); Now, let's discover what happens here. In the first few lines, I create an instance of MkContainer , which literally has four methods: next(MkAnswer) , start() , stop() , and home() . It works as an HTTP server with a \"first-in-first-out\" queue for HTTP answers. We add answers, and the server returns them in response to HTTP requests. The server starts on start() call and stops on stop() . Its method home() returns a URL of its \"home page\". The server then binds itself to a randomly allocated TCP port. The container finds the first available and unoccupied port. In the example above, I added just one answer. This means that the container will reply only to the first HTTP request with that answer and that all consecutive requests will cause HTTP responses with status \"internal server error 500 .\" In lines 5 through 8, I make an HTTP request to the already started server. Also, I make an assertion that the body of the HTTP response contains the text \"hello\" . Obviously, this assertion will pass because the server will return \"hello, world!\" to my first request: new JdkRequest ( container . home ()) . header ( \"User-agent\" , \"Myself\" ) . fetch () . assertBody ( Matchers . containsString ( \"hello\" )); As you can see, I use container.home() in order to get the URL of the server. It is recommended that you allow the container to find the first unoccupied TCP port and bind itself to it. Nevertheless, if you need to specify your own port, you can do it with a one-argument method start(int) in MkContainer . I use try/finally to stop the container safely. In unit tests, this is not critical, as you can simplify your code and never stop the container. Besides, the container will be killed together with the JVM. However, for the sake of clarity, I would recommend you stop the container in the finally block. On line 12, I ask the stopped container to give me the first request it received. This mechanism is similar conceptually to the \"verify\" technology of mocking frameworks. For example, Mockito . MkQuery query = container . take (); MatcherAssert . assertThat ( query . headers (). get ( \"User-agent\" ), Matchers . hasItem ( \"Myself\" ) ); An instance of MkQuery exposes information about the query made. In this example, I get all headers of the HTTP request and making an assertion that the \"User-Agent\" header was there and had at least one value equal to \"Myself\" . This mocking technology is used actively in unit and integration tests of jcabi-github , which is a Java client to Github API. In its development, the technology is very important in checking which requests are being sent to the server and validating whether they comply with our requirements. Here, we are using jcabi-http mocking. As with the client, you need the jcabi-http.jar dependency (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-http </artifactId> </dependency> Besides the above, you need to add one more dependency, which is a Grizzly HTTP server. MkGrizzlyContainer is based on it. <dependency> <groupId> com.sun.grizzly </groupId> <artifactId> grizzly-servlet-webserver </artifactId> <scope> test </scope> </dependency> If you have any questions or suggestions, please submit them through Github issues . As always, bugs are welcome :) "},{"title":"How Hourly Rate Is Calculated","url":"/2014/04/20/how-hourly-rate-is-calculated.html","tags":["xdsd"],"date":"2014-04-20 00:00:00 +0000","categories":[],"body":" In XDSD , everyone — including project managers, analysts, programmers, and product owners — receives payments based on deliverables with agreed upon budgets. In the fhe first section of the article, How XDSD Is Different I explain exactly how this concept works. I don't explain in the article, though, how we decide which hourly rate is acceptable for each project participant. When new people come to us, usually they have some numbers in mind. They know how much they expect to make per week, per month or per day. We rarely negotiate the payment rates, but rather just accept reasonable offers (see How Much Do You Cost? ). Nonetheless, every few months, we review payments rates and change them accordingly (increasing or decreasing them as appropriate). Further along in the article, is a list of factors that influence our decision making process regarding payment rates. However, before we get to the factors that influence our rate-setting decisions, it is important to mention that — unlike most other companies or software teams — we don't pay attention to the following: Your geographic location; Skills and experience listed in your CV; Amount of time already spent on our projects; Age, sex, nationality, religious beliefs, etc. The factors listed below, though, are indeed very important to us. They affect your \"overall score\" significantly and play a major part in decisions to decrease or increase a payment rate. After changing a payment rate, we don't negotiate it with the project member. Keep in mind that besides decreasing your hourly rate, a low overall score may affect the number of tasks you receive from us. The best developers receive most of the new tasks. So, continue reading, follow our principles and learn how to earn and enjoy higher rates :) Fast Delivery The faster you deliver on a task, the better. We track all your completed tasks and can calculate easily how many days it takes you, on average, to close tasks. To increase this metric, you should try to close all tasks as soon as possible to reduce your overall completion-time average. If you see that a specific task is not suitable for you, don't hold on to it. Instead, inform your project manager as soon as possible that you do not want to work on the task. After you inform the project manager, he will try find you something else more suitable. By the way, the best developers usually close their tasks in five calendar days (or less) on average. Past Due Tasks Though we encourage everyone to reject tasks they don't like, we are strongly against overdue tasks. Once you have started to work on a task, we expect you to finish it on time. Our No Obligation Principle gives our project managers freedom to take any task away from you if don’t complete it in a reasonable amount of time (ten days). Removal of tasks by project managers affects your overall score negatively. Nevertheless, even the best developers sometimes have overdue tasks, and we understand that it happens from time to time. However, our best developers they keep their number of overdue tasks to a minimum. A good rule of thumb for acceptable numbers in this area is about one overdue task per twenty completed successfully and on time. Complexity Every XDSD task has a project role assigned to it. The article, Puzzle Driven Development by Roles , lists the key roles we use in XDSD projects. Generally speaking, the higher the role, the higher the complexity of tasks assigned to it. Therefore, closing a task in an \"architect\" role is much more important than closing one as an \"implementer\" (or \"developer.\") The more tasks you close in your current role, the faster you will receive promotions and receive pay-rate increases. Very often, our developers work in a few roles at the same time. Lengthy Discussions We discourage long conversations on one task. The longer the discussions about a task, the longer it takes to complete — which lowers your quality as a developer. Ideally, developers should receive a task, deliver the result and inform the task author after it's done. Afterwards, the task author closes the task and payment is made. We track the number of messages you post and receive in your tasks automatically. Consequently, too many messages may affect your overall score in a negative way. To avoid long conversations in tasks, submit new tickets with questions or bug reports. Again, the Puzzle Driven Development by Roles article explains the whole idea of helping us \"to break the project\" by submitting new bugs. Follow this concept and you'll be fine. Contribution via Bugs In XDSD Bugs Are Welcome . You are supposed to report bugs along the normal development activities. Besides receiving extra money for reporting bugs, you can also increase your overall rating. The best developers submit one bug for every 2 to 3 tasks they complete. "},{"title":"Basic HTTP Auth for S3 Buckets","url":"/2014/04/21/s3-http-basic-auth.html","tags":["aws","s3","http","s3auth"],"date":"2014-04-21 00:00:00 +0000","categories":[],"body":" Amazon S3 is a simple and very useful storage of binary objects (aka \"files\"). To use it, you create a \"bucket\" there with a unique name and upload your objects. Afterwards, AWS guarantees your object will be available for download through their RESTful API . A few years ago, AWS introduced a S3 feature called static website hosting . With static website hosting, you simply turn on the feature and all objects in your bucket become available through public HTTP. This is an awesome feature for hosting static content, such as images, JavaScript files, video and audio content. When using the hosting, you need to change the CNAME record in your DNS so that it points to www.example.com.aws.amazon.com . After changing the DNS entry, your static website is available at www.example.com just as it would be normally. When using Amazon S3, though, it is not possible to protect your website because the content is purely static. This means you can't have a login page on the front end. With the service, you can either make your objects either absolutely public — so that anyone can see them online — or assign access rights to them — but only for users connected through RESTful API. My use case with the service was a bit more complex, though. I wanted to host my static content as S3 objects. However, I wanted to do this while ensuring only a few people had access to the content using their Web browsers. HTTP Basic Authentication The HTTP protocol offers a nice \"basic access authentication\" feature that doesn't require any extra site pages. When an HTTP request arrives at the server, it doesn't deliver the content but replies with a 401 status response. This response means literally \"I don't know who you are, please authenticate yourself.\" The browser shows its native login screen and prompts for a user name and password. After entering the login credentials, they are concatenated, Base64 encoded, and added to the next request in Authorization HTTP header. Now, the browser tries to make another attempt to fetch the same webpage. But, this time, the HTTP request contains a header: Authorization: Basic am9lOnNlY3JldA== The above is just an example. In the example, the Base64 encoded part means joe:secret , where joe is the user name and secret the password entered by the user. This time the server has authentication information and can make a decision whether this user is authenticated (his password matches the server's records) and authorized (he has permission to access the request webpage). s3auth.com Since Amazon doesn't provide this feature, I decided to create a simple web service, s3auth.com , which stays in front of my Amazon S3 buckets and implements the HTTP-native authentication and authorization mechanism. Instead of making my objects public, though, I make them private and point my CNAME record to relay.s3auth.com . HTTP requests from Web browsers then arrive at my server, connect to Amazon S3, retrieve my objects and deliver them back in HTTP responses. The server implements authentication and authorization using a special file .htpasswd in the root of my bucket. The format of the \".htpasswd\" file is identical to the one used by Apache HTTP Server — one user per line. Every line has the name of a user and a hash version of his password. Implementation I made this software open source mostly to guarantee to my users that the server doesn't store their private data anywhere, but rather acts only as a pass-through service. As a result, the software is on Github . For the sake of privacy and convenience, I use only OAuth2 for user accounts. This means that I don't know who my users are. I don't possess their names or emails, but only their account numbers in Facebook, Google Plus or Github. Of course, I can find their names using these numbers, but this information is public anyway. The server is implemented in Java6. For its hosting, I'm using a single Amazon EC2 m1.small Ubuntu server. These days, the server seems to work properly and is stable. Extra Features Besides authentication and authorization, the s3auth.com server can render lists of pages — just like Apache HTTP Server. If you have a collection of objects in your bucket — but the index.html file is missing — Amazon S3 delivers a \"page not found\" result. Conversely, my server displays a list of objects in the bucket, when no \"index.html\" is present, and makes it possible to navigate up or down one folder. When your bucket has the versioning feature turned on, you are able to list all versions of any object in the browser. To do this, just add ?all-versions to the end of the URL to display the list. Next, click a version to have s3auth.com retrieve and render it. Traction I created this service mostly for myself, but apparently I'm not the only with the problems described above. At the moment, s3auth.com hosts over 300 domains and sends through more than 10Mb of data each hour. "},{"title":"Java XML Parsing Made Easy","url":"/2014/04/24/java-xml-parsing-and-traversing.html","tags":["xml","java","jcabi"],"date":"2014-04-24 00:00:00 +0000","categories":[],"body":"Unlike with many other modern languages, parsing XML in Java requires more than one line of code. XML traversing using XPath takes even more code, and I find this is unfair and annoying. I'm a big fan of XML and use it it in almost every Java application. Some time ago, I decided to put all of that XML-to-DOM parsing code into a small library — jcabi-xml . Put simply, the library is a convenient wrapper for JDK-native DOM manipulations. That's why it is small and dependency-free. With the following example, you can see just how simple XML parsing can be: import com.jcabi.xml.XML ; import com.jcabi.xml.XMLDocument ; XML xml = new XMLDocument ( \"<root><a>hello</a><b>world!</b></root>\" ); Now, we have an object of interface XML that can traverse the XML tree and convert it back to text. For example: // outputs \"hello\" System . out . println ( xml . xpath ( \"/root/a/text()\" ). get ( 0 )); // outputs the entire XML document System . out . println ( xml . toString ()); Method xpath() allows you to find a collection of text nodes or attributes in the document, and then convert them to a collection of strings, using XPath query : // outputs \"hello\" and \"world\" for ( String text : xml . xpath ( \"/root/*/text()\" )) { System . out . println ( text ); } Method nodes() enables the same XPath search operation, but instead returns a collection of instances of XML interface: // outputs \"<a>hello</a>\" and \"<b>world</b>\" for ( XML node : xml . xpath ( \"/root/*\" )) System . out . println ( node ); } Besides XML parsing, printing and XPath traversing, jcabi-xml also provides XSD validation and XSL transformations. I'll write about those features in the next post :) "},{"title":"Incremental Requirements With Requs","url":"/2014/04/26/incremental-requirements-with-requs.html","tags":["requs","xdsd","requirements"],"date":"2014-04-26 00:00:00 +0000","categories":[],"body":"Requirements engineering is one of the most important disciplines in software development. Perhaps, even more important than architecture, design or coding itself. Joy Beatty and Karl Wiegers in Software Requirements argue that the cost of mistakes made in a requirements specification is significantly higher than a bug in source code. I totally agree. In XDSD projects we specify requirements using Requs , a controlled natural language that sounds like English, while at the same time is parseable by computers. A simple requirements document in Requs may look similar to: 1 2 3 Department has employee-s. Employee has name and salary. UC1 where Employee gets raise: \"TBD\". This Software Requirements Specification (SRS) defines two types ( Department and Employee ) and one method UC (aka \"use case\"). Requs syntax is explained here . The main and only goal of requirements engineering in any XDSD project is to create a complete and non-ambiguous SRS document. The person who performs this task is called the \"system analyst\". This article explains his or her main tasks and discusses possible pitfalls. Tasks We modify SRS incrementally, and our increments are very small. For instance, say we have the sample document I mentioned above, and I'm a system analyst on the project. All my tasks will be similar to \"there is a bug in SRS, let's fix it\". Even if it is a suggestion, it will still start with a complaint about the incompleteness of the SRS. For example: UC1 doesn't explain how exactly an employee receives a raise. Does the salary of an employee have limits? Can it be negative? How many employees can a department have? Can it be zero? Can an employee receive a decrease in salary? All of these bugs are addressed to me. I need to fix them by improving the SRS. My workflow is the same in every task: Understand what is required Change the SRS Close the task Let's try this step by step. Requirements Providers As a system analyst, my job is to understand what product owners (aka \"requirements providers\") want and document their wishes. In most cases, their wants and wishes are very vague and chaotic. My job is to make them complete and unambiguous. That's why the first step is to understand what is required. First of all, I must determine who the product owner is before I can begin. The product owner signs the SRS, so I should pay complete attention to his opinions. However, my job is not only to listen, but also to suggest. A good system analyst can provoke creative thinking in a product owner by asking the right questions. OK, now I that know the identity of the product owner, I need to talk to him. In XDSD, we don't do any meetings, phone calls, or any other type of informal communications. Therefore, my only mechanism for receiving the information I need is with is — tickets. I will submit new tickets, addressing them to the product owner. As there can be many product owners in a project, I must submit tickets that clearly state in the first sentence that the ticket pertains to questions for a particular owners. The person receiving the ticket will then determine the best person to answer it. Thus, while working with a single task, I will submit many questions and receive many interesting answers. I'll do all this in order to improve my understanding of the product the owners are developing. When I understand how the SRS should be fixed, it is time to make changes in the Requs files. Requs Files The SRS document is generated automatically on every continuous integration build cycle. It is compiled from pieces called .req files, which are usually located in the src/main/requs directory in a project repository. My job, as a system analyst, is to make changes to some of these files and submit a pull request for review. Github Guidelines explains [how to work with Github. However, in short, I need to: Clone the repository; Check out its copy to my computer; Make changes; Commit my changes; Push them to my remote fork; Submit a pull request It doesn't really matter which files I edit because Requs automatically composes together all files with the req extension. I can even add new files to the directory — they will be picked up. Likewise, I can also add sub- directories with files. Local Build Before submitting a pull request, I will try to validate that my changes are syntactically and grammatically valid. I will compile Requs files into the SRS document using the same method our continuous integration server uses to compile them. Before I can compile, though, I need to install JDK7 and Maven . Afterwards, I make the following command line call in the project directory: 1 mvn clean requs:compile After entering the commands, I expect to see the BUILD SUCCESS message. If not, there are some errors and I should fix them. My pull request won't be merged and I won't be able to close the task if Requs can't compile the files. Once compiled, I can open the SRS in Firefox. It is in target/requs/index.xml . Even though it is an XML file, Firefox can open it as a webpage. Other browsers won't work. Well, Google Chrome will work, but only with this small trick . Pull Request Review Once all changes are finished, I will submit a pull request. A project manager will the assign someone to review my pull request and I will receive feedback. In most cases, there will be at least a few corrections requested by the reviewer. Generally speaking, my requests are reviewed by other system analysts. Therefore, I must address all comments and make sure my changes satisfy the reviewer. I will make extra changes to the same branch locally, and push them to Github. The pull request will be updated automatically, so I don't need to create a new one. Once the pull request is clean enough for the reviewer, he will merge it into the master branch. Close and Get Paid Finally, my pull request is merged and I get back to the task owner. I tell him that the SRS was fixed and request that he review it. His original problem should be fixed by now — the SRS should provide the information required. He then closes the task and the project manager pays me within a few hours. "},{"title":"Typical Mistakes in Java Code","url":"/2014/04/27/typical-mistakes-in-java-code.html","tags":["anti-pattern","java","oop"],"date":"2014-04-27 00:00:00 +0000","categories":["jcg"],"body":"This page contains most typical mistakes I see in the Java code of people working with me. Static analysis (we're using qulice can't catch all of the mistakes for obvious reasons, and that's why I decided to list them all here. Let me know if you want to see something else added here, and I'll be happy to oblige. All of the listed mistakes are related to object-oriented programming in general and to Java in particular. Class Names Your class should be an abstraction of a real life entity with no \"validators\", \"controllers\", \"managers\", etc. If your class name ends with an \"-er\" — it's a bad design . BTW, here are my seven virtues of a good object. And, of course, utility classes are anti-patterns, like StringUtils , FileUtils , and IOUtils from Apache. The above are perfect examples of terrible designs. Read this follow up post: OOP Alternative to Utility Classes Of course, never add suffixes or prefixes to distinguish between interfaces and classes . For example, all of these names are terribly wrong: IRecord , IfaceEmployee , or RecordInterface . Usually, interface name is the name of a real-life entity, while class name should explain its implementation details. If there is nothing specific to say about an implementation, name it Default, Simple , or something similar. For example: class SimpleUser implements User {}; class DefaultRecord implements Record {}; class Suffixed implements Name {}; class Validated implements Content {}; Method Names Methods can either return something or return void . If a method returns something, then its name should explain what it returns , for example (don't use the get prefix ever ): boolean isValid ( String name ); String content (); int ageOf ( File file ); If it returns void, then its name should explain what it does . For example: void save ( File file ); void process ( Work work ); void append ( File file , String line ); There is only one exception to the rule just mentioned — test methods for JUnit. They are explained below. Test Method Names Method names in JUnit tests should be created as English sentences without spaces. It's easier to explain by example: /** * HttpRequest can return its content in Unicode. * @throws Exception If test fails */ public void returnsItsContentInUnicode () throws Exception { } It's important to start the first sentence of your JavaDoc with the name of the class you're testing followed by can . So, your first sentence should always be similar to \"somebody can do something\". The method name will state exactly the same, but without the subject. If I add a subject at the beginning of the method name, I should get a complete English sentence, as in above example: \"HttpRequest returns its content in unicode\". Pay attention that the test method doesn't start with can .Only JavaDoc comments start with 'can.' Additionally, method names shouldn’t start with a verb. It's a good practice to always declare test methods as throwing Exception . Variable Names Avoid composite names of variables, like timeOfDay , firstItem , or httpRequest . I mean with both — class variables and in-method ones. A variable name should be long enough to avoid ambiguity in its scope of visibility, but not too long if possible. A name should be a noun in singular or plural form, or an appropriate abbreviation. For example: List < String > names ; void sendThroughProxy ( File file , Protocol proto ); private File content ; public HttpRequest request ; Sometimes, you may have collisions between constructor parameters and in-class properties if the constructor saves incoming data in an instantiated object. In this case, I recommend to create abbreviations by removing vowels (see how USPS abbreviates street names ). Another example: public class Message { private String recipient ; public Message ( String rcpt ) { this . recipient = rcpt ; } } In many cases, the best hint for a name of a variable can ascertained by reading its class name. Just write it with a small letter, and you should be good: File file ; User user ; Branch branch ; However, never do the same for primitive types, like Integer number or String string . You can also use an adjective, when there are multiple variables with different characteristics. For instance: String contact(String left, String right); Constructors Without exceptions, there should be only one constructor that stores data in object variables. All other constructors should call this one with different arguments. For example: public class Server { private String address ; public Server ( String uri ) { this . address = uri ; } public Server ( URI uri ) { this ( uri . toString ()); } } One-time Variables Avoid one-time variables at all costs. By \"one-time\" I mean variables that are used only once. Like in this example: String name = \"data.txt\" ; return new File ( name ); This above variable is used only once and the code should be refactored to: return new File ( \"data.txt\" ); Sometimes, in very rare cases — mostly because of better formatting — one-time variables may be used. Nevertheless, try to avoid such situations at all costs. Exceptions Needless to say, you should never swallow exceptions, but rather let them bubble up as high as possible. Private methods should always let checked exceptions go out. Never use exceptions for flow control. For example this code is wrong: int size ; try { size = this . fileSize (); } catch ( IOException ex ) { size = 0 ; } Seriously, what if that IOException says \"disk is full\"? Will you still assume that the size of the file is zero and move on? Indentation For indentation, the main rule is that a bracket should either end a line or be closed on the same line (reverse rule applies to a closing bracket). For example, the following is not correct because the first bracket is not closed on the same line and there are symbols after it. The second bracket is also in trouble because there are symbols in front of it and it is not opened on the same line: final File file = new File(directory, \"file.txt\"); Correct indentation should look like: StringUtils.join( Arrays.asList( \"first line\", \"second line\", StringUtils.join( Arrays.asList(\"a\", \"b\") ) ), \"separator\" ); The second important rule of indentation says that you should put as much as possible on one line - within the limit of 80 characters. The example above is not valid since it can be compacted: StringUtils.join( Arrays.asList( \"first line\", \"second line\", StringUtils.join(Arrays.asList(\"a\", \"b\")) ), \"separator\" ); Redundant Constants Class constants should be used when you want to share information between class methods, and this information is a characteristic (!) of your class. Don't use constants as a replacement of string or numeric literals — very bad practice that leads to code pollution. Constants (as with any object in OOP) should have a meaning in a real world. What meaning do these constants have in the real world: class Document { private static final String D_LETTER = \"D\" ; // bad practice private static final String EXTENSION = \".doc\" ; // good practice } Another typical mistake is to use constants in unit tests to avoid duplicate string/numeric literals in test methods. Don't do this! Every test method should work with its own set of input values. Use new texts and numbers in every new test method. They are independent. So, why do they have to share the same input constants? Test Data Coupling This is an example of data coupling in a test method: User user = new User ( \"Jeff\" ); // maybe some other code here MatcherAssert . assertThat ( user . name (), Matchers . equalTo ( \"Jeff\" )); On the last line, we couple \"Jeff\" with the same string literal from the first line. If, a few months later, someone wants to change the value on the third line, he/she has to spend extra time finding where else \"Jeff\" is used in the same method. To avoid this data coupling, you should introduce a variable. "},{"title":"XML/XPath Matchers for Hamcrest","url":"/2014/04/28/xml-xpath-hamcrest-matchers.html","tags":["xml","hamcrest","xpath","testing"],"date":"2014-04-28 00:00:00 +0000","categories":[],"body":" Hamcrest is my favorite instrument in unit testing. It replaces the JUnit procedural assertions of org.junit.Assert with an object-oriented mechanism. However, I will discuss that subject in more detail sometime later. Now, though, I want to demonstrate a new library published today on Github and Maven Central: jcabi-matchers . jcabi-matchers is a collection of Hamcrest matchers to make XPath assertions in XML and XHTML documents. Let's say, for instance, a class that is undergoing testing produces an XML that needs to contain a single <message> element with the content \"hello, world!\" This is how that code would look in a unit test: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import com.jcabi.matchers.XhtmlMatchers ; import org.hamcrest.MatcherAssert ; import org.junit.Test ; public class FooTest { @Test public void hasWelcomeMessage () { MatcherAssert . assertThat ( new Foo (). createXml (), XhtmlMatchers . hasXPaths ( \"/document[count(message)=1]\" , \"/document/message[.='hello, world!']\" ) ); } } There are two alternatives to the above that I'm aware of, which are do almost the same thing: xml-matchers by David Ehringer and hasXPath() method in Hamcrest itself. I have tried them both, but faced a number of problems. First, Hamcrest hasXPath() works only with an instance of Node . With this method, converting a String into Node becomes a repetitive and routine task in every unit test. The above is a very strange limitation of Hamcrest in contrast to jcabi-matchers , which works with almost anything, from a String to a Reader and even an InputStream . Second, `XmlMatchers from xml-matchers provides a very inconvenient way for working with namespaces. Before you can use an XPath query with a non-default namespace, you should create an instance of NamespaceContext. The library provides a simple implementation of this interface, but, still, it is requires extra code in every unit test. jcabi-matchers simplifies namespace handling problems even further, as it pre-defines most popular namespaces, including xtml , xs , xsl , etc. The following example works right out-of-the-box — without any extra configuration: 1 2 3 4 MatcherAssert . assertThat ( new URL ( \"http://www.google.com\" ). getContent (), XhtmlMatchers . hasXPath ( \"//xhtml:body\" ) ); To summarize, my primary objective with the library was its simplicity of usage. "},{"title":"W3C Java Validators","url":"/2014/04/29/w3c-java-validators.html","tags":["w3c","java","jcabi"],"date":"2014-04-29 00:00:00 +0000","categories":[],"body":" A few years ago, I created two Java wrappers for W3C validators: ( HTML and CSS ). Both wrappers seemed to be working fine and were even listed by W3C on their website in the API section. Until recently, these wrappers have always been part of ReXSL library. A few days ago, though, I took the wrappers out of ReXSL and published them as a standalone library — jcabi-w3c . Consequently, now seems to be a good time to write a few words about them. Below is an example that demonstrates how you can validate an HTML document against W3C compliancy rules: 1 2 3 4 import com.jcabi.w3c.ValidatorBuilder ; assert ValidatorBuilder . html () . validate ( \"<html>hello, world!</html>\" ) . valid (); The valid() method is a black or white indicator that returns false when the document is not valid. Additionally, you can obtain more information through a list of \"defects\" returned by the W3C server: 1 2 3 Collection < Defect > defects = ValidatorBuilder . html () . validate ( \"<html>hello, world!</html>\" ) . errors (); The same can be done with CSS: 1 2 3 Collection < Defect > defects = ValidatorBuilder . css () . validate ( \"body { font-family: Arial; }\" ) . errors (); Personally, I think it is a good practice to validate all of HTML pages produced by your application against W3C during integration testing. It's not a matter of seeking perfection, but rather of preventing bigger problems later. These dependencies are mandatory when using jcabi-w3c (get their latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-w3c </artifactId> </dependency> <dependency> <groupId> org.glassfish </groupId> <artifactId> javax.json </artifactId> </dependency> <dependency> <groupId> com.sun.jersey </groupId> <artifactId> jersey-client </artifactId> </dependency> <dependency> <groupId> org.hamcrest </groupId> <artifactId> hamcrest-core </artifactId> </dependency> "},{"title":"DynamoDB Local Maven Plugin","url":"/2014/05/01/dynamodb-local-maven-plugin.html","tags":["dynamodb","maven","aws","java"],"date":"2014-05-01 00:00:00 +0000","categories":[],"body":" DynamoDB Local is a locally running copy of Amazon DynamoDB server. Amazon developed the tool and based it on SQLite. It acts as a real DynamoDB service through the RESTful API. I guess, DynamoDB Local is meant to be used in integration testing and this is how we're going to use it below. I use Maven to run all of my Java integration testing using maven-failsafe-plugin . The philosophy of integration testing with Maven is that you start all your supplementary test stubs during the pre-integration-test phase, run your tests in the integration-test phase and then shutdown all stubs during the post-integration-test . It would be great if it were possible to use DynamoDB Local that way. I didn't find any Maven plugins for that purpose, so I decided to create my own — jcabi-dynamodb-maven-plugin . Full usage details for the plugin are explained on its website . However, here is a simple example (get its latest versions in Maven Central ): <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamodb-maven-plugin </artifactId> <executions> <execution> <goals> <goal> start </goal> <goal> stop </goal> </goals> <configuration> <port> 10500 </port> <dist> ${project.build.directory}/dynamodb-dist </dist> </configuration> </execution> </executions> </plugin> The above configuration will start DynamoDB Local right before running integration tests, and then stop it immediately afterwards. The server will listen at TCP port 10500. While the number is used in the example, you're supposed to use a randomly allocated port instead. When the DynamoDB Local server is up and running, we can create an integration test for it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import com.amazonaws.auth.BasicAWSCredentials ; import com.amazonaws.services.dynamodbv2.AmazonDynamoDB ; import com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient ; import com.amazonaws.services.dynamodbv2.model.ListTablesResult ; public class FooITCase { @Test public void worksWithAwsDynamoDb () { AmazonDynamoDB aws = new AmazonDynamoDBClient ( new BasicAWSCredentials ( \"\" , \"\" ) ); aws . setEndpoint ( \"http://localhost:10500\" ); ListTablesResult list = aws . listTables (); for ( String name : list . getTableNames ()) { System . out . println ( \"table found: \" + name ); } } } Of course, there won't be any output because the server starts without any tables. Since the server is empty, you should create tables before every integration test, using createTable() from DynamoDB SDK . To avoid this type of extra hassle, in the latest version 0.6 of jcabi-dynamodb-maven-plugin we introduced a new goal create-tables : <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamodb-maven-plugin </artifactId> <executions> <execution> <goals> <goal> create-tables </goal> </goals> <configuration> <tables> <table> ${basedir}/src/test/dynamodb/foo.json </table> </tables> </configuration> </execution> </executions> </plugin> The foo.json file used above should contain a JSON request that is sent to DynamoDB Local right after it is up and running. The request should comply with the specification of CreateTable request. For example: { \"AttributeDefinitions\" : [ { \"AttributeName\" : \"id\" , \"AttributeType\" : \"N\" } ], \"KeySchema\" : [ { \"AttributeName\" : \"id\" , \"KeyType\" : \"HASH\" } ], \"ProvisionedThroughput\" : { \"ReadCapacityUnits\" : \"1\" , \"WriteCapacityUnits\" : \"1\" }, \"TableName\" : \"foo\" } The table will be created during the pre-integration-test phase and dropped at the post-integration-test phase. Now, we can make our integration test much more meaningful with the help of jcabi-dynamo : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import com.jcabi.dynamo.Attributes ; import com.jcabi.dynamo.Conditions ; import com.jcabi.dynamo.Credentials ; import com.jcabi.dynamo.Region ; import com.jcabi.dynamo.Table ; import org.hamcrest.MatcherAssert ; import org.hamcrest.Matchers ; public class FooITCase { @Test public void worksWithAwsDynamoDb () { Region region = new Region . Simple ( new Credentials . Simple ( \"\" , \"\" )); Table table = region . table ( \"foo\" ); table . put ( new Attributes () . with ( \"id\" , 123 ) . with ( \"name\" , \"Robert DeNiro\" ) ); MatcherAssert . assertThat ( table . frame (). where ( \"id\" , Conditions . equalTo ( 123 )), Matchers . notEmpty () ); } } The above test will put a new item into the table and then assert that the item is there. The plugin was tested with three operating systems, and proved to work without problems: Mac OS X 10.8.5, Windows 7 SP1 and Ubuntu Linux 12.04 Desktop. "},{"title":"OOP Alternative to Utility Classes","url":"/2014/05/05/oop-alternative-to-utility-classes.html","tags":["oop","anti-pattern"],"date":"2014-05-05 00:00:00 +0000","categories":["best","jcg"],"body":"A utility class (aka helper class) is a \"structure\" that has only static methods and encapsulates no state. StringUtils , IOUtils , FileUtils from Apache Commons ; Iterables and Iterators from Guava , and Files from JDK7 are perfect examples of utility classes. This design idea is very popular in the Java world (as well as C#, Ruby, etc.) because utility classes provide common functionality used everywhere. Here, we want to follow the DRY principle and avoid duplication. Therefore, we place common code blocks into utility classes and reuse them when necessary: // This is a terrible design, don't reuse public class NumberUtils { public static int max ( int a , int b ) { return a > b ? a : b ; } } Indeed, this a very convenient technique!? Utility Classes Are Evil However, in an object-oriented world, utility classes are considered a very bad (some even may say \"terrible\") practice. There have been many discussions of this subject; to name a few: Are Helper Classes Evil? by Nick Malik, Why helper, singletons and utility classes are mostly bad by Simon Hart, Avoiding Utility Classes by Marshal Ward, Kill That Util Class! by Dhaval Dalal, Helper Classes Are A Code Smell by Rob Bagby. Additionally, there are a few questions on StackExchange about utility classes: If a “Utilities” class is evil, where do I put my generic code? , Utility Classes are Evil . A dry summary of all their arguments is that utility classes are not proper objects; therefore, they don't fit into object-oriented world. They were inherited from procedural programming, mostly because most were used to a functional decomposition paradigm back then. Assuming you agree with the arguments and want to stop using utility classes, I'll show by example how these creatures can be replaced with proper objects. Procedural Example Say, for instance, you want to read a text file, split it into lines, trim every line and then save the results in another file. This is can be done with FileUtils from Apache Commons: 1 2 3 4 5 6 7 8 void transform ( File in , File out ) { Collection < String > src = FileUtils . readLines ( in , \"UTF-8\" ); Collection < String > dest = new ArrayList <>( src . size ()); for ( String line : src ) { dest . add ( line . trim ()); } FileUtils . writeLines ( out , dest , \"UTF-8\" ); } The above code may look clean; however, this is procedural programming, not object-oriented. We are manipulating data (bytes and bits) and explicitly instructing the computer from where to retrieve them and then where to put them on every single line of code. We're defining a procedure of execution . Object-Oriented Alternative In an object-oriented paradigm, we should instantiate and compose objects, thus letting them manage data when and how they desire. Instead of calling supplementary static functions, we should create objects that are capable of exposing the behaviour we are seeking: public class Max implements Number { private final int a ; private final int b ; public Max ( int x , int y ) { this . a = x ; this . b = y ; } @Override public int intValue () { return this . a > this . b ? this . a : this . b ; } } This procedural call: int max = NumberUtils . max ( 10 , 5 ); Will become object-oriented: int max = new Max ( 10 , 5 ). intValue (); Potato, potato? Not really; just read on... Objects Instead of Data Structures This is how I would design the same file-transforming functionality as above but in an object-oriented manner: 1 2 3 4 5 6 7 8 9 void transform ( File in , File out ) { Collection < String > src = new Trimmed ( new FileLines ( new UnicodeFile ( in )) ); Collection < String > dest = new FileLines ( new UnicodeFile ( out ) ); dest . addAll ( src ); } FileLines implements Collection<String> and encapsulates all file reading and writing operations. An instance of FileLines behaves exactly as a collection of strings and hides all I/O operations. When we iterate it — a file is being read. When we addAll() to it — a file is being written. Trimmed also implements Collection<String> and encapsulates a collection of strings ( Decorator pattern ). Every time the next line is retrieved, it gets trimmed. All classes taking participation in the snippet are rather small: Trimmed , FileLines , and UnicodeFile . Each of them is responsible for its own single feature, thus following perfectly the single responsibility principle . On our side, as users of the library, this may be not so important, but for their developers it is an imperative. It is much easier to develop, maintain and unit-test class FileLines rather than using a readLines() method in a 80+ methods and 3000 lines utility class FileUtils . Seriously, look at its source code . An object-oriented approach enables lazy execution. The in file is not read until its data is required. If we fail to open out due to some I/O error, the first file won't even be touched. The whole show starts only after we call addAll() . All lines in the second snippet, except the last one, instantiate and compose smaller objects into bigger ones. This object composition is rather cheap for the CPU since it doesn't cause any data transformations. Besides that, it is obvious that the second script runs in O(1) space, while the first one executes in O(n). This is the consequence of our procedural approach to data in the first script. In an object-oriented world, there is no data; there are only objects and their behavior! "},{"title":"Why NULL is Bad?","url":"/2014/05/13/why-null-is-bad.html","tags":["oop","anti-pattern"],"date":"2014-05-13 00:00:00 +0000","categories":["best","jcg"],"body":"A simple example of NULL usage in Java: 1 2 3 4 5 6 7 public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { return null ; } return new Employee ( id ); } What is wrong with this method? It may return NULL instead of an object — that's what is wrong. NULL is a terrible practice in an object-oriented paradigm and should be avoided at all costs. There have been a number of opinions about this published already, including Null References, The Billion Dollar Mistake presentation by Tony Hoare and the entire Object Thinking book by David West. Here, I'll try to summarize all the arguments and show examples of how NULL usage can be avoided and replaced with proper object-oriented constructs. Basically, there are two possible alternatives to NULL . The first one is Null Object design pattern (the best way is to make it a constant): public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { return Employee . NOBODY ; } return Employee ( id ); } The second possible alternative is to fail fast by throwing an Exception when you can't return an object: public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { throw new EmployeeNotFoundException ( name ); } return Employee ( id ); } Now, let's see the arguments against NULL . Besides Tony Hoare's presentation and David West's book mentioned above, I read these publications before writing this post: Clean Code by Robert Martin, Code Complete by Steve McConnell, Say \"No\" to \"Null\" by John Sonmez, Is returning null bad design? discussion at StackOverflow. Ad-hoc Error Handling Every time you get an object as an input you must check whether it is NULL or a valid object reference. If you forget to check, a NullPointerException (NPE) may break execution in runtime. Thus, your logic becomes polluted with multiple checks and if/then/else forks: // this is a terrible design, don't reuse Employee employee = dept . getByName ( \"Jeffrey\" ); if ( employee == null ) { System . out . println ( \"can't find an employee\" ); System . exit (- 1 ); } else { employee . transferTo ( dept2 ); } This is how exceptional situations are supposed to be handled in C and other imperative procedural languages. OOP introduced exception handling primarily to get rid of these ad-hoc error handling blocks. In OOP, we let exceptions bubble up until they reach an application-wide error handler and our code becomes much cleaner and shorter: dept . getByName ( \"Jeffrey\" ). transferTo ( dept2 ); Consider NULL references an inheritance of procedural programming, and use 1) Null Objects or 2) Exceptions instead. Ambiguous Semantic In order to explicitly convey its meaning, the function getByName() has to be named getByNameOrNullIfNotFound() . The same should happen with every function that returns an object or NULL . Otherwise, ambiguity is inevitable for a code reader. Thus, to keep semantic unambiguous, you should give longer names to functions. To get rid of this ambiguity, always return a real object, a null object or throw an exception. Some may argue that we sometimes have to return NULL , for the sake of performance. For example, method get() of interface Map in Java returns NULL when there is no such item in the map: Employee employee = employees . get ( \"Jeffrey\" ); if ( employee == null ) { throw new EmployeeNotFoundException (); } return employee ; This code searches the map only once due to the usage of NULL in Map . If we would refactor Map so that its method get() will throw an exception if nothing is found, our code will look like this: if (! employees . containsKey ( \"Jeffrey\" )) { // first search throw new EmployeeNotFoundException (); } return employees . get ( \"Jeffrey\" ); // second search Obviously, this is method is twice as slow as the first one. What to do? The Map interface (no offense to its authors) has a design flaw. Its method get() should have been returning an Iterator so that our code would look like: Iterator found = Map . search ( \"Jeffrey\" ); if (! found . hasNext ()) { throw new EmployeeNotFoundException (); } return found . next (); BTW, that is exactly how C++ STL map::find() method is designed. Computer Thinking vs. Object Thinking Statement if (employee == null) is understood by someone who knows that an object in Java is a pointer to a data structure and that NULL is a pointer to nothing ( 0x00000000 , in Intel x86 processors). However, if you start thinking as an object, this statement makes much less sense. This is how our code looks from an object point of view: - Hello, is it a software department? - Yes. - Let me talk to your employee \"Jeffrey\" please. - Hold the line please... - Hello. - Are you NULL? The last question in this conversation sounds weird, doesn’t it? Instead, if they hang up the phone after our request to speak to Jeffrey, that causes a problem for us (Exception). At that point, we try to call again or inform our supervisor that we can't reach Jeffrey and complete a bigger transaction. Alternatively, they may let us speak to another person, who is not Jeffrey, but who can help with most of our questions or refuse to help if we need something \"Jeffrey specific\" (Null Object). Slow Failing Instead of failing fast , the code above attempts to die slowly, killing others on its way. Instead of letting everyone know that something went wrong and that an exception handling should start immediately, it is hiding this failure from its client. This argument is close to the \"ad-hoc error handling\" discussed above. It is a good practice to make your code as fragile as possible, letting it break when necessary. Make your methods extremely demanding as to the data they manipulate. Let them complain by throwing exceptions, if the provided data provided is not sufficient or simply doesn’t fit with the main usage scenario of the method. Otherwise, return a Null Object, that exposes some common behavior and throws exceptions on all other calls: public Employee getByName ( String name ) { int id = database . find ( name ); Employee employee ; if ( id == 0 ) { employee = new Employee () { @Override public String name () { return \"anonymous\" ; } @Override public void transferTo ( Department dept ) { throw new AnonymousEmployeeException ( \"I can't be transferred, I'm anonymous\" ); } }; } else { employee = Employee ( id ); } return employee ; } Mutable and Incomplete Objects In general, it is highly recommended to design objects with immutability in mind. This means that an object gets all necessary knowledge during its instantiating and never changes its state during the entire lifecycle. Very often, NULL values are used in lazy loading , to make objects incomplete and mutable. For example: public class Department { private Employee found = null ; public synchronized Employee manager () { if ( this . found == null ) { this . found = new Employee ( \"Jeffrey\" ); } return this . found ; } } This technology, although widely used, is an anti-pattern in OOP. Mostly because it makes an object responsible for performance problems of the computational platform, which is something an Employee object should not be aware of. Instead of managing a state and exposing its business-relevant behavior, an object has to take care of the caching of its own results — this is what lazy loading is about. Caching is not something an employee does in the office, does he? The solution? Don't use lazy loading in such a primitive way, as in the example above. Instead, move this caching problem to another layer of your application. For example, in Java, you can use aspect-oriented programming aspects. For example, jcabi-aspects has @Cacheable annotation that caches the value returned by a method: import com.jcabi.aspects.Cacheable ; public class Department { @Cacheable ( forever = true ) public Employee manager () { return new Employee ( \"Jacky Brown\" ); } } I hope this analysis was convincing enough that you will stop NULL -ing your code :) "},{"title":"Object-Oriented Github API","url":"/2014/05/14/object-oriented-github-java-sdk.html","tags":["github","jcabi"],"date":"2014-05-14 00:00:00 +0000","categories":[],"body":" Github is an awesome platform for maintaining Git sources and tracking project issues. I moved all my projects (both private and public) to Github about three years ago and have no regrets. Moreover, Github gives access to almost all of its features through RESTful JSON API. There are a few Java SDKs that wrap and expose the API. I tried to use them, but faced a number of issues: They are not really object-oriented (even though one of them has a description that says it is) They are not based on JSR-353 (JSON Java API) They provide no mocking instruments They don't cover the entire API and can't be extended Keeping in mind all those drawbacks, I created my own library — jcabi-github . Let's look at its most important advantages. Object Oriented for Real Github server is an object. A collection of issues is an object, an individual issue is an object, its author is an author, etc. For example, to retrieve the name of the author we use: Github github = new RtGithub ( /* credentials */ ); Repos repos = github . repos (); Repo repo = repos . get ( new Coordinates . Simple ( \"jcabi/jcabi-github\" )); Issues issues = github . issues (); Issue issue = issues . get ( 123 ); User author = new Issue . Smart ( issue ). author (); System . out . println ( author . name ()); Needless to say, Github , Repos , Repo , Issues , Issue , and User are interfaces. Classes that implement them are not visible in the library. Mock Engine MkGithub class is a mock version of a Github server. It behaves almost exactly the same as a real server and is the perfect instrument for unit testing. For example, say that you're testing a method that is supposed to post a new issue to Github and add a message into it. Here is how the unit test would look: public class FooTest { @Test public void createsIssueAndPostsMessage () { Github github = new MkGithub ( \"jeff\" ); github . repos (). create ( Json . createObjectBuilder (). add ( \"name\" , owner ). build () ); new Foo (). doTheThing ( github ); MatcherAssert . assertThat ( github . issues (). get ( 1 ). comments (). iterate (), Matchers . not ( Matchers . emptyIterable ()) ); } } This is much more convenient and compact than traditional mocking via Mockito or a similar framework. Extendable It is based on JSR-353 and uses jcabi-http for HTTP request processing. This combination makes it highly customizable and extendable, when some Github feature is not covered by the library (and there are many of them). For example, you want to get the value of hireable attribute of a User . Class User.Smart doesn't have a method for it. So, here is how you would get it: User user = // get it somewhere // name() method exists in User.Smart, let's use it System . out . println ( new User . Smart ( user ). name ()); // there is no hireable() method there System . out . println ( user . json (). getString ( \"hireable\" )); We're using method json() that returns an instance of JsonObject from JSR-353 (part of Java7). No other library allows such direct access to JSON objects returned by the Github server. Let's see another example. Say, you want to use some feature from Github that is not covered by the API. You get a Request object from Github interface and directly access the HTTP entry point of the server: Github github = new RtGithub ( oauthKey ); int found = github . entry () . uri (). path ( \"/search/repositories\" ). back () . method ( Request . GET ) . as ( JsonResponse . class ) . getJsonObject () . getNumber ( \"total_count\" ) . intValue (); jcabi-http HTTP client is used by jcabi-github . Immutable All classes are truly immutable and annotated with @Immutable . This may sound like a minor benefit, but it was very important for me. I'm using this annotation in all my projects to ensure my classes are truly immutable. Version 0.8 A few days ago we released the latest version 0.8 . It is a major release, that included over 1200 commits. It covers the entire Github API and is supposed to be very stable. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-github </artifactId> </dependency> "},{"title":"Atomic Counters at Stateful.co","url":"/2014/05/18/cloud-autoincrement-counters.html","tags":["stateful","aws","dynamodb"],"date":"2014-05-18 00:00:00 +0000","categories":[],"body":" Amazon DynamoDB is a great NoSQL cloud database. It is cheap, highly reliable and rather powerful. I'm using it in many web systems. There is one feature that it lacks, though — auto-increment attributes. Say that you have a table with a list of messages: +------+----------------------------+ | id | Attributes | +------+----------------------------+ | 205 | author=\"jeff\", text=\"...\" | | 206 | author=\"bob\", text=\"...\" | | 207 | author=\"alice\", text=\"...\" | +------+----------------------------+ Every time you add a new item to the table, a new value of id has to be set. And this has to be done with concurrency in mind. SQL databases like PostgreSQL, Oracle, MySQL and others support auto-increment features. When you add a new record to the table, the value of the primary key is omitted and the server retrieves the next one automatically. If a number of INSERT requests arrive at the same time the server guarantees that the numbers won't be duplicated. However, DynamoDB doesn't have this feature. Instead, DynamoDB has Atomic Counters and Conditional Updates , which are very similar features. Still, they're not exactly the same. In case of an atomic counter, you should create a supplementary table and keep the latest value of id in it. In case of conditional updates, you should retry a few times in case of collisions. To make life easier in a few of my applications, I created a simple web service — stateful.co . It provides a simple atomic counter feature through its RESTful API. First, you create a counter with a unique name. Then, you set its initial value (it is zero by default). And, that's it. Every time you need to obtain a new value for id column in DynamoDB table, you make an HTTP request to stateful.co asking to increment your counter by one and return its next value. stateful.co guarantees that values returned will never duplicate each other — no matter how many clients are using a counter or how fast they request increments simultaneously. Moreover, I designed a small Java SDK for stateful.co . All you need to do is add this java-sdk.jar Maven dependency to your project: <dependency> <groupId> co.stateful </groupId> <artifactId> java-sdk </artifactId> <version> 0.6 </version> </dependency> And, you can use stateful.co counters from Java code: Sttc sttc = new RtSttc ( new URN ( \"urn:github:526301\" ), \"9FF3-41E0-73FB-F900\" ); Counters counters = sttc . counters (); Counter counter = counters . get ( \"foo\" ); long value = counter . incrementAndGet ( 1L ); System . out . println ( \"new value: \" + value ); You can review authentication parameters for RtSttc constructor at stateful.co . The service is absolutely free of charge. "},{"title":"MySQL Maven Plugin","url":"/2014/05/21/mysql-maven-plugin.html","tags":["mysql","maven","java"],"date":"2014-05-21 00:00:00 +0000","categories":[],"body":"I was using MySQL in a few Java web projects and found out there was no Maven plugin that would help me to test my DAO classes against a real MySQL server. There are plenty of mechanisms to mock a database persistence layer both in memory and on disc. However, it is always good to make sure that your classes are tested against a database identical to the one you have in production environment. I've created my own Maven plugin, jcabi-mysql-maven-plugin , that does exactly two things: starts a MySQL server on pre-integration-test phase and shuts it down on post-integration-test . This is how you configure it in pom.xml (see also its full usage instructions ): <project> <build> <plugins> <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> build-helper-maven-plugin </artifactId> <executions> <execution> <goals> <goal> reserve-network-port </goal> </goals> <configuration> <portNames> <portName> mysql.port </portName> </portNames> </configuration> </execution> </executions> </plugin> <plugin> <artifactId> maven-dependency-plugin </artifactId> <executions> <execution> <goals> <goal> unpack </goal> </goals> <configuration> <artifactItems> <artifactItem> <groupId> com.jcabi </groupId> <artifactId> mysql-dist </artifactId> <version> 5.6.14 </version> <classifier> ${mysql.classifier} </classifier> <type> zip </type> <overWrite> false </overWrite> <outputDirectory> ${project.build.directory}/mysql-dist </outputDirectory> </artifactItem> </artifactItems> </configuration> </execution> </executions> </plugin> <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-mysql-maven-plugin </artifactId> <executions> <execution> <id> mysql-test </id> <goals> <goal> classify </goal> <goal> start </goal> <goal> stop </goal> </goals> <configuration> <port> ${mysql.port} </port> <data> ${project.build.directory}/mysql-data </data> </configuration> </execution> </executions> </plugin> <plugin> <artifactId> maven-failsafe-plugin </artifactId> <configuration> <systemPropertyVariables> <mysql.port> ${mysql.port} </mysql.port> </systemPropertyVariables> </configuration> <executions> <execution> <goals> <goal> integration-test </goal> <goal> verify </goal> </goals> </execution> </executions> </plugin> </plugins> </build> [...] </project> There are two plugins configured above. Let's take a look at what each does. build-helper-maven-plugin is reserving a temporary random TCP port, which will be used by MySQL server. We don't want to start a server on its default 3306 port, because there could be another server already running there. Besides that, if we use a hard-coded TCP port, we won't be able to run multiple builds in parallel. Maybe not a big deal when you're developing locally, but in continuous integration environment this can be a problem. That's why we're reserving a TCP port first. maven-dependency-plugin is downloading a MySQL distribution in a zip archive (rather big file, over 300Mb for Linux), and unpacks it. This archive contains exactly the same files as you would use for a traditional MySQL installation. When the archive is unpacked, it is ready to start serving SQL requests as a normal MySQL server. jcabi-mysql-maven-plugin starts a server, binding it to a TCP port reserved randomly. The main responsibility of my Maven plugin is to make sure that MySQL server starts correctly on every platform (Mac OS, Linux, Windows) and stops when it's not needed any more. All the rest is done by the MySQL distribution itself. maven-failsafe-plugin is running unit tests on integration-test phase. Its main difference from maven-surefire-plugin is that it doesn't fail a build when some tests fail. Instead, it saves all failures into supplementary files in target directory and allows the build continue. Later, when we call its verify goal, it will fail a build if there were any errors during its integration-test goal execution. To be precise, this is the order in which Maven will execute configured goals: jcabi-mysql-maven-plugin:classify maven-dependency-plugin:unpack build-helper-maven-plugin:reserve-network-port jcabi-mysql-maven-plugin:start maven-failsafe-plugin:integration-test jcabi-mysql-maven-plugin:stop maven-failsafe-plugin:verify Run mvn clean install and see how it works. If it doesn't work for some reason, don't hesitate to report an issue to Github . Now it's time to create an integration test, which will connect to the temporary MySQL server, create a table there and insert some data into it. This is just an example to show that MySQL server is running and is capable of serving transactions (I'm using jcabi-jdbc ): public class FooITCase { private static final String PORT = System . getProperty ( \"mysql.port\" ); @Test public void worksWithMysqlServer () { Connection conn = DriverManager . getConnection ( String . format ( \"jdbc:mysql://localhost:%s/root?user=root&password=root\" , FooITCase . PORT ) ); new JdbcSession ( conn ) . sql ( \"CREATE TABLE foo (id INT PRIMARY KEY)\" ) . execute (); } } If you're using Hibernate, just create a db.properties file in src/test/resources directory. In that file you would do something like: hibernate.connection.url = jdbc:mysql://localhost:${mysql.port}/root hibernate.connection.username = root hibernate.connection.password = root Maven will replace that ${mysql.port} with the number of reserved TCP port, during resources copying. This operation is called \"resources filtering\", and you can read about it here . That's pretty much it. I'm using jcabi-mysql-maven-plugin in a few projects, and it helps me to stay confident that my code works with a real MySQL server. I'm also using the Liquibase Maven plugin in order to populate an empty server with tables required for the application. Nevertheless, that is a story for the next post :) "},{"title":"Get Rid of Java Static Loggers","url":"/2014/05/23/avoid-java-static-logger.html","tags":["logging","java","slf4j"],"date":"2014-05-23 00:00:00 +0000","categories":[],"body":"This is a very common practice in Java (using LoggerFactory from slf4j ): import org.slf4j.LoggerFactory ; public class Foo { private static final Logger LOGGER = LoggerFactory . getLogger ( Foo . class ); public void save ( String file ) { // save the file if ( Foo . LOGGER . isInfoEnabled ()) { Foo . LOGGER . info ( \"file {} saved successfuly\" , file ); } } } What's wrong with it? Code duplication. This static LOGGER property has to be declared in every class where logging is required. Just a few lines of code, but this is pure noise, as I see it. To make life easier, I created a library about two years ago, jcabi-log , which has a convenient utility class Logger (yes, I know that utility classes are evil ). import com.jcabi.log.Logger ; public class Foo { public void save ( String file ) { // save the file Logger . info ( this , \"file %s saved successfuly\" , file ); } } This looks much cleaner to me and does exactly the same — sends a single log line to the SLF4J logging facility. Besides, it check automatically whether a given logging level is enabled (for performance optimization) and formats the given string using Formatter (same as String.format() ). For convenience, there are also a number of \"decors\" implemented in the library. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-log </artifactId> </dependency> "},{"title":"Object-Oriented Java Adapter of Amazon S3 SDK","url":"/2014/05/26/amazon-s3-java-oop-adapter.html","tags":["aws","s3","java"],"date":"2014-05-26 00:00:00 +0000","categories":[],"body":" I'm a big fan of Amazon Web Services (AWS). I'm using them in almost all of my projects. One of their most popular services is Simple Storage Service (S3) . It is a storage for binary objects (files) with unique names, accessible through HTTP or RESTful API. Using S3 is very simple. You create a \"bucket\" with a unique name, upload your \"object\" into the bucket through their web interface or through RESTful API, and then download it again (either through HTTP or the API.) Amazon ships the Java SDK that wraps their RESTful API. However, this SDK is not object-oriented at all. It is purely imperative and procedural — it just mirrors the API. For example, in order to download an existing object doc.txt from bucket test-1 , you have to do something like this: AWSCredentials creds = new BasicAWSCredentials ( key , secret ); AmazonS3 aws = new AmazonS3Client ( creds ); S3Object obj = aws . getObject ( new GetObjectRequest ( \"test-1\" , \"doc.txt\" ) ); InputStream input = obj . getObjectContent (); String content = IOUtils . toString ( input , \"UTF-8\" ); input . close (); As always, procedural programming has its inevitable disadvantages. To overcome them all, I designed jcabi-s3 , which is a small object-oriented adapter for Amazon SDK. This is how the same object-reading task can be accomplished with jcabi-s3 : Region region = new Region . Simple ( key , secret ); Bucket bucket = region . bucket ( \"test-1\" ); Ocket ocket = bucket . ocket ( \"doc.txt\" ); String content = new Ocket . Text ( ocket ). read (); Why is this approach better? Well, there are a number of obvious advantages. S3 Object is an Object in Java S3 object get its representative in Java. It is not a collection of procedures to be called in order to get its properties (as with AWS SDK). Rather, it is a Java object with certain behaviors. I called them \"ockets\" (similar to \"buckets\"), in order to avoid clashes with java.lang.Object . Ocket is an interface, that exposes the behavior of a real AWS S3 object: read, write, check existence. There is also a convenient decorator Ocket.Text that simplifies working with binary objects: Ocket . Text ocket = new Ocket . Text ( ocket_from_s3 ); if ( ocket . exists ()) { System . out . print ( ocket . read ()); } else { ocket . write ( \"Hello, world!\" ); } Now, you can pass an object to another class, instead of giving it your AWS credentials, bucket name, and object name. You simply pass a Java object, which encapsulates all AWS interaction details. Extendability Through Decoration Since jcabi-s3 exponses all entities as interfaces, they can easily be extended through encapsulation ( Decorator Pattern ). For example, you want your code to retry S3 object read operations a few times before giving up and throwing an IOException (by the way, this is a very good practice when working with web services). So, you want all your S3 reading operations to be redone a few times if first attempts fail. You define a new decorator class, say, RetryingOcket , which encapsulates an original Ocket : public RetryingOcket implements Ocket { private final Ocket origin ; public RetryingOcket ( Ocket ocket ) { this . origin = ocket ; } @Override public void read ( OutputStream stream ) throws IOException { int attempt = 0 ; while ( true ) { try { this . origin . read ( stream ); } catch ( IOException ex ) { if ( attempt ++ > 3 ) { throw ex ; } } } } // same for other methods } Now, everywhere where Ocket is expected you send an instance of RetryingOcket that wraps your original object: foo . process ( new RetryingOcket ( ocket )); Method foo.process() won't see a difference, since it is the same Ocket interface it is expecting. By the way, this retry functionality is implemented out-of-the-box in jcabi-s3 , in com.jcabi.s3.retry package. Easy Mocking Again, due to the fact that all entities in jcabi-s3 are interfaces, they are very easy to mock. For example, your class expects an S3 object, reads its data and calculates the MD5 hash (I'm using DigestUtils from commons-codec ): import com.jcabi.s3.Ocket ; import org.apache.commons.codec.digest.DigestUtils ; public class S3Md5Hash { private final Ocket ocket ; public S3Md5Hash ( Ocket okt ) { this . ocket = okt ; } public hash () throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream (); this . ocket . read ( baos ); return DigestUtils . md5hex ( baos . toByteArray ()); } } Here is how simple a unit test will look (try to create a unit test for a class using AWS SDK and you will see the difference): import com.jcabi.s3.Ocket ; import org.junit.Test ; public class S3Md5HashTest { @Test public void generatesHash () { Ocket ocket = Mockito . mock ( Ocket . class ); Mockito . doAnswer ( new Answer < Void >() { public Void answer ( final InvocationOnMock inv ) throws IOException { OutputStream . class . cast ( inv . getArguments ()[ 0 ]). write ( ' ' ); } } ). when ( ocket ). read ( Mockito . any ( OutputStream . class )); String hash = new S5Md5Hash ( ocket ); Assert . assertEquals ( hash , \"7215ee9c7d9dc229d2921a40e899ec5f\" ); } } I'm using JUnit and Mockito in this test. Immutability All classes in jcabi-s3 are annotated with @Immutable and are truly immutable. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-s3 </artifactId> </dependency> As always, your comments and criticism are welcome as Github issues . "},{"title":"Java Method Logging with AOP and Annotations","url":"/2014/06/01/aop-aspectj-java-method-logging.html","tags":["aop","java","logging","jcabi"],"date":"2014-06-01 00:00:00 +0000","categories":["best","jcg"],"body":"Sometimes, I want to log (through slf4j and log4j ) every execution of a method, seeing what arguments it receives, what it returns and how much time every execution takes. This is how I'm doing it, with help of AspectJ , jcabi-aspects and Java 6 annotations: public class Foo { @Loggable public int power ( int x , int p ) { return Math . pow ( x , p ); } } This is what I see in log4j output: [INFO] com.example.Foo #power(2, 10): 1024 in 12μs [INFO] com.example.Foo #power(3, 3): 27 in 4μs Nice, isn't it? Now, let's see how it works. Annotation with Runtime Retention Annotations is a technique introduced in Java 6. It is a meta-programming instrument that doesn't change the way code works, but gives marks to certain elements (methods, classes or variables). In other words, annotations are just markers attached to the code that can be seen and read. Some annotations are designed to be seen at compile time only — they don't exist in .class files after compilation. Others remain visible after compilation and can be accessed in runtime. For example, @Override is of the first type (its retention type is SOURCE ), while @Test from JUnit is of the second type (retention type is RUNTIME ). @Loggable — the one I'm using in the script above — is an annotation of the second type, from jcabi-aspects . It stays with the bytecode in the .class file after compilation. Again, it is important to understand that even though method power() is annotated and compiled, it doesn't send anything to slf4j so far. It just contains a marker saying \"please, log my execution\". Aspect Oriented Programming (AOP) AOP is a useful technique that enables adding executable blocks to the source code without explicitly changing it. In our example, we don't want to log method execution inside the class. Instead, we want some other class to intercept every call to method power() , measure its execution time and send this information to slf4j. We want that interceptor to understand our @Loggable annotation and log every call to that specific method power() . And, of course, the same interceptor should be used for other methods where we'll place the same annotation in the future. This case perfectly fits the original intent of AOP — to avoid re-implementation of some common behavior in multiple classes. Logging is a supplementary feature to our main functionality, and we don't want to pollute our code with multiple logging instructions. Instead, we want logging to happen behind the scenes. In terms of AOP, our solution can be explained as creating an aspect that cross-cuts the code at certain join points and applies an around advice that implements the desired functionality. AspectJ Let's see what these magic words mean. But, first, let's see how jcabi-aspects implements them using AspectJ (it's a simplified example, full code you can find in MethodLogger.java ): @Aspect public class MethodLogger { @Around ( \"execution(* *(..)) && @annotation(Loggable)\" ) public Object around ( ProceedingJoinPoint point ) { long start = System . currentTimeMillis (); Object result = point . proceed (); Logger . info ( \"#%s(%s): %s in %[msec]s\" , MethodSignature . class . cast ( point . getSignature ()). getMethod (). getName (), point . getArgs (), result , System . currentTimeMillis () - start ); return result ; } } This is an aspect with a single around advice around() inside. The aspect is annotated with @Aspect and advice is annotated with @Around . As discussed above, these annotations are just markers in .class files. They don't do anything except provide some meta-information to those w ho are interested in runtime. Annotation @Around has one parameter, which — in this case — says that the advice should be applied to a method if: its visibility modifier is * ( public , protected or private ); its name is name * (any name); its arguments are .. (any arguments); and it is annotated with @Loggable When a call to an annotated method is to be intercepted, method around() executes before executing the actual method. When a call to method power() is to be intercepted, method around() receives an instance of class ProceedingJoinPoint and must return an object, which will be used as a result of method power() . In order to call the original method, power() , the advice has to call proceed() of the join point object. We compile this aspect and make it available in classpath together with our main file Foo.class . So far so good, but we need to take one last step in order to put our aspect into action — we should apply our advice. Binary Aspect Weaving Aspect weaving is the name of the advice applying process. Aspect weaver modifies original code by injecting calls to aspects. AspectJ does exactly that. We give it two binary Java classes Foo.class and MethodLogger.class ; it gives back three — modified Foo.class , Foo$AjcClosure1.class and unmodified MethodLogger.class . In order to understand which advices should be applied to which methods, AspectJ weaver is using annotations from .class files. Also, it uses reflection to browse all classes on classpath. It analyzes which methods satisfy the conditions from the @Around annotation. Of course, it finds our method power() . So, there are two steps. First, we compile our .java files using javac and get two files. Then, AspectJ weaves/modifies them and creates its own extra class. Our Foo class looks something like this after weaving: public class Foo { private final MethodLogger logger ; @Loggable public int power ( int x , int p ) { return this . logger . around ( point ); } private int power_aroundBody ( int x , int p ) { return Math . pow ( x , p ); } } AspectJ weaver moves our original functionality to a new method, power_aroundBody() , and redirects all power() calls to the aspect class MethodLogger . Instead of one method power() in class Foo now we have four classes working together. From now on, this is what happens behind the scenes on every call to power() : Original functionality of method power() is indicated by the small green lifeline on the diagram. As you see, the aspect weaving process connects together classes and aspects, transferring calls between them through join points. Without weaving, both classes and aspects are just compiled Java binaries with attached annotations. jcabi-aspects jcabi-aspects is a JAR library that contains Loggable annotation and MethodLogger aspect (btw, there are many more aspects and annotations). You don't need to write your own aspect for method logging. Just add a few dependencies to your classpath and configure jcabi-maven-plugin for aspect weaving (get their latest versions in Maven Central ): <project> <dependencies> <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-aspects </artifactId> </dependency> <dependency> <groupId> org.aspectj </groupId> <artifactId> aspectjrt </artifactId> </dependency> </dependencies> <build> <plugins> <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-maven-plugin </artifactId> <executions> <execution> <goals> <goal> ajc </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> Since this weaving procedure takes a lot of configuration effort, I created a convenient Maven plugin with an ajc goal, which does the entire aspect weaving job. You can use AspectJ directly, but I recommend that you use jcabi-maven-plugin . That's it. Now you can use @com.jcabi.aspects.Loggable annotation and your methods will be logged through slf4j. If something doesn't work as explained, don't hesitate to submit a Github issue . "},{"title":"Objects Should Be Immutable","url":"/2014/06/09/objects-should-be-immutable.html","tags":["oop","anti-pattern"],"date":"2014-06-09 00:00:00 +0000","categories":["best","jcg"],"body":"In object-oriented programming, an object is immutable if its state can't be modified after it is created. In Java, a good example of an immutable object is String . Once created, we can't modify its state. We can request that it creates new strings, but its own state will never change. However, there are not so many immutable classes in JDK. Take, for example, class Date . It is possible to modify its state using setTime() . I don't know why the JDK designers decided to make these two very similar classes differently. However, I believe that the design of a mutable Date has many flaws, while the immutable String is much more in the spirit of the object-oriented paradigm. Moreover, I think that all classes should be immutable in a perfect object-oriented world . Unfortunately, sometimes, it is technically not possible due to limitations in JVM. Nevertheless, we should always aim for the best. This is an incomplete list of arguments in favor of immutability: immutable objects are simpler to construct, test, and use truly immutable objects are always thread-safe they help to avoid temporal coupling their usage is side-effect free (no defensive copies) identity mutability problem is avoided they always have failure atomicity they are much easier to cache they prevent NULL references, which are bad Let's discuss the most important arguments one by one. Thread Safety The first and the most obvious argument is that immutable objects are thread-safe. This means that multiple threads can access the same object at the same time, without clashing with another thread. If no object methods can modify its state, no matter how many of them and how often are being called parallel — they will work in their own memory space in stack. Goetz et al. explained the advantages of immutable objects in more details in their very famous book Java Concurrency in Practice (highly recommended). Avoiding Temporal Coupling Here is an example of temporal coupling (the code makes two consecutive HTTP POST requests, where the second one contains HTTP body): 1 2 3 4 5 Request request = new Request ( \"http://example.com\" ); request . method ( \"POST\" ); String first = request . fetch (); request . body ( \"text=hello\" ); String second = request . fetch (); This code works. However, you must remember that the first request should be configured before the second one may happen. If we decide to remove the first request from the script, we will remove the second and the third line, and won't get any errors from the compiler: Request request = new Request ( \"http://example.com\" ); // request.method(\"POST\"); // String first = request.fetch(); request . body ( \"text=hello\" ); String second = request . fetch (); Now, the script is broken although it compiled without errors. This is what temporal coupling is about — there is always some hidden information in the code that a programmer has to remember. In this example, we have to remember that the configuration for the first request is also used for the second one. We have to remember that the second request should always stay together and be executed after the first one. If Request class were immutable, the first snippet wouldn't work in the first place, and would have been rewritten like: final Request request = new Request ( \"\" ); String first = request . method ( \"POST\" ). fetch (); String second = request . method ( \"POST\" ). body ( \"text=hello\" ). fetch (); Now, these two requests are not coupled. We can safely remove the first one, and the second one will still work correctly. You may point out that there is a code duplication. Yes, we should get rid of it and re-write the code: final Request request = new Request ( \"\" ); final Request post = request . method ( \"POST\" ); String first = post . fetch (); String second = post . body ( \"text=hello\" ). fetch (); See, refactoring didn't break anything and we still don't have temporal coupling. The first request can be removed safely from the code without affecting the second one. I hope this example demonstrates that the code manipulating immutable objects is more readable and maintainable, b ecause it doesn't have temporal coupling. Avoiding Side Effects Let's try to use our Request class in a new method (now it is mutable): public String post ( Request request ) { request . method ( \"POST\" ); return request . fetch (); } Let's try to make two requests — the first with GET method and the second with POST: Request request = new Request ( \"http://example.com\" ); request . method ( \"GET\" ); String first = this . post ( request ); String second = request . fetch (); Method post() has a \"side effect\" — it makes changes to the mutable object request . These changes are not really expected in this case. We expect it to make a POST request and return its body. We don't want to read its documentation just to find out that behind the scene it also modifies the request we're passing to it as an argument. Needless to say, such side effects lead to bugs and maintainability issues. It would be much better to work with an immutable Request : public String post ( Request request ) { return request . method ( \"POST\" ). fetch (); } In this case, we may not have any side effects. Nobody can modify our request object, no matter where it is used and how deep through the call stack it is passed by method calls: Request request = new Request ( \"http://example.com\" ). method ( \"GET\" ); String first = this . post ( request ); String second = request . fetch (); This code is perfectly safe and side effect free. Avoiding Identity Mutability Very often, we want objects to be identical if their internal states are the same. Date class is a good example: Date first = new Date ( 1L ); Date second = new Date ( 1L ); assert first . equals ( second ); // true There are two different objects; however, they are equal to each other because their encapsulated states are the same. This is made possible through their custom overloaded implementation of equals() and hashCode() methods. The consequence of this convenient approach being used with mutable objects is that every time we modify object's state it changes its identity: Date first = new Date ( 1L ); Date second = new Date ( 1L ); first . setTime ( 2L ); assert first . equals ( second ); // false This may look natural, until you start using your mutable objects as keys in maps: Map < Date , String > map = new HashMap <>(); Date date = new Date (); map . put ( date , \"hello, world!\" ); date . setTime ( 12345L ); assert map . containsKey ( date ); // false When modifying the state of date object, we're not expecting it to change its identity. We're not expecting to lose an entry in the map just because the state of its key is changed. However, this is exactly what is happening in the example above. When we add an object to the map, its hashCode() returns one value. This value is used by HashMap to place the entry into the internal hash table. When we call containsKey() hash code of the object is different (because it is based on its internal state) and HashMap can't find it in the internal hash table. It is a very annoying and difficult to debug side effects of mutable objects. Immutable objects avoid it completely. Failure Atomicity Here is a simple example: public class Stack { private int size ; private String [] items ; public void push ( String item ) { size ++; if ( size > items . length ) { throw new RuntimeException ( \"stack overflow\" ); } items [ size ] = item ; } } It is obvious that an object of class Stack will be left in a broken state if it throws a runtime exception on overflow. Its size property will be incremented, while items won't get a new element. Immutability prevents this problem. An object will never be left in a broken state because its state is modified only in its constructor. The constructor will either fail, rejecting object instantiation, or succeed, making a valid solid object, which never changes its encapsulated state. For more on this subject, read Effective Java, 2nd Edition by Joshua Bloch. Arguments Against Immutability There are a number of arguments against immutability. “Immutability is not for enterprise systems”. Very often, I hear people say that immutability is a fancy feature, while absolutely impractical in real enterprise systems. As a counter-argument, I can only show some examples of real-life applications that contain only immutable Java objects: jcabi-http , jcabi-xml , jcabi-github , jcabi-s3 , jcabi-dynamo , jcabi-simpledb The above are all Java libraries that work solely with immutable classes/objects. netbout.com and stateful.co are web applications that work solely with immutable objects. “It's cheaper to update an existing object than create a new one”. Oracle thinks that “The impact of object creation is often overestimated and can be offset by some of the efficiencies associated with immutable objects. These include decreased overhead due to garbage collection, and the elimination of code needed to protect mutable objects from corruption.” I agree. If you have some other arguments, please post them below and I'll try to comment. "},{"title":"Avoid String Concatenation","url":"/2014/06/19/avoid-string-concatenation.html","tags":["java","oop","anti-pattern"],"date":"2014-06-19 00:00:00 +0000","categories":[],"body":"This is \"string concatentation\", and it is a bad practice: // bad practice, don't reuse! String text = \"Hello, \" + name + \"!\" ; Why? Some may say that it is slow, mostly because parts of the resulting string are copied multiple times. Indeed, on every + operator, String class allocates a new block in memory and copies everything it has into it; plus a suffix being concatenated. This is true, but this is not the point here. Actually, I don't think performance in this case is a big issue. Moreover, there were multiple experiments showing that concatenation is not that slow when compared to other string building methods and sometimes is even faster. Some say that concatenated strings are not localizable because in different languages text blocks in a phrase may be positioned in a different order. The example above can't be translated to, say, Russian, where we would want to put a name in front of \"привет\". We will need to localize the entire block of code, instead of just translating a phrase. However, my point here is different. I strongly recommend avoiding string concatenation because it is less readable than other methods of joining texts together. Let's see these alternative methods. I'd recommend three of them (in order of preference): String.format() , Apache StringUtils and Guava Joiner . There is also a StringBuilder , but I don't find it as attractive as StringUtils . It is a useful builder of strings, but not a proper replacer or string concatenation tool when readability is important. String.format() String.format() is my favorite option. It makes text phrases easy to understand and modify. It is a static utility method that mirrors sprintf() from C. It allows you to build a string using a pattern and substitutors: String text = String . format ( \"Hello, %s!\" , name ); When the text is longer, the advantages of the formatter become much more obvious. Look at this ugly code: String msg = \"Dear \" + customer . name () + \", your order #\" + order . number () + \" has been shipped at \" + shipment . date () + \"!\" ; This one looks much more beautiful doesn’t it: String msg = String . format ( \"Dear %1$s, your order #%2$d has been shipped at %3$tR!\" , customer . name (), order . number (), shipment . date () ); Please note that I'm using argument indexes in order to make the pattern even more localizable. Let's say, I want to translate it to Greek. This is how will it look: Αγαπητέ %1$s, στις %3$tR στείλαμε την παραγγελία σου με αριθμό #%2$d! I'm changing the order of substitutions in the pattern, but not in the actual list of methods arguments. Apache StringUtils.join() When the text is rather long (longer than your screen width), I would recommend that you use the utility class StringUtils from Apache commons-lang3 : import org.apache.commons.lang3.StringUtils ; String xml = StringUtils . join ( \"<?xml version='1.0'?>\" , \"<html><body>\" , \"<p>This is a test XHTML document,\" , \" which would look ugly,\" , \" if we would use a single line,\" \" or string concatenation or String format().</p>\" \"</body></html>\" ); The need to include an additional JAR dependency to your classpath may be considered a downside with this method (get its latest versions in Maven Central ): <dependency> <groupId> org.apache.commons </groupId> <artifactId> commons-lang3 </artifactId> </dependency> Guava Joiner Similar functionality is provided by Joiner from Google Guava : import com.google.common.base.Joiner ; String text = Joiner . on ( '' ). join ( \"WE HAVE BUNNY.\\n\" , \"GATHER ONE MILLION DOLLARS IN UNMARKED \" , \"NON-CONSECUTIVE TWENTIES.\\n\" , \"AWAIT INSTRUCTIONS.\\n\" , \"NO FUNNY STUFF\" ); It is a bit less convenient than StringUtils since you always have to provide a joiner (character or a string placed between text blocks). Again, a dependency is required in this case: <dependency> <groupId> com.google.guava </groupId> <artifactId> guava </artifactId> </dependency> Yes, in most cases, all of these methods work slower than a plain simple concatenation. However, I strongly believe that computers are cheaper than people . What I mean is that the time spent by programmers understanding and modifying ugly code is much more expensive than a cost of an additional server that will make beautifully written code work faster. If you know any other methods of avoiding string concatenation, please comment below. "},{"title":"Limit Java Method Execution Time","url":"/2014/06/20/limit-method-execution-time.html","tags":["java","aop"],"date":"2014-06-20 00:00:00 +0000","categories":[],"body":" Say, you want to allow a Java method to work for a maximum of five seconds and want an exception to be thrown if the timeframe is exceeded. Here is how you can do it with jcabi-aspects and AspectJ : public class Resource { @Timeable ( limit = 5 , unit = TimeUnit . SECONDS ) public String load ( URL url ) { return url . openConnection (). getContent (); } } Keep in mind that you should weave your classes after compilation, as explained here . Let's discuss how this actually works, but first, I recommend you read this post , which explains how AOP aspects work together with Java annotations. Due to @Timeable annotation and class weaving, every call to a method load() is intercepted by an aspect from jcabi-aspects . That aspect starts a new thread that monitors the execution of a method every second, checking whether it is still running. If the method runs for over five seconds, the thread calls interrupt() on the method's thread. Despite a very common expectation that a thread should be terminated immediately on that call, it is not happening at all. This article explains the mechanism in more detail. Let's discuss it briefly: interrupt() sets a marker in a thread; The thread checks interrupted() as often as it can; If the marker is set, the thread stops and throws InterruptedException This method will not react to interrupt() call and will work until JVM is killed (very bad design): public void work () { while ( true ) { // do something } } This is how we should refactor it in order to make sensitive to interruption requests: public void work () { while ( true ) { if ( Thread . interruped ()) { throw new InterruptedException (); } // do something } } In other words, your method can only stop itself. Nothing else can do it. The thread it is running in can't be terminated by another thread. The best thing that the other thread can do is to send your thread a \"message\" (through interrupt() method) that it's time to stop. If your thread ignores the message, nobody can do anything. Most I/O operations in JDK are designed this way. They check the interruption status of their threads while waiting for I/O resources. Thus, use @Timeable annotation, but keep in mind that there could be situations when a thread can't be interrupted. "},{"title":"CasperJS Tests in Maven Build","url":"/2014/06/21/casperjs-with-maven.html","tags":["maven","casperjs","phantomjs","testing"],"date":"2014-06-21 00:00:00 +0000","categories":[],"body":"I'm a big fan of automated testing in general and integration testing in particular. I strongly believe that effort spent on writing tests are direct investments into quality and stability of the product under development. CasperJS is a testing framework on top of PhantomJS , which is a headless browser. Using CasperJS, we can ensure that our application responds correctly to requests sent by a regular web browser. This is a sample CasperJS test, which makes an HTTP request to a home page of a running WAR application and asserts that the response has 200 HTTP status code: casper . test . begin ( 'home page can be rendered' , function ( test ) { casper . start ( casper . cli . get ( 'home' ), // URL of home page function () { test . assertHttpStatus ( 200 ); } ); casper . run ( function () { test . done (); } ); } ); I keep this test in the src/test/casperjs/home-page.js file. Let's see how CasperJS can be executed automatically on every Maven build. Here is the test scenario, implemented with a combination of Maven plugins: Install PhantomJS Install CasperJS Reserve a random TCP port Start Tomcat on that TCP port (with WAR inside) Run CasperJS tests and point them to the running Tomcat Shutdown Tomcat I'm using a combination of plugins. Let's go through the steps one by one. BTW, I'm not showing plugin versions in the examples below, primarily because most of them are in active development. Check their versions at Maven Central (yes, all of them are available there). 1. Install PhantomJS First of all, we have to download the PhantomJS executable. It is a platform-specific binary. Thanks to Kyle Lieber , we have an off-the-shelf Maven plugin: phantomjs-maven-plugin that understands what the current platform is and downloads the appropriate binary automatically, placing it into the target directory. <plugin> <groupId> com.github.klieber </groupId> <artifactId> phantomjs-maven-plugin </artifactId> <executions> <execution> <goals> <goal> install </goal> </goals> </execution> </executions> <configuration> <version> 1.9.2 </version> </configuration> </plugin> The exact name of the downloaded binary is stored in the ${phantomjs.binary} Maven property. 2. Install CasperJS Unfortunately, there is no similar plugin for the CasperJS installation (at least I haven't found any as of yet). That's why I'm using plain old git (you should have it installed on your build machine). <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> exec-maven-plugin </artifactId> <executions> <execution> <id> casperjs-install </id> <phase> pre-integration-test </phase> <goals> <goal> exec </goal> </goals> <configuration> <executable> git </executable> <arguments> <argument> clone </argument> <argument> --depth=1 </argument> <argument> https://github.com/n1k0/casperjs.git </argument> <argument> ${project.build.directory}/casperjs </argument> </arguments> </configuration> </execution> </executions> </plugin> 3. Reserve TCP Port I need to obtain a random TCP port where Tomcat will be started. The port has to be available on the build machine. I want to be able to run multiple Maven builds in parallel, so that's why I get a random port on every build. In other examples, you may see people using fixed port numbers, like 5555 or something similar. This is a very bad practice. Always reserve a new random port when you need it. <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> build-helper-maven-plugin </artifactId> <executions> <execution> <id> tomcat-port </id> <goals> <goal> reserve-network-port </goal> </goals> <configuration> <portNames> <portName> tomcat.port </portName> </portNames> </configuration> </execution> </executions> </plugin> The plugin reserves a port and sets it value to the ${tomcat.port} Maven property. 4. Start Tomcat Now, it's time to start Tomcat with the WAR package inside. I'm using tomcat7-maven-plugin that starts a real Tomcat7 server and configures it to serve on the port reserved above. <plugin> <groupId> org.apache.tomcat.maven </groupId> <artifactId> tomcat7-maven-plugin </artifactId> <configuration> <path> / </path> </configuration> <executions> <execution> <id> start-tomcat </id> <phase> pre-integration-test </phase> <goals> <goal> run-war-only </goal> </goals> <configuration> <port> ${tomcat.port} </port> <fork> true </fork> </configuration> </execution> </executions> </plugin> Due to the option fork being set to true , Tomcat7 continues to run when the plugin execution finishes. That's exactly what I need. 5. Run CasperJS Now, it's time to run CasperJS. Even though there are some plugins exist for this, I'm using plain old exec-maven-plugin , mostly because it is more configurable. <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> exec-maven-plugin </artifactId> <executions> <execution> <id> casperjs-test </id> <phase> integration-test </phase> <goals> <goal> exec </goal> </goals> <configuration> <executable> ${project.build.directory}/casperjs/bin/casperjs </executable> <workingDirectory> ${basedir} </workingDirectory> <arguments> <argument> test </argument> <argument> --verbose </argument> <argument> --no-colors </argument> <argument> --concise </argument> <argument> --home=http://localhost:${tomcat.port} </argument> <argument> ${basedir}/src/test/casperjs </argument> </arguments> <environmentVariables> <PHANTOMJS_EXECUTABLE> ${phantomjs.binary} </PHANTOMJS_EXECUTABLE> </environmentVariables> </configuration> </execution> </executions> </plugin> The environment variable PHANTOMJS_EXECUTABLE is the undocumented feature that makes this whole scenario possible. It configures the location of the PhantomJS executable, which was downloaded a few steps above. 6. Shutdown Tomcat In the last step, I shut down the Tomcat server. <plugin> <groupId> org.apache.tomcat.maven </groupId> <artifactId> tomcat7-maven-plugin </artifactId> <executions> <execution> <id> stop-tomcat </id> <phase> post-integration-test </phase> <goals> <goal> shutdown </goal> </goals> </execution> </executions> </plugin> Real Example If you want to see how this all works in action, take a look at stateful.co . It is a Java Web application hosted at CloudBees . Its source code is open and available in Github . Its pom.xml contains exactly the same configurations explained above, but joined together. If you have any questions, please don't hesitate to ask below. "},{"title":"Deploy Jekyll to Github Pages","url":"/2014/06/24/jekyll-github-deploy.html","tags":["jekyll","github","ruby"],"date":"2014-06-24 00:00:00 +0000","categories":[],"body":"This blog is written in Jekyll and is hosted at Github Pages . It uses half a dozen custom plugins, which are not allowed there . Here is how I deploy it: $ jgd That's it. jgd is my Ruby gem (stands for \"Jekyll Github Deploy\"), which does the trick. Here is what it does : It clones your existing repository from the current directory to a temporary one (guessing the URL of the repo from .git/config file). Runs jekyll build in that temporary directory, which saves the output in another temporary directory. Checks out gh-pages branch or creates one if it doesn't exist. Copies the content of the site built by jekyll build into the branch, thus overwriting existing files, commits and pushes to Github. Cleans up all temporary directories. Using this gem is very easy. Just install it with gem install jgd and then run in the root directory of your Jekyll blog. What is important is that your Jekyll site files be located in the root directory of the repository. Just as they do on this blog; see its sources in Github . You can easily integrate jgd with Travis. See .travis.yml of this blog. Full documentation about the gem is located here . "},{"title":"XML+XSLT in a Browser","url":"/2014/06/25/xml-and-xslt-in-browser.html","tags":["xslt","java","restful"],"date":"2014-06-25 00:00:00 +0000","categories":[],"body":"Separating data and their presentation is a great concept. Take HTML and CSS for example. HTML is supposed to have pure data and CSS is supposed to format that data in order to make it readable by a human. Years ago, that was probably the intention of HTML/CSS, but in reality it doesn't work like that. Mostly because CSS is not powerful enough. We still have to format our data using HTML tags, while CSS can help slightly with positioning and decorating. On the other hand, XML with XSLT implements perfectly the idea of separating data and presentation. XML documents, like HTML, are supposed to contain data only without any information about positioning or formatting. XSL stylesheets position and decorate the data. XSL is a much more powerful language. That's why it's possible to avoid any formatting inside XML. The latest versions of Chrome, Safari, FireFox and IE all support this mechanism. When a browser retrieves an XML document from a server, and the document has an XSL stylesheet associated with it — the browser transforms XML into HTML on-fly. Working Example Let's review a simple Java web application that works this way. It is using ReXSL framework that makes this mechanism possible. In the next post, I'll explain how ReXSL works. For now, though, let's focus on the idea of delivering bare data in XML and formatting it with an XSL stylesheet. Open http://www.stateful.co — it is a collection of stateful web primitives, explained in the Atomic Counters at Stateful.co article. Open it in Chrome or Safari. When you do, you should see a normal web page with a logo, some text, some links, a footer, etc. Now check its sources (I assume you know how to do this). This is approximately what you will see (I assume you understand XML, if not, start learning it immediately): <?xml-stylesheet type='text/xsl' href='/xsl/index.xsl'?> <page date= \"2014-06-15T15:30:49.521Z\" ip= \"10.168.29.135\" > <menu> home </menu> <documentation> .. some text here .. </documentation> <version> <name> 1.4 </name> <revision> 5c7b5af </revision> <date> 2014-05-29 07:58 </date> </version> <links> <link href= \"...\" rel= \"rexsl:google\" type= \"text/xml\" /> <link href= \"...\" rel= \"rexsl:github\" type= \"text/xml\" /> <link href= \"...\" rel= \"rexsl:facebook\" type= \"text/xml\" /> </links> <millis> 70 </millis> </page> As you see, it is a proper XML document with attributes, elements and data. It contains absolutely no information about how its elements have to be presented to an end-user. Actually, this document is more suitable for machine parsing instead of reading by a human. The document contains data, which is important for its requestor. It's up to the requestor on how to render the data or to not render it at all. Its second line associates the document with the XSL stylesheet /xsl/index.xsl that is loaded by the browser separately: <?xml-stylesheet type='text/xsl' href='/xsl/index.xsl'?> Open developer tools in Chrome and you will see that right after the page is loaded, the browser loads the XSL stylesheet and then all other resources including a few CSS stylesheets, jQuery and an SVG logo: index.xsl includes layout.xsl , that's why it is loaded right after. Let's consider an example of index.xsl (in reality it is much more complex, check layout.xsl . For example: <xsl:stylesheet version= \"2.0\" xmlns:xsl= \"http://www.w3.org/1999/XSL/Transform\" xmlns= \"http://www.w3.org/1999/xhtml\" > <xsl:template match= \"page\" > <html> <body> <p> Current version of the application is <xsl:value-of select= \"version/name\" /> </p> </body> </html> </xsl:template> </xsl:stylesheet> I think it's obvious how the HTML page will look like after applying this XSL stylesheet to our XML document. For me, this XSL looks clean and easy to understand. However, I often hear people say that XSLT is a hard-to-understand programming language. I don't find it hard to understand at all. Of course, I'm not using all of its features. But, for simple page rendering, all I need to know are a few simple commands and the principle of XML transformation. Why Not a Templating Engine? Now, why is this approach better than all that widely use Java templating engines, including JSP , JSF , Velocity , FreeMarker , Tiles , etc? Well, I see a number of reasons. But, the most important are: Web UI and API are same pages . There is no need to develop separate pages for RESTful API — Web user interface, being accessed by a computer, is an API. In my experience, this leads to massive avoidance of code duplication. XSL is testable by itself without a server . In order to test how our web site will look with certain data, we just create a new XML document with necessary test data, associate it with an XSL and open it in a browser. We can also modify XML and refresh the page in browser. This makes the work of HTML/CSS designer much easier and independent of programmers. XSL is a powerful functional language . Compared with all other templating engines, which look mostly like workarounds, XSL is a complete and well-designed environment. Writing XSL (after you get used to its syntax and programming concepts) is a pleasure in itself. You're not injecting instructions into a HTML document (like in JSP and all others). Instead, you are programming transformation of data into presentation — a different mindset and much better feeling. XML output is perfectly testable . A controller in MVC that generates an XML document with all data required for the XSL stylesheet can easily be tested in a single unit test using simple XPath expressions. Testing of a controller that injects data into a templating engine is a much more complex operation — even impossible sometimes. I'm also writing in PHP and Ruby. They have exactly the same problems — even though their templating engines are much more powerful due to the interpretation nature of the languages. Is It Fully Supported? Everything would be great if all browsers would support XML+XSL rendering. However, this is far from being true. Only the latest versions of modern browsers support XSL. Check this comparison done by Julian Reschke. Besides that, XSLT 2.0 is not supported at all. There is a workaround, though. We can understand which browser is making a request (via its User-Agent HTTP header) and transform XML into HTML on the server side. Thus, for modern browsers that support XSL, we will deliver XML and for all others — HTML. This is exactly how ReXSL framework works. Open http://www.stateful.co in Internet Explorer and you will see an HTML document, not an XML document as is the case with Chrome. In one of the next posts, I'll explain ReXSL framework . Read this one, it continues the discussion of this subject: RESTful API and a Web Site in the Same URL "},{"title":"SASS in Java Webapp","url":"/2014/06/26/sass-in-java-webapp.html","tags":["java","sass"],"date":"2014-06-26 00:00:00 +0000","categories":[],"body":"SASS is a powerful and very popular language for writing CSS style sheets. This is how I'm using SASS in my Maven projects. First, I change the extensions of .css files to .scss and move them from src/main/webapp/css to src/main/scss . Then, I configure the sass-maven-plugin (get its latest versions in Maven Central ): <plugin> <groupId> org.jasig.maven </groupId> <artifactId> sass-maven-plugin </artifactId> <executions> <execution> <id> generate-css </id> <phase> generate-resources </phase> <goals> <goal> update-stylesheets </goal> </goals> <configuration> <sassSourceDirectory> ${basedir}/src/main/scss </sassSourceDirectory> <destination> ${project.build.directory}/css </destination> </configuration> </execution> </executions> </plugin> The SASS compiler will compile .scss files from src/main/scss and place .css files into target/css . Then, I configure the minify-maven-plugin to compress/minify the style sheets produced by the SASS compiler: <plugin> <groupId> com.samaxes.maven </groupId> <artifactId> minify-maven-plugin </artifactId> <configuration> <charset> UTF-8 </charset> <nosuffix> true </nosuffix> <webappTargetDir> ${project.build.directory}/css-min </webappTargetDir> </configuration> <executions> <execution> <id> minify-css </id> <goals> <goal> minify </goal> </goals> <configuration> <webappSourceDir> ${project.build.directory} </webappSourceDir> <cssSourceDir> css </cssSourceDir> <cssSourceIncludes> <include> *.css </include> </cssSourceIncludes> <skipMerge> true </skipMerge> </configuration> </execution> </executions> </plugin> Minified .css files will be placed into target/css-min . The final step is to configure the maven-war-plugin to pick up .css files and package them into the final WAR archive: <plugin> <artifactId> maven-war-plugin </artifactId> <configuration> [..other configuration options..] <webResources combine.children= \"append\" > <resource> <directory> ${project.build.directory}/css-min </directory> </resource> </webResources> </configuration> </plugin> That's it. "},{"title":"Custom Pygments Lexer in Jekyll","url":"/2014/06/29/custom-lexer-in-jekyll.html","tags":["jekyll","pygments"],"date":"2014-06-29 00:00:00 +0000","categories":[],"body":"I needed to create a custom syntax highlighting for requs.org on which I'm using Jekyll for site rendering. This is how my code blocks look in markdown pages: { % highlight requs %} User is a \"human being\". { % endhighlight %} I created a custom Pygments lexer : from pygments.lexer import RegexLexer from pygments.token import Punctuation , Text , Keyword , Name , String from pygments.util import shebang_matches class RequsLexer ( RegexLexer ): name = 'requs' aliases = [ 'requs' ] tokens = { 'root' : [ ( r'\"[^\"]+\"' , String ), ( r'\"\"\".+\"\"\"' , Text ), ( r'\\b(needs|includes|requires|when|fail|since|must|is|a|the)\\s*\\b' , Keyword ), ( r'([A-Z][a-z]+)+' , Name ), ( r'[,;:]' , Punctuation ), ], } def analyse_text ( text ): return shebang_matches ( text , r'requs' ) Then, I packaged it for easy_install and installed locally: $ easy_install src/requs_pygment Processing requs_pygment Running setup.py -q bdist_egg --dist-dir /Volumes/ssd2/code/requs/src/requs_pygment/egg-dist-tmp-ISj8Nx zip_safe flag not set ; analyzing archive contents... Adding requs-pygment 0.1 to easy-install.pth file Installed /Library/Python/2.7/site-packages/requs_pygment-0.1-py2.7.egg Processing dependencies for requs-pygment == 0.1 Finished processing dependencies for requs-pygment == 0.1 It's done. Now I run jekyll build and my syntax is highlighted according to the custom rules I specified in the lexer. "},{"title":"How to Read MANIFEST.MF Files","url":"/2014/07/03/how-to-read-manifest-mf.html","tags":["java","jcabi"],"date":"2014-07-03 00:00:00 +0000","categories":[],"body":" Every Java package (JAR, WAR, EAR, etc.) has a MANIFEST.MF file in the META-INF directory. The file contains a list of attributes, which describe this particular package. For example: Manifest-Version: 1.0 Created-By: 1.7.0_06 (Oracle Corporation) Main-Class: MyPackage.MyClass When your application has multiple JAR dependencies, you have multiple MANIFEST.MF files in your class path. All of them have the same location: META-INF/MANIFEST.MF . Very often it is necessary to go through all of them in runtime and find the attribute by its name. jcabi-manifests makes it possible with a one-liner: import com.jcabi.manifests.Manifests ; String created = Manifests . read ( \"Created-By\" ); Let's see why you would want to read attributes from manifest files, and how it works on a low level. Package Versioning When you package a library or even a web application, it is a good practice to add an attribute to its MANIFEST.MF with the package version name and build number. In Maven, maven-jar-plugin can help you (almost the same configuration for maven-war-plugin ): <plugin> <artifactId> maven-jar-plugin </artifactId> <configuration> <archive> <manifestEntries> <Foo-Version> ${project.version} </Foo-Version> <Foo-Hash> ${buildNumber} </Foo-Hash> </manifestEntries> </archive> </configuration> </plugin> buildnumber-maven-plugin will help you to get ${buildNumber} from Git, SVN or Mercurial: <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> buildnumber-maven-plugin </artifactId> <executions> <execution> <goals> <goal> create </goal> </goals> </execution> </executions> </plugin> After all these manipulations, MANIFEST.MF , in your JAR will contain these two extra lines (on top of all others added there by Maven by default): Foo-Version: 1.0-SNAPSHOT Foo-Hash: 7ef4ac3 In runtime, you can show these values to the user to help him understand which version of the product he is working with at any given moment. Look at stateful.co , for example. At the bottom of its front page, you see the version number and Git hash. They are retrieved from MANIFEST.MF of the deployed WAR package, on every page click. Credentials Although this may be considered as a bad practice (see Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation by Jez Humble and David Farley), sometimes it is convenient to package production credentials right into the JAR/WAR archive during the continuous integration/delivery cycle. For example, you can encode your PostgreSQL connection details right into MANIFEST.MF : <plugin> <artifactId> maven-war-plugin </artifactId> <configuration> <archive> <manifestEntries> <Pgsql> jdbc:postgresql://${pg.host}:${pg.port}/${pg.db} </Pgsql> </manifestEntries> </archive> </configuration> </plugin> Afterwards, you can retrieve them in runtime using jcabi-manifests : String url = Manifests . read ( \"Pgsql\" ); If you know of any other useful purposes for MANIFEST.MF , let me know :) "},{"title":"Liquibase with Maven","url":"/2014/07/20/liquibase-in-maven.html","tags":["liquibase","maven","java"],"date":"2014-07-20 00:00:00 +0000","categories":[],"body":"Liquibase is a migration management tool for relational databases. It versionalizes schema and data changes in a database; similar to the way Git or SVN works for source code. Thanks to their Maven plugin , Liquibase can be used as a part of a build automation scenario. Maven Plugin Let's assume you're using MySQL (PostgreSQL or any other database configuration will be very similar.) Add liquibase-maven-plugin to your pom.xml (get its latest version in Maven Central ): <project> [...] <build> [...] <plugins> <plugin> <groupId> org.liquibase </groupId> <artifactId> liquibase-maven-plugin </artifactId> <configuration> <changeLogFile> ${basedir}/src/main/liquibase/master.xml </changeLogFile> <driver> com.mysql.jdbc.Driver </driver> <url> jdbc:mysql://${mysql.host}:${mysql.port}/${mysql.db} </url> <username> ${mysql.login} </username> <password> ${mysql.password} </password> </configuration> </plugin> </plugins> </build> </project> To check that it works, run mvn liquibase:help . I would recommend you keep database credentials in settings.xml and in their respective profiles. For example: <settings> <profiles> <profile> <id> production </id> <properties> <mysql.host> db.example.com </mysql.host> <mysql.port> 3306 </mysql.port> <mysql.db> example </mysql.db> </properties> </profile> <profile> <id> test </id> <properties> <mysql.host> test-db.example.com </mysql.host> <mysql.port> 3306 </mysql.port> <mysql.db> example-db </mysql.db> </properties> </profile> </profiles> </settings> When you run Maven, don't forget to turn on one of the profiles. For example: mvn -Pproduction . Initial Schema I assume you already have a database with a schema (tables, triggers, views, etc.) and some data. You should \"reverse engineer\" it and create an initial schema file for Liquibase. In other words, we should inform Liquibase where we are at the moment, so that it starts to apply changes from this point. Maven plugin doesn't support it, so you will have to run Liquibase directly. But, it's not that difficult. First, run mvn liquibase:help in order to download all artifacts. Then, replace placeholders with your actual credentials: $ java -jar ~/.m2/repository/org/liquibase/liquibase-core/3.1.1/liquibase-core-3.1.1.jar \\ --driver = com.mysql.jdbc.Driver \\ --url = jdbc:mysql://db.example.com:3306/example \\ --username = example --password = example \\ generateChangeLog > src/main/liquibase/2014/000-initial-schema.xml Liquibase will analyze your current database schema and copy its own schema into src/main/liquibase/2014/000-initial-schema.xml . Master Changeset Now, create XML master changeset and save it to src/main/liquibase/master.xml : <databaseChangeLog xmlns= \"http://www.liquibase.org/xml/ns/dbchangelog\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-2.0.xsd\" > <includeAll path= \"src/main/liquibase/2014\" /> </databaseChangeLog> It is an entry point for Liquibase. It starts from this file and loads all other changesets available in src/main/liquibase/2014 . They should be either .xml or .sql . I recommend that you use XML mostly because it is easier to maintain and works faster. Incremental Changesets Let's create a simple changeset, which adds a new column to an existing table: <databaseChangeLog xmlns= 'http://www.liquibase.org/xml/ns/dbchangelog' xmlns:xsi= 'http://www.w3.org/2001/XMLSchema-instance' xsi:schemaLocation= 'http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-2.0.xsd' > <changeSet id= \"002\" author= \"Yegor\" > <sql> ALTER TABLE user ADD COLUMN address VARCHAR(1024); </sql> </changeSet> </databaseChangeLog> We save this file we in src/main/liquibase/2014/002-add-user-address.xml . In big projects, you can name your files by the names of the tickets they are produced in. For example, 045-3432.xml , which means changeset number 45 coming from ticket #3432. The important thing is to have this numeric prefix in front of file names, in order to sort them correctly. We want changes to be applied in their correct chronological order. That's it. We're ready to run mvn liquibase:update -Pproduction and our production database will be updated — a new column will be added to the user table. Also, see how MySQL Maven Plugin can help you to automate integration testing of database-connected classes. "},{"title":"Master Branch Must Be Read-Only","url":"/2014/07/21/read-only-master-branch.html","tags":["rultor","devops","mgmt"],"date":"2014-07-21 00:00:00 +0000","categories":["best"],"body":"Continuous integration is easy. Download Jenkins, install, create a job, click the button, and get a nice email saying that your build is broken (I assume your build is automated). Then, fix broken tests (I assume you have tests), and get a much better looking email saying that your build is clean. Then, tweet about it, claiming that your team is using continuous integration. Then, in a few weeks, start filtering out Jenkins alerts, into their own folder, so that they don't bother you anymore. Anyway, your team doesn't have the time or desire to fix all unit tests every time someone breaks them. After all, we all know that unit testing is not for a team working with deadlines, right? Wrong. Continuous integration can and must work. What is Continuous Integration? Nowadays, software development is done in teams. We develop in feature branches and isolate changes while they are in development. Then, we merge branches into master . After every merge, we test the entire product, executing all available unit and integration tests. This is called continuous integration (aka \"CI\"). Sometimes, some tests fail. When this happens, we say that our \"build is broken\". Such a failure is a positive side effect of quality control because it raises a red flag immediately after an error gets into master . It is a well-known practice, when fixing that error becomes a top priority for its author and the entire team. The error should be fixed right after a red flag is raised by the continuous integration server. Continuous Delivery by Jez Humble et. al. explains this approach perfectly in Chapter 7, pages 169–186. There are a few good tools on the market, which automate DevOps procedures. Some of them are open source, you can download and install them on your own servers. For example: Jenkins , Go , and CruiseControl . Some of them are available as a service in cloud, such as: Travis , Drone , Wercker , and many others. Why Continuous Integration Doesn't Work? CI is great, but the bigger the team (and the code base), the more often builds get broken. And, the longer it takes to fix them. I've seen many examples where a hard working team starts to ignore red flags, raised by Jenkins, after a few weeks or trying to keep up. The team simply becomes incapable of fixing all errors in time. Mostly because the business has other priorities. Product owners do not understand the importance of a \"clean build\" and technical leaders can't buy time for fixing unit tests. Moreover, the code that broke them was already in master and, in most cases, has been already deployed to production and delivered to end-users. What's the urgency of fixing some tests if business value was already delivered? In the end, most development teams don't take continuous integration alerts seriously. Jenkins or Travis are just fancy tools for them that play no role in the entire development and delivery pipeline. No matter what continuous integration server says, we still deliver new features to our end-users. We'll fix our build later. And it's only logical . What Is a Solution? Four years ago, in 2010, I published an article in php|Architect called \"Prevent Conflicts in Distributed Agile PHP Projects\". In the article, a solution was proposed (full article in PDF ) for Subversion and PHP. Since that time, I used experimentally that approach in multiple open source projects and a few commercial ones with PHP, Java, Ruby and JavaScript, Git and Subversion. In all cases, my experience was only positive, and that's why rultor.com was born (later about that though). So, the solution is simple — prohibit anyone from merging anything into master and create a script that anyone can call. The script will merge, test, and commit. The script will not make any exceptions. If any branch is breaking at even one unit test, the entire branch will be rejected. In other words, we should raise that red flag before the code gets into master . We should put the blame for broken tests on the shoulders of its author. Say, I'm developing a feature in my own branch. I finished the development and broke a few tests, accidentally. It happens, we all make mistakes. I can't merge my changes into master . Git simply rejects my push , because I don't have the appropriate permissions. All I can do is call a magic script, asking it to merge my branch. The script will try to merge, but before pushing into master , it will run all tests. And if any of them break, my branch will be rejected. My changes won't be merged. Now it's my responsibility — to fix them and call the script again. In the beginning, this approach slows down the development, because everybody has to start writing cleaner code. At the end, though, this method pays off big time. Pre-flight Builds Some CI servers offer pre-flight builds feature, which means testing branches before they get merged into master . Travis, for example, has this feature and it is very helpful. When you make a new commit to a branch, Travis immediately tries to build it, and reports in Github pull request, if there are problems. Pay attention, pre-flight builds don't merge. They just check whether your individual branch is clean. After merge, it can easily break master . And, of course, this mechanism doesn't guarantee that no collaborators can commit directly to master , breaking it accidentally. Pre-flight builds are a preventive measure, but do not solve the problem entirely. Rultor.com In order to start working as explained above, all you have to do is to revoke write permissions to master branch (or /trunk , in Subversion). Unfortunately, this is not possible in Github. The only solution is to work through forks and pull requests only. Simply remove everybody from the list of \"collaborators\" and they will have to submit changes through pull requests. Then, start using Rultor.com , which will help you to test, merge and push every pull request. Basically, Rultor is the script we were talking about above. It is available as a free cloud service. ps. A short version of this article is also published at devops.com "},{"title":"Rultor.com, a Merging Bot","url":"/2014/07/24/rultor-automated-merging.html","tags":["rultor","devops"],"date":"2014-07-24 00:00:00 +0000","categories":[],"body":" You get a Github pull request. You review it. It looks correct — it's time to merge it into master . You post a comment in it, asking @rultor to test and merge. Rultor starts a new Docker container, merges the pull request into master , runs all tests and, if everything looks clean — merges, pushes, and closes the request. Then, you ask @rultor to deploy the current version to production environment. It checks out your repository, starts a new Docker container, executes your deployment scripts and reports to you right there in the Github issue. Why not Jenkins or Travis? There are many tools on the market, which automate continuous integration and continuous delivery (let's call them DevOps). For example, downloadable open-source Jenkins and hosted Travis both perform these tasks. So, why do we need one more? Well, there are three very important features that we need for our projects, but we can't find all of them in any of the DevOps tools currently available on the market: Merging . We make master branch read-only in our projects, as this article recommends. All changes into master we pass through a script that validates them and merges. Docker . Every build should work in its own Docker container, in order to simplify configuration, isolate resources and make errors easily reproduceable. Tell vs. Trigger . We need to communicate with DevOps tool through commands, right from our issue tracking system (Github issues, in most projects). All existing DevOps systems trigger builds on certain conditions. We need our developers to be able to talk to the tool, through human-like commands in the tickets they are working with. A combination of these three features is what differs Rultor from all other existing systems. How Rultor Merges Once Rultor finds a merge command in one of your Github pull requests, it does exactly this: Reads the .rultor.yml YAML config file from the root directory of your repository. Gets automated build execution command from it, for example bundle test . Checks out your repository into a temporary directory on one of its servers. Merges pull request into master branch. Starts a new Docker container and runs bundle test in it. If everything is fine, pushes modified master branch to Github. Reports back to you, in the Github pull request. You can see it in action, for example, in this pull request: jcabi/jcabi-github#878 . "},{"title":"Every Build in Its Own Docker Container","url":"/2014/07/29/docker-in-rultor.html","tags":["docker","rultor","devops"],"date":"2014-07-29 00:00:00 +0000","categories":[],"body":" Docker is a command line tool that can run a shell command in a virtual Linux, inside an isolated file system. Every time we build our projects, we want them to run in their own Docker containers. Take this Maven project for example: $ sudo docker run -i -t ubuntu mvn clean test This command will start a new Ubuntu system and execute mvn clean test inside it. Rultor.com , our virtual assistant, does exactly that with our builds, when we deploy, package, test and merge them. Why Docker? What benefits does it give us? And why Docker, when there are many other virtualization technologies , like LXC, for example? Well, there are a few very important benefits: Image repository (hub.docker.com) Versioning Application-centric Let's discuss them in details. Image Repository Docker enables image sharing through its public repository at hub.docker.com . This means that after I prepare a working environment for my application, I make an image out of it and push it to the hub. Let's say, I want my Maven build to be executed in a container with a pre-installed graphviz package (in order to enable dot command line tool). First, I would start a plain vanilla Ubuntu container, and install graphviz inside it: $ sudo docker run -i -t ubuntu /bin/bash root@215d2696e8ad:/# sudo apt-get install -y graphviz Reading package lists... Done Building dependency tree Reading state information... Done The following extra packages will be installed: ... root@215d2696e8ad:/# exit $ sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 215d2696e8ad ubuntu:14.04 /bin/bash About a minute ago Exited ( 0 ) 3 seconds ago high_mccarthy I have a container that stopped a few seconds ago. Container's ID is 215d2696e8ad . Now, I want to make it reusable for all further tests in Rultor.com. I have to create an image from it: $ sudo docker commit 215d2696e8ad yegor256/beta c5ad7718fc0e20fe4bf2c8a9bfade4db8617a25366ca5b64be2e1e8aa0de6e52 I just made my new commit to a new image yegor256/beta . This image can be reused right now. I can create a new container from this image and it will have graphviz installed inside! Now it's time to share my image at Docker hub, in order to make it available for Rultor: $ sudo docker push yegor256/beta The push refers to a repository [ yegor256/beta ] ( len: 1 ) Sending image list Pushing repository yegor256/beta ( 1 tags ) 511136ea3c5a: Image already pushed, skipping d7ac5e4f1812: Image already pushed, skipping 2f4b4d6a4a06: Image already pushed, skipping 83ff768040a0: Image already pushed, skipping 6c37f792ddac: Image already pushed, skipping e54ca5efa2e9: Image already pushed, skipping c5ad7718fc0e: Image successfully pushed Pushing tag for rev [ c5ad7718fc0e ] on { https://registry-1.docker.io/v1/repositories/yegor256/beta/tags/latest } The last step is to configure Rultor to use this image in all builds. To do this, I will edit .rultor.yml in the root directory of my Github repository: docker : image : yegor256/beta That's it. From now on, Rultor will use my custom Docker image with pre-installed graphviz, in every build (merge, release, deploy, etc.) Moreover, if and when I want to add something else to the image, it's easy to do. Say, I want to install Ruby into my build image. I start a container from the image and install it (pay attention, I'm starting a container not from ubuntu image, as I did before, but from yegor256/beta ): $ sudo docker run -i -t yegor256/beta /bin/bash root@7e0fbd9806c9:/# sudo apt-get install -y ruby Reading package lists... Done Building dependency tree Reading state information... Done The following extra packages will be installed: ... root@7e0fbd9806c9:/# exit $ sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e0fbd9806c9 yegor256/beta:latest /bin/bash 28 seconds ago Exited ( 0 ) 2 seconds ago pensive_pare 215d2696e8ad ubuntu:14.04 /bin/bash 10 minutes ago Exited ( 0 ) 8 minutes ago high_mccarthy You can now see that I have two containers. The first one is the one I am using right now; it contains Ruby. The second one is the one I was using before and it contains graphviz. Now I have to commit again and push: $ sudo docker commit 7e0fbd9806c9 yegor256/beta 6cbfb7a6b18a2182f42171f6bb5aef67c4819b5c2795edffa6a63ba78aaada2d $ sudo docker push yegor256/beta ... Thus, this Docker hub is a very convenient feature for Rultor and similar systems. Versioning As you saw in the example above, every change to a Docker image has its own version (hash) and it's possible to track changes. It is also possible to roll back to any particular change. Rultor is not using this functionality itself, but Rultor users are able to control their build configurations with much better precision. Application-Centric Docker, unlike LXC or Vagrant, for example, is application-centric. This means that when we start a container — we start an application. With other virtualization technologies, when you get a virtual machine — you get a fully functional Unix environment, where you can login through SSH and do whatever you want. Docker makes things simpler. It doesn't give you SSH access to container, but runs an application inside and shows you its output. This is exactly what we need in Rultor. We need to run an automated build (for example Maven or Bundler), see its output and get its exit code. If the code is not zero, we fail the build and report to the user. This is how we run Maven build: $ sudo docker run --rm -i -t yegor256/rultor mvn clean test [ INFO ] ------------------------------------------------------------------------ [ INFO ] Building jcabi-github 0.13 [ INFO ] ------------------------------------------------------------------------ [ INFO ] [ INFO ] --- maven-clean-plugin:2.5:clean ( default-clean ) @ jcabi-github --- [ INFO ] ... As you can see, Maven starts immediately. We don't worry about the internals of the container. We just start an application inside it. Furthermore, thanks to the --rm option, the container gets destroyed immediately after Maven execution is finished. This is what application-centric is about. Our overall impression of Docker is highly positive. ps. A compact version of this article was published at devops.com "},{"title":"Rultor + Travis","url":"/2014/07/31/travis-and-rultor.html","tags":["docker","rultor","devops"],"date":"2014-07-31 00:00:00 +0000","categories":["best"],"body":" Rultor is a coding team assistant. Travis is a hosted continuous integration system. In this article I'll show how our open source projects are using them in tandem to achieve seamless continuous delivery. I'll show a few practical scenarios. Scenario #1: Merge Pull Request jcabi-mysql-maven-plugin is a Maven plugin for MySQL integration testing . @ChristianRedl submitted pull request #35 with a new feature. I reviewed the request and asked Rultor to merge it into master : As you can see, an actual merge operation was made by Rultor. I gave him access to the project by adding his Github account to the list of project collaborators. Before giving a \"go ahead\" to Rultor I checked the status of the pre-build reported by Travis: Travis found a new commit in the pull request and immediately (without any interaction from my side) triggered a build in that branch. The build didn't fail, that's why Travis gave me a green sign. I looked at that sign and at the code. Since all problems in the code were corrected by the pull request author and Travis didn't complain — I gave a \"go\" to Rultor. Scenario #2: Continuous Integration Even though the previous step guarantees that master branch is always clean and stable, we're using Travis to continuously integrate it. Every commit made to master triggers a new build in Travis. The result of the build changes the status of the project in Travis: either \"failing\" or \"passing\". jcabi-aspects is a collection of AOP AspectJ aspects . We configured Travis to build it continuously. This is the badge it produces (the left one): Again, let me stress that even through read-only master is a strong protection against broken builds, it doesn't guarantee that at any moment master is stable. For example, sometimes unit tests fail sporadically due to changes in calendar, in environment, in dependencies, in network connection qualities, etc. Well, ideally, unit tests should either fail or pass because they are environment independent. However, in reality, unit tests are far from being ideal. That's why a combination of read-only master with Rultor and continuous integration with Travis gives us higher stability. Scenario #3: Release to RubyGems jekyll-github-deploy is a Ruby gem that automates deployment of Jekyll sites to Github Pages . @leucos submitted a pull request #4 with a new feature. The request was merged successfully into master branch. Then, Rultor was instructed by myself that master branch should be released to RubyGems and a new version to set is 1.5: Rultor executed a simple script, pre-configured in its .rultor.yml : release : script : | ./test.sh rm -rf *.gem sed -i \"s/2.0-SNAPSHOT/${tag}/g\" jgd.gemspec gem build jgd.gemspec chmod 0600 ../rubygems.yml gem push *.gem --config-file ../rubygems.yml The script is parameterized, as you see. There is one parameter that is passed by Rultor into the script: ${tag} . This parameter was provided by myself in the Github issue, when I submitted a command to Rultor. The script tests that the gem works (integration testing) and clean up afterwords: $ ./test.sh $ rm -rf *.gem Then, it changes the version of itself in jgd.gemspec to the one provided in the ${tag} (it is an environment variable), and builds a new .gem : $ sed -i \"s/2.0-SNAPSHOT/${tag}/g\" jgd.gemspec $ gem build jgd.gemspec Finally, it pushes a newly built .gem to RubyGems, using login credentials from ../rubygems.yml . This file is created by Rultor right before starting the script (this mechanism is discussed below): $ chmod 0600 ../rubygems.yml $ gem push *.gem --config-file ../rubygems.yml If everything works fine and RubyGems confirms successful deployment, Rultor reports to Github. This is exactly what happened in pull request #4 . Scenario #4: Deploy to CloudBees s3auth.com is a Basic HTTP authentication gateway for Amazon S3 Buckets . It is a Java web app. In its pull request #195 , a resource leakage problem was fixed by @carlosmiranda and the pull request was merged by Rultor. Then, @davvd instructed Rultor to deploy master branch to production environment. Rultor created a new Docker container and ran mvn clean deploy in it. Maven deployed the application to CloudBees : The overall procedure took 21 minutes, as you see the in the report generated by Rultor. There is one important trick worth mentioning. Deployment to production always means using secure credentials, like login, password, SSH keys, etc. In this particular example, Maven CloudBees Plugin needed API key, secret and web application name. These three parameters are kept secure and can't be revealed in an \"open source\" way. So, there is a mechanism that configures Rultor accordingly through its .rultor.yml file (pay attention to the first few lines): assets : settings.xml : \"yegor256/home#assets/s3auth/settings.xml\" pubring.gpg : \"yegor256/home#assets/pubring.gpg\" secring.gpg : \"yegor256/home#assets/secring.gpg\" These YAML entries inform Rultor that it has to get assets/s3auth/settings.xml file from yegor256/home private (!) Github repository and put it into the working directory of Docker container, right before starting the Maven build. This settings.xml file contains that secret data CloudBees plugin needs in order to deploy the application. How to Deploy to CloudBees, in One Click explains this process even better. You Can Do The Same Both Rultor and Travis are free hosted products, provided your projects are open source and hosted at Github. Other good examples of Rultor+Travis usage can be seen in these Github issues: jcabi/jcabi-http#47 , jcabi/jcabi-http#48 "},{"title":"Cache Java Method Results","url":"/2014/08/03/cacheable-java-annotation.html","tags":["java","jcabi","aop"],"date":"2014-08-03 00:00:00 +0000","categories":[],"body":" Say, you have a method that takes time to execute and you want its result to be cached. There are many solutions , including Apache Commons JCS , Ehcache , JSR 107 , Guava Caching and many others. jcabi-aspects offers a very simple one, based on AOP aspects and Java6 annotations: import com.jcabi.aspects.Cacheable ; public class Page { @Cacheable ( lifetime = 5 , unit = TimeUnit . MINUTES ) String load () { return new URL ( \"http://google.com\" ). getContent (). toString (); } } The result of load() method will be cached in memory for five minutes. How It Works? This post about AOP, AspectJ and method loging explains how \"aspect weaving\" works (I highly recommend that you read it first). Here I'll explain how caching works. The approach is very straight forward. There is a static hash map with keys as \"method coordinates\" and values as their results. Method coordinates consist of the object, an owner of the method and a method name with parameter types. In the example above, right after the method load() finishes, the map gets a new entry (simplified example, of course): key: [page, \"load()\"] value: \"<html>...</html>\" Every consecutive call to load() will be intercepted by the aspect from jcabi-aspects and resolved immediately with a value from the cache map. The method will not get any control until the end of its lifetime, which is five minutes in the example above. What About Cache Flushing? Sometimes it's necessary to have the ability to flush cache before the end of its lifetime. Here is a practical example: import com.jcabi.aspects.Cacheable ; public class Employees { @Cacheable ( lifetime = 1 , unit = TimeUnit . HOURS ) int size () { // calculate their amount in MySQL } @Cacheable.FlushBefore void add ( Employee employee ) { // add a new one to MySQL } } It's obvious that the number of employees in the database will be different after add() method execution and the result of size() should be invalidated in cache. This invalidation operation is called \"flushing\" and @Cacheable.FlushBefore triggers it. Actually, every call to add() invalidates all cached methods in this class, not only size() . There is also @Cacheable.FlushAfter . The difference is that FlushBefore guarantees that cache is already invalidated when the method add() starts. FlushAfter invalidates cache after method add() finishes. This small difference makes a big one, sometimes. This article explains how to add jcabi-aspects to your project . "},{"title":"Strict Control of Java Code Quality","url":"/2014/08/13/strict-code-quality-control.html","tags":["java","qulice","static-analysis"],"date":"2014-08-13 00:00:00 +0000","categories":["best"],"body":"There are many tools that control the quality of Java code, including Checkstyle , PMD , FindBugs , Cobertura , etc. All of them are usually used to analyze quality and build some fancy reports. Very often, those reports are published by continuous integration servers, like Jenkins. Qulice takes things one step further. It aggregates a few quality checkers, configures them to a maximum strict mode, and breaks your build if any of them fail. Seriously. There are over 130 checks in Checkstyle, over 400 rules in PMD, and over 400 bugs in FindBugs. All of them should say: \"Yes, we like your code\". Otherwise, your build shouldn't pass. What do you think? Would it be convenient for you — to have your code rejected every time it breaks just one of 900 checks? Would it be productive for the team — to force developers to focus so much on code quality? First Reaction If you join one of our teams as a Java developer, you will develop your code in branches and, then, Rultor will merge your changes into master . Before actually merging, though, Rultor will run an automated build script to make sure that your branch doesn't break it. As a static analysis tool, Qulice is just one of the steps in the automated build script. It is actually a Maven plugin and we automate Java builds with Maven 3x. Thus, if your changes break any of Qulice's rules, your entire branch gets rejected. Your first reaction - I've seen it hundreds of times - will be negative. You may actually become frustrated enough to leave the project immediately. You may say something like this (I'm quoting real life stories): \"These quality rules entirely ruin my creativity!\" \"Instead of wasting time on these misplaced commas and braces, we'd be better off developing new features!\" \"I've done many successful projects in my life, never heard about this ridiculous quality checking...\" This first reaction is only logical. I've seen many people say things like this, in both open source and commercial projects. Not only in Java, but also in PHP (with phpcs and phpmd ) and Ruby (with rubocop and simplecov ). How do I answer? Read on. On Second Thought My experience tells me that the sooner someone can get used to the strict quality control of Qulice, the faster he/she can learn and grow; the better programmer he/she is; and the further he/she can go with us and our projects. Having this experience in mind, I recommend that all new project members be patient and try to get used to this new approach to quality. In a few weeks, those who stick with it start to understand why this approach is good for the project and for them, as Java engineers. Ratatouille (2007) by Brad Bird and Jan Pinkava Why is it good? Read on. What Do Projects Get From Qulice? Let's take one simple rule as an example. Here is a piece of Java code that Qulice would complain about (due to the DesignForExtension rule from Checkstyle): public class Employee { public String name () { return \"Jeff\" ; } } What is wrong with this code? Method name() is not final and can be overridden by a class that extends Employee . Design-wise this is wrong, since a child class is allowed to break a super class, overriding its method. What is the right design? This one: public class Employee { public final String name () { return \"Jeff\" ; } } Now, the method is final and can't be overriden by child classes. It is a much safer design (according to Checkstyle, and I agree). So, let's say we make this rule mandatory for all classes in the project. What does the project gain from this? It can promise its members (programmers) a higher quality of work, compared to other projects that don't have this restriction, mostly because of: Predictability of Design — I don't have to scroll through the entire class to make sure it doesn't have methods that can be accidentally overriden. I know for sure that this can't happen in this project. In other words, I know what to expect. Less Hidden Tricks — Higher predictability of design leads to better visibility of mistakes and tricks. Standardization of source code makes it uniform. This means that it's easier to read and spot problems. Industry Standards — The decision to use this design is made by Checkstyle, not by a project architect. For me, as a project developer, this means that I'm following industry standards. That makes the project (and its leaders) more respectable. Learning — I'll bet that most of you who read this post didn't know about the design rule explained above. Just by reading this article, you learned something new. Imagine how much you could learn after making your code compliant to all 900 rules of Qulice (Checkstyle + PMD + FindBugs). The point about learning brings us to the last, and the most important, thought to discuss. What Do I Get from Qulice? As a programmer, I hope you already realize what you get from working in a project that raises its quality bar as high as Qulice asks. Yes, you'll learn a lot of new things about writing quality Java code. On top of that though, I would actually say that you are getting free lessons with every new line of code you write. And the teacher is a software, written by hundreds of Java developers, for the last ten years. Qulice just integrates those software tools together. Truthfully, it is the developers who are the real authors of quality checks and rules. So, what do I tell those who complain about quality rules being too strict? I say this: \"Do you want to learn and improve, or do you just want to get paid and get away with it?\" ps. You can use my settings.jar for IntelliJ, they are rather strict and will help you clean your code even before Qulice starts to complain. "},{"title":"How to Retry Java Method Call on Exception","url":"/2014/08/15/retry-java-method-on-exception.html","tags":["jcabi","java","aop"],"date":"2014-08-15 00:00:00 +0000","categories":[],"body":" If you have a method that fails occasionally and you want to retry it a few times before throwing an exception. @RetryOnFailure from jcabi-aspects can help. For example, if you're downloading the following web page: @RetryOnFailure ( attempts = 3 , delay = 10 , unit = TimeUnit . SECONDS ) public String load ( URL url ) { return url . openConnection (). getContent (); } This method call will throw an exception only after three failed executions with a ten seconds interval between them. This post explains how jcabi-aspects works with binary weaving. This mechanism integrates AspectJ with your code. When method load() from the example above is called, this is what is happening behind the scene (pseudo-code): while ( attempts ++ < 3 ) { try { return original_load ( url ); } catch ( Throwable ex ) { log ( \"we failed, will try again in 10 seconds\" ); sleep ( 10 ); } } This approach may be very useful in the following situations (based on my experience): Executing JDBC SELECT statements Loading data from HTTP, S3, FTP, etc resources Uploading data over the network Fetching data through RESTful stateless APIs The project is in Github . "},{"title":"Fluent JDBC Decorator","url":"/2014/08/18/fluent-jdbc-decorator.html","tags":["jcabi","java","jdbc"],"date":"2014-08-18 00:00:00 +0000","categories":[],"body":" This is how you fetch text from a SQL table with jcabi-jdbc : String name = new JdbcSession ( source ) . sql ( \"SELECT name FROM employee WHERE id = ?\" ) . set ( 1234 ) . select ( new SingleOutcome < String >( String . class )); Simple and straight forward, isn't it? The library simplifies interaction with relational databases via JDBC, avoiding the need to use ORMs. jcabi-jdbc is a lightweight wrapper of JDBC . It is very convenient to use when you don't need a full-scale ORM (like Hibernate), but want just to select, insert, or update a few rows in a relational database. Every instance of JdbcSession is a \"transaction\" in a database. You start it by instantiating the class with a single parameter — data source. You can obtain the data source from your connection pool. There are many implementations of connection pools. I would recommend that you use BoneCP . Below is an example of how you would connect to PostgreSQL: @Cacheable ( forever = true ) private static DataSource source () { BoneCPDataSource src = new BoneCPDataSource (); src . setDriverClass ( \"org.postgresql.Driver\" ); src . setJdbcUrl ( \"jdbc:postgresql://localhost/db_name\" ); src . setUser ( \"jeff\" ); src . setPassword ( \"secret\" ); return src ; } Be sure to pay attention to the @Cacheable annotation. This post explains how it can help you to cache Java method results for some time. Setting the forever attribute to true means that we don't want this method to be called more than once. Instead, we want the connection pool to be created just once, and every second call should return its existing instance (kind of like a Singleton pattern). jcabi-jdbc website explains how you can insert , update , or delete a row. You can also execute any SQL statement . By default, JdbcSession closes the JDBC connection right after the first select/update/insert operation. Simply put, it is designed to be used mainly for single atomic transactions. However, it is possible to leave the connection open and continue, for example: new JdbcSession ( source ) . autocommit ( false ) . sql ( \"START TRANSACTION\" ) . update () . sql ( \"DELETE FROM employee WHERE name = ?\" ) . set ( \"Jeff Lebowski\" ) . update () . sql ( \"INSERT INTO employee VALUES (?)\" ) . set ( \"Walter Sobchak\" ) . insert ( Outcome . VOID ) . commit (); In this example we're executing three SQL statements one by one, leaving connection (and transaction) open unti commit() is called. "},{"title":"How to Release to Maven Central, in One Click","url":"/2014/08/19/how-to-release-to-maven-central.html","tags":["java","rultor","devops","maven"],"date":"2014-08-19 00:00:00 +0000","categories":["jcg"],"body":"When I release a new version of jcabi-aspects , a Java open source library, to Maven Central, it takes 30 seconds of my time. Maybe even less. Recently, I released version 0.17.2. You can see how it all happened, in Github issue #80 : As you see, I gave a command to Rultor , and it released a new version to Maven central. I didn't do anything else. Now let's see how you can do the same. How you can configure your project so that the release of its new version to Maven Central takes just a few seconds of your time. By the way, I assume that you're hosting your project in Github. If not, this entire tutorial won't work. If you are still not in Github, I would strongly recommend moving there. Prepare Your POM Make sure your pom.xml contains all elements required by Sonatype, explained in Central Sync Requirements . We will deploy to Sonatype, and they will syncronize all JAR (and not only) artifacts to Maven Central. Register a Project With Sonatype Create an account in Sonatype JIRA and raise a ticket, asking to approve your groupId. This OSSRH Guide explains this step in more detail. Create and Distribute a GPG Key Create a GPG key and distribute it, as explained in this Working with PGP Signatures article. When this step is done, you should have two files: pubring.gpg and secring.gpg . Create settings.xml Create settings.xml , next to the two .gpg files created in the previous step: <settings> <profiles> <profile> <id> foo </id> <!-- give it the name of your project --> <properties> <gpg.homedir> /home/r </gpg.homedir> <gpg.keyname> 9A105525 </gpg.keyname> <gpg.passphrase> my-secret </gpg.passphrase> </properties> </profile> </profiles> <servers> <server> <id> sonatype </id> <username> <!-- Sonatype JIRA user name --> </username> <password> <!-- Sonatype JIRA pwd --> </password> </server> </servers> </settings> In this example, 9A105525 is the ID of your public key, and my-secret is the pass phrase you have used while generating the keys. Encrypt Security Assets Now, encrypt these three files with a rultor remote : $ gem install rultor $ rultor encrypt -p me/test pubring.gpg $ rultor encrypt -p me/test secring.gpg $ rultor encrypt -p me/test settings.xml Instead of me/test you should use the name of your Github project. You will get three new files: pubring.gpg.asc , secring.gpg.asc and settings.xml.asc . Add them to the root directory of your project, commit and push. The files contain your secret information, but only the Rultor server can decrypt them. Add Sonatype Repositories I would recommend using jcabi-parent , as a parent pom for your project. This will make many further steps unnecessary. If you're using jcabi-parent, skip this step. However, if you don't use jcabi-parent, you should add these two repositories to your pom.xml : <project> [...] <distributionManagement> <repository> <id> oss.sonatype.org </id> <url> https://oss.sonatype.org/service/local/staging/deploy/maven2/ </url> </repository> <snapshotRepository> <id> oss.sonatype.org </id> <url> https://oss.sonatype.org/content/repositories/snapshots </url> </snapshotRepository> </distributionManagement> </project> Configure GPG Plugin Again, I'd recommend using http://parent.jcabi.com , which configures this plugin automatically. If you're using it, skip this step. Otherwise, add this plugin to your pom.xml : <project> [..] <build> [..] <plugins> [..] <plugin> <artifactId> maven-gpg-plugin </artifactId> <version> 1.5 </version> <executions> <execution> <id> sign-artifacts </id> <phase> verify </phase> <goals> <goal> sign </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> Configure Versions Plugin Once again, I recommend using http://parent.jcabi.com . It configures all required plugins out-of-the-box. If you're using it, skip this step. Otherwise, add this plugin to your pom.xml : <project> [..] <build> [..] <plugins> [..] <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> versions-maven-plugin </artifactId> <version> 2.1 </version> <configuration> <generateBackupPoms> false </generateBackupPoms> </configuration> </plugin> </plugins> </build> </project> Configure Sonatype Plugin Yes, you're right, http://parent.jcabi.com will help you here as well. If you're using it, skip this step too. Otherwise, add these four plugins to your pom.xml : <project> [..] <build> [..] <plugins> [..] <plugin> <artifactId> maven-deploy-plugin </artifactId> <configuration> <skip> true </skip> </configuration> </plugin> <plugin> <artifactId> maven-source-plugin </artifactId> <executions> <execution> <id> package-sources </id> <goals> <goal> jar </goal> </goals> </execution> </executions> </plugin> <plugin> <artifactId> maven-javadoc-plugin </artifactId> <executions> <execution> <id> package-javadoc </id> <phase> package </phase> <goals> <goal> jar </goal> </goals> </execution> </executions> </plugin> <plugin> <groupId> org.sonatype.plugins </groupId> <artifactId> nexus-staging-maven-plugin </artifactId> <version> 1.6 </version> <extensions> true </extensions> <configuration> <serverId> oss.sonatype.org </serverId> <nexusUrl> https://oss.sonatype.org/ </nexusUrl> <description> ${project.version} </description> </configuration> <executions> <execution> <id> deploy-to-sonatype </id> <phase> deploy </phase> <goals> <goal> deploy </goal> <goal> release </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> Create Rultor Config Create a .rultor.yml file in the root directory of your project ( reference page explains this format in details): decrypt : settings.xml : \"repo/settings.xml.asc\" pubring.gpg : \"repo/pubring.gpg.asc\" secring.gpg : \"repo/secring.gpg.asc\" release : script : | mvn versions:set \"-DnewVersion=${tag}\" git commit -am \"${tag}\" mvn clean deploy --settings /home/r/settings.xml You can compare your file with live Rultor configuration of jcabi-aspects . Run It! Now it's time to see how it all works. Create a new ticket in the Github issue tracker, and post something like that into it (read more about Rultor commands ): @rultor release, tag is `0.1` You will get a response in a few seconds. The rest will be done by Rultor. Enjoy :) BTW, if something doesn't work as I've explained, don't hesitate to submit a ticket to Rultor issue tracker . I will try to help you. Yeah, forgot to mention, Rultor is also doing two important things. First, it creates a Github release with a proper description. Second, it posts a tweet about the release, which you can retweet, to make an announcement to your followers. Both features are very convenient for me. For example: DynamoDB Local Maven Plugin, 0.7.1 released https://t.co/C3KULouuKS — rultor.com (@rultors) August 19, 2014 "},{"title":"The Art of Software Testing by Glenford Myers","url":"/2014/08/22/art-of-software-testing.html","tags":["book-review","testing"],"date":"2014-08-22 00:00:00 +0000","categories":[],"body":" \"The Art of Software Testing\" by Glenford J. Myers, Tom Badgett and Corey Sandler is one of my favorite books concerning testing and software engineering in general. In this article, I will provide an overview of the book, as well as highlight the ideas and quotes that I found to be the most interesting. There were three editions of the book. The first one was published in 1979, when I was just too young to appreciate it. The second one was published in 2004 — I read it first in 2007. The third one was published just two years ago, in 2012. I bought this edition also, and read it like it was my first time. This book is still one of the top books in the software testing domain, despite its age and some content that is rather out-dated. Out-dated Content First, let's filter out what is not worth reading (in my opinion). There are eleven chapters, but you can easily skim through nine of them. This is because those chapter discuss concepts that are discussed elsewhere in the book with a more robust level of detail or on a much higher level of abstraction. For example, Chapter 3 contains an eleven-page checklist to be used by a code reviewer in order to find programming mistakes. This list is definitely not comprehensive and it can't compete with, say, \"Code Complete\" by Steve McConnell. I believe, this checklist had significant value twenty years ago, but now it is out of date. Chapter 5 discusses basic principles and strategies of unit testing. However, the discussion is not abstract enough for a short 25-page summary, and is not specific enough for a detailed discussion. Again, twenty years ago this information may have had some value. Nowadays, \"Growing Object-Oriented Softare, Guided by Tests\" by Steven Freeman and Nat Pryce is a much better source for this subject. There are also articles about usability testing, debugging, web application testing, and mobile testing. Here we have the same issue — they are not abstract enough and they are much too outdated to be relevant to the current issues in software testing. I would recommend readers to briefly skim those subjects for background information, but to not read too much into it. Psychology of Testing The most important and valuable part of the book is Chapter 2. It is full of priceless quotes that can also be very practical. For example, on page 6: Testing is a destructive, even sadistic, process, which explains why most people find it difficult In Chapter 2, Dr. Myers discusses the psychology of testing and a very common and crucial misunderstanding of testing objectives. He claims that it is commonly accepted that the goal of software testing is \"to show that a program performs its intended functions correctly\" (p.5). Testers are hired to check whether the software functions as expected. They then report back to management whether all tests have successfully passed and whether the program can be delivered to end users. Despite the plethora of software testing tomes available on the market today, many developers seem to have an attitude that is counter to extensive testing This is what Dr. Myers says on the second page, and I can humbly confirm that in all software groups I've been worked in thus far, almost everyone, including testers, project managers, and programmers, share this philosophy. They all believe that \"testing is the process of demonstrating that errors are not present\" (p.5) However, \"these definitions are upside down\" (p.6). The psychology of testing should be viewed as the opposite. There are two quotes that support this theory and I feel that they make the entire book. The first one, on page 6, defines the goal of software testing: Testing is the process of executing a program with the intent of finding errors The second one, on the following page, further refines the first goal: An unsuccessful test case is one that causes a program to produce the correct results without finding any errors Dr. Myers comes back to these two thoughts in every chapter. He reiterates over and over again that we should change the underlying psychology of how we view testing, in order to change our testing results. We should focus on breaking the software instead of confirming that it works. Because testing is a \"sadistic process\" (p.6) of breaking things. It is a \"destructive process\" (p.8). If you read Chapter2 very carefully and truly understand its underlying ideas, it may change your entire life :) This chapter should be a New Testament of every tester. Test Completion Criteria In Chapter 2, Dr. Myers also mentions that a program, no matter how simple, contains an unlimited number of errors. He says that \"you cannot test a program to guarantee that it is error free\" (p.10) and that \"it is impractical, often impossible, to find all the errros in a program\" (p.8). Furthermore, at the end of Chapter 6, he makes an important observation (p.135): One of the most difficult questions to answer when testing a program is determining when to stop, since there is no way of knowing if the error just detected is the last remaining error The problem is obvious. Since any program contains an unlimited number of errors, it doesn't matter how long we test, we won't find all of them. So when do we stop? What goals do we set for our testers? And even more importantly, when do we pay them and how much (this question is important to me since I only work with contractors and am required to define measurable and achievable goals)? The answer Dr. Myers gives is brilliant (p.136): Since the goal of testing is to find errors, why not make the completion criterion the detection of some predefined number of errors? He then goes on to discuss exactly how this \"predefined number\" can be estimated. I find this idea very interesting. I have even applied it to a few projects I've had in the last few years. It works. However it can also cause serious psychological problems for the team. Most people simply resent the goal of \"testing until you find a required number of bugs.\" The most common response is \"what if there are no bugs any more?\". However, after a few fights, the team eventually begins to appreciate the concept and get used to it. So, I can humbly confirm that Dr. Myers is right in his suggestion. You can successful plan testing based on a predefined number of errors. Summary I consider this book a fundamental writing in the area of software testing. This is mostly due to Chapter 2 of the book. In fact, there are just three pages of text that build the foundation of the entire book. They are the skeleton of the other two hundred pages. Unfortunately, since 1979, this skeleton hasn't become the backbone of the software testing industry. Most of us are still working against these principles. "},{"title":"How to Deploy to CloudBees, in One Click","url":"/2014/08/25/deploy-to-cloudbees.html","tags":["java","rultor","devops"],"date":"2014-08-25 00:00:00 +0000","categories":[],"body":"When I deploy a new version of stateful.co , a Java web application, to CloudBees, it takes 30 seconds of my time. Maybe even less. Recently, I deployed version 1.6.5. You can see how it all happened, in Github issue #6 : As you see, I gave a command to Rultor , and it packaged, tested and deployed a new version to CloudBees. I didn't do anything else. Now let's see how you can do the same. How you can configure your project so that the deployment of its new version to CloudBees takes just a few seconds of your time. Since CloudBees is [shutting down its PaaS service](http://www.cloudbees.com/press/cloudbees-becomes-enterprise-jenkins-company) by the end of December, 2014, this article will have no sense after that. Configure the CloudBees Maven Plugin Add this profile to your pom.xml : <project> [..] <profiles> <profile> <id> cloudbees </id> <activation> <property><name> bees.appId </name></property> </activation> <pluginRepositories> <pluginRepository> <id> cloudbees-public-release </id> <url> http://repository-cloudbees.forge.cloudbees.com/public-release </url> </pluginRepository> </pluginRepositories> <build> <pluginManagement> <plugins> <plugin> <artifactId> maven-deploy-plugin </artifactId> <configuration> <skip> true </skip> </configuration> </plugin> </plugins> </pluginManagement> <plugins> <plugin> <groupId> com.cloudbees </groupId> <artifactId> bees-maven-plugin </artifactId> <version> 1.3.2 </version> <configuration> <appid> ${bees.id} </appid> <apikey> ${bees.key} </apikey> <secret> ${bees.secret} </secret> </configuration> <executions> <execution> <id> deploy-to-production </id> <phase> deploy </phase> <goals> <goal> deploy </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> This plugin is not in Maven Central (unfortunately). That's why we have to specify that <pluginRepository> . Pay attention to the fact that we're also disabling maven-deploy-plugin , since it would try to deploy your WAR package to the repository from the <distributionManagement> section. We want to avoid this. The profile gets activated only when the bees.id property is defined. This won't happen during your normal development and testing, but it will occur during the deployment cycle initiated by Rultor, because we will define this property in settings.xml (discussed below). Secure Access to CloudBees Create an account in CloudBees and register your web application there. CloudBees is free, as long as you don't need too much computing power. I believe that web applications should be light-weight by definition, so CloudBees' free layer is an ideal choice. Create a settings.xml file (but don't commit it to your repo!): <settings> <profiles> <profile> <id> cloudbees </id> <properties> <bees.id> stateful/web </bees.id> <bees.key> <!-- your key --> </bees.key> <bees.secret> <!-- your secret --> </bees.secret> </properties> </profile> </profiles> </settings> Encrypt this file using rultor remote : $ gem install rultor $ rultor encrypt -p me/test settings.xml Instead of me/test use the name of your Github project. You should get a settings.xml.asc file; add it to the root directory of your project, commit and push. This file contains your CloudBees credentials, but in an encrypted format. Nobody can read it, except the Rultor server. Configure Versions Plugin I recommend using http://parent.jcabi.com . It configures the required plugin out-of-the-box. If you're using it, skip this step. Otherwise, add this plugin to your pom.xml : <project> [..] <build> [..] <plugins> [..] <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> versions-maven-plugin </artifactId> <version> 2.1 </version> <configuration> <generateBackupPoms> false </generateBackupPoms> </configuration> </plugin> </plugins> </build> </project> Configure Rultor Create a .rultor.yml file in the root directory of your project (this reference page explains this format in detail): decrypt : settings.xml : \"repo/settings.xml.asc\" release : script : | mvn versions:set \"-DnewVersion=${tag}\" git commit -am \"${tag}\" mvn clean deploy --settings /home/r/settings.xml You can compare your file with live Rultor configuration of stateful.co . Run It! Now it's time to see how it all works. Create a new ticket in the Github issue tracker, and post something like that into it (read more about Rultor commands ): @rultor release, tag is `0.1` You will get a response in a few seconds. The rest will be done by Rultor. Enjoy :) BTW, if something doesn't work as I've explained, don't hesitate to submit a ticket to the Rultor issue tracker . I will try to help you. Also, a similar configuration can be performed for Heroku (using jcabi-heroku-maven-plugin ) and for AWS Elastic Beanstalk (using jcabi-beanstalk-maven-plugin ). I'll probably dedicate individual posts to them, as well. "},{"title":"How to Publish to Rubygems, in One Click","url":"/2014/08/26/publish-to-rubygems.html","tags":["rubygems","rultor","devops","ruby"],"date":"2014-08-26 00:00:00 +0000","categories":[],"body":"When I release a new version of jgd , a Ruby gem, to Rubygems.org, it takes 30 seconds of my time. Here is how I released a bug fix for version 1.5.1, in Github issue #6 : As you see, I gave a command to Rultor , and it released a new version to Rubygems. I didn't do anything else. Now let's see how you can do the same. How you can configure your project so that the release of its new version to Rubygems.org takes just a few seconds of your time. By the way, I assume that you're hosting your project in Github. If not, this entire tutorial won't work. If you are still not in Github, I would strongly recommend moving there. Create Rubygems Account Create an account in Rubygems.org . Create rubygems.yml Create a rubygems.yml file (you may already have it as ~/.gem/credentials ): :rubygems_api_key : d355d8940bb031bfe9acf03ed3da4c0d You should get this API key from Rubygems. To find your API key, click on your username when logged in to RubyGems.org and then click on \"Edit Profile\". Encrypt rubygems.yml Now, encrypt rubygems.yml with a rultor remote : $ gem install rultor $ rultor encrypt -p me/test rubygems.yml Instead of me/test use the name of your Github project. You will get a new file rubygems.yml.asc . Add this file to the root directory of your project, commit and push. The file contains your secret information, but only the Rultor server can decrypt it. Prepare Gemspec In your gemspec file, make sure you use 1.0.snapshot as a version number: # coding: utf-8 Gem :: Specification . new do | s | # ... s . version = '1.0.snapshot' # ... end This version name will be replaced by Rultor during deployment. Configure Rultor Create a .rultor.yml file in the root directory of your project: decrypt : rubygems.yml : \"repo/rubygems.yml.asc\" release : script : | rm -rf *.gem sed -i \"s/1.0.snapshot/${tag}/g\" foo.gemspec gem build foo.gemspec chmod 0600 /home/r/rubygems.yml gem push *.gem --config-file /home/r/rubygems.yml In this example, replace foo with the name of your gem. Run It! Now it's time to see how it all works. Create a new ticket in the Github issue tracker, and post something like that into it (read more about Rultor commands ): @rultor release, tag is `0.1` You will get a response in a few seconds. The rest will be done by Rultor. Enjoy :) BTW, if something doesn't work as I've explained, don't hesitate to submit a ticket to Rultor issue tracker . I will try to help you. "},{"title":"How We Run as a Non-Root Inside Docker Container","url":"/2014/08/29/docker-non-root.html","tags":["docker","rultor"],"date":"2014-08-29 00:00:00 +0000","categories":[],"body":"Docker starts a process inside its container as a \"root\" user. In some cases, this is not convenient though. For example, initdb from PostgreSQL doesn't like to be started as root and will fail. In rultor.com , a DevOps team assistant, we're using Docker as a virtualization technology for every build we run. Here is how we change the user inside a running container, right after it is started. First, this is how we start a new Docker container: $ sudo docker run -i -t --rm -v \"$(pwd):/main\" yegor256/rultor /main/entry.sh There are two files in the current directory: entry.sh and script.sh . entry.sh is the file being executed by Docker on start, and it contains the following: #!/bin/bash adduser --disabled-password --gecos '' r adduser r sudo echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers su -m r -c /home/r/script.sh script.sh will be executed as a user r inside the container. And this r user will have sudo permissions. This is exactly what all projects, managing their DevOps procedures with rultor.com , need. "},{"title":"Simple Java SSH Client","url":"/2014/09/02/java-ssh-client.html","tags":["java","jcabi","ssh"],"date":"2014-09-02 00:00:00 +0000","categories":["jcg"],"body":"An execution of a shell command via SSH can be done in Java, in just a few lines, using jcabi-ssh : String hello = new Shell . Plain ( new SSH ( \"ssh.example.com\" , 22 , \"yegor\" , \"-----BEGIN RSA PRIVATE KEY-----...\" ) ). exec ( \"echo 'Hello, world!'\" ); jcabi-ssh is a convenient wrapper of JSch , a well-known pure Java implementation of SSH2. Here is a more complex scenario, where I upload a file via SSH and then read back its grepped content: Shell shell = new SSH( \"ssh.example.com\", 22, \"yegor\", \"-----BEGIN RSA PRIVATE KEY-----...\" ); File file = new File(\"/tmp/data.txt\"); new Shell.Safe(shell).exec( \"cat > d.txt && grep 'some text' d.txt\", new FileInputStream(file), Logger.stream(Level.INFO, this), Logger.stream(Level.WARNING, this) ); Class SSH , which implements interface Shell , has only one method, exec . This method accepts four arguments: interface Shell { int exec( String cmd, InputStream stdin, OutputStream stdout, OutputStream stderr ); } I think it's obvious what these arguments are about. There are also a few convenient decorators that make it easier to operate with simple commands. Shell.Safe Shell.Safe decorates an instance of Shell and throws an exception if the exec exit code is not equal to zero. This may be very useful when you want to make sure that your command executed successfully, but don't want to duplicate if/throw in many places of your code. Shell ssh = new Shell.Safe( new SSH( \"ssh.example.com\", 22, \"yegor\", \"-----BEGIN RSA PRIVATE KEY-----...\" ) ); Shell.Verbose Shell.Verbose decorates an instance of Shell and copies stdout and stderr to the slf4j logging facility (using jcabi-log ). Of course, you can combine decorators, for example: Shell ssh = new Shell.Verbose( new Shell.Safe( new SSH( \"ssh.example.com\", 22, \"yegor\", \"-----BEGIN RSA PRIVATE KEY-----...\" ) ) ); Shell.Plain Shell.Plain is a wrapper of Shell that introduces a new exec method with only one argument, a command to execute. It also doesn't return an exit code, but stdout instead. This should be very convenient when you want to execute a simple command and just get its output (I'm combining it with Shell.Safe for safety): String login = new Shell.Plain(new Shell.Safe(ssh)).exec(\"whoami\"); Download You need a single dependency jcabi-ssh.jar in your Maven project (get its latest version in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-ssh </artifactId> </dependency> The project is in Github . If you have any problems, just submit an issue. I'll try to help. "},{"title":"RESTful API and a Web Site in the Same URL","url":"/2014/09/09/restful-web-sites.html","tags":["restful","xslt","xml"],"date":"2014-09-09 00:00:00 +0000","categories":["jcg"],"body":"Look at Github RESTful API, for example. To get information about a repository you should make a GET request to api.github.com/repos/yegor256/rultor . In response, you will get a JSON document with all the details of the yegor256/rultor repository. Try it, the URL doesn't require any authentication. To open the same repository in a nice HTML+CSS page, you should use a different URL: github.com/yegor256/rultor . The URL is different, the server-side is definitely different, but the nature of the data is exactly the same. The only thing that changes is a representation layer. In the first case, we get JSON; in the second — HTML. How about combining them? How about using the same URL and the same server-side processing mechanism for both of them? How about shifting the whole rendering task to the client-side (the browser) and letting the server work solely with the data? The Good, the Bad, The Wierd (2008) by Kim Jee-woon XSLT is the technology that can help us do this. In \"XML+XSLT in a Browser\" I explained briefly how it works in a browser. In a nutshell, the server returns an XML with some data and a link to the XSL stylesheet. The stylesheet, being executed in a browser, converts XML to HTML. XSL language is as powerful as any other rendering engine, like JSP, JSF, Tiles, or what have you. Actually, it is much more powerful. Using this approach we literally remove the entire rendering layer (\"View\" in the MVC paradigm) from the server and move it to the browser. If we can make it possible, the web server will exponse just a RESTful API, and every response page will have an XSL stylesheet attached. What do we gain? We'll discuss later, at the end of the post. Now, let's see what problems we will face: JSON doesn't have a rendering layer. There is no such thing as XSLT for JSON. So, we will have to forget about JSON and stay with XML only. For me, this sounds perfectly all right. Others don't like XML and prefer to work with JSON only. Never understood them :) XSLT 2.0 is not supported by all browsers. Even XSLT 1.0 is only supported by some of them. For example, Internet Explorer 8 doesn't support XSLT at all. Browsers support only GET and POST HTTP methods, while traditional RESTful APIs exploit also, at least, PUT and DELETE . The first problem is not really a problem. It's just a matter of taste (and level of education). The last two problems are much more serious. Let's discuss them. XSL Transformation on the Server XSLT is not supported by some browsers. How do we solve this? I think that the best approach is to parse the User-Agent HTTP header in every request and make a guess, whether this particular version of the browser supports XSLT or not. It's not so difficult to do, since this compatibility information is public. If the browser doesn't support XSLT, we can do the transformation on the server side. We already have the XML with data, generated by the server, and we already have the XSL attached to it. All we need to do is to apply the latter to the former and obtain an HTML page. Then, we return the HTML to the browser. Besides that, we can also pay attention to the Accept header. If it is set to application/xml or text/xml , we return XML, no matter what User-Agent is saying. This means, basically, that some API client is talking to us, not a browser. And this client is not interested in HTML, but in pure data in XML format. POST Instead of PUT There is no workaround for this. Browsers don't know anything about PUT or DELETE . So, we should also forget them in our RESTful APIs. We should design our API using only two methods: GET and POST . Is this even possible? Yes. Why not? It won't look as fancy as with all six methods (some APIs also use OPTIONS and HEAD ), but it will work. What Do We Gain? OK, here is the question — why do we need this? What's wrong with the way most people work now? Why can't we make a web site separate from the API? What benefits do we get if we combine them? I've been combining them in all web applications I've worked with since 2011. And the biggest advantage I'm experiencing is avoiding code duplication. It is obvious that in the server we don't duplicate controllers (in the case of MVC). We have one layer of controllers, and they control both the API and the web site (since they are one thing now). Avoiding code duplication is a very important achievement. Moreover, I believe that it is the most important target for any software project. These small web apps work exactly as explained above: s3auth.com , stateful.co , bibrarian.com . They are all open source, and you can see their source code in Github. "},{"title":"Anti-Patterns in OOP","url":"/2014/09/10/anti-patterns-in-oop.html","tags":["oop","anti-pattern"],"date":"2014-09-10 00:00:00 +0000","categories":[],"body":"Here they come: NULL References Utility Classes Mutable Objects Getters and Setters Object-Relational Mapping (ORM) Singletons Controllers, Managers, Validators Public Static Methods Avoid them at all cost. "},{"title":"Deployment Script vs. Rultor","url":"/2014/09/11/deployment-script-vs-rultor.html","tags":["rultor","devops"],"date":"2014-09-11 00:00:00 +0000","categories":["jcg"],"body":" When I explain how Rultor automates deployment/release processes, very often I hear something like: But I already have a script that deploys everything automatically. This response is very common, so I decided to summarize my three main arguments for automated Rultor deployment/release processes in one article: 1) isolated docker containers, 2) visibility of logs and 3) security of credentials. Read about them and see what Rultor gives you on top of your existing deployment script(s). Charlie and the Chocolate Factory (2005) by Tim Burton Before we start with the arguments, let me emphasize that Rultor is a useful interface to your custom scripts. When you decide to automate deployment with Rultor, you don't throw away any of your existing scripts. You just teach Rultor how to call them. Isolated Docker Containers The first advantage you get once you start calling your deployment scripts from Rultor is the usage of Docker . I'm sure you know what Docker is, but for those who don't — it is a manager of virtual Linux \"machines\". It's a command line script that you call when you need to run some script in a new virtual machine (aka \"container\"). Docker starts the container almost immediately and runs your script. The beauty of Docker is that every container is a perfectly isolated Linux environment, with its own file system, memory, processes, etc. When you tell Rultor to run your deployment script, it starts a new Docker container and runs your script there. But what benefit does this give me, you ask? The main benefit is that the container gets destroyed right after your script is done. This means that you can do all pre-configuration inside the container without any fear of conflict with your main working platform. Let me give an example. I'm developing on MacBook, where I install and remove packages which I need for development. At the same time, I have a project that, in order to be deployed, requires PHP 5.3, MySQL 5.6, phing, phpunit, phpcs and xdebug. Every MacOS version needs to be configured specifically to get these applications up and running, and it's a time-consuming job. I can change laptops, and I can change MacOS versions, but the project stays the same. It still requires the same set of packages in order to run its deployment script successfully. And the project is not in active development any more. I simply don't need these packages for my day-to-day work, since I'm working with Java more now. But, when I need to make a minor fix to that PHP project and deploy it, I have to install all the required PHP packages and configure them. Only after that can I deploy that minor fix. It is annoying, to say the least. Docker gives me the ability to automate all of this together. My existing deployment script will get a preamble, which will install and configure all necessary PHP-related packages in a clean Ubuntu container. This preamble will be executed on every run of my deployment script, inside a Docker container. For example, it may look like this: My deployment script looked like this before I started to use Rultor: 1 2 3 #!/bin/bash phing test git ftp push --user \"..\" --passwd \"..\" --syncroot php/src ftp://ftp.example.com/ Just two lines. The first one is a full run of unit tests. The second one is an FTP deployment to the production server. Very simple. But this script will only work if PHP 5.3, MySQL, phing, xdebug, phpcs and phpunit are installed. Again, it's a lot of work to install and configure them every time I upgrade my MacOS or change a laptop. Needless to say, that if/when someone joins the project and tries to run my scripts, he/she will have to do this pre-installation work again. So, here is a new script, which I'm using now. It is being executed inside a new Docker container, every time: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # First, we install all prerequisites sudo apt-get install -y php5 php5-mysql mysql sudo apt-get install php-pear sudo pear channel-discover pear.phpunit.de sudo pear install phpunit/PHPUnit sudo pear install PHP_CodeSniffer sudo pecl install xdebug sudo pear channel-discover pear.phing.info sudo pear install phing/phing # And now the same script I had before phing test git ftp push --user \"..\" --passwd \"..\" --syncroot php/src ftp://ftp.example.com/ Obviously, running this script on my MacBook (without virtualization) would cause a lot of trouble. Well, I don't even have apt-get here :) Thus, the first benefit that Rultor gives you is an isolation of your deployment script in its own virtual environment. We have this mostly thanks to Docker. Visibility of Logs Traditionally, we keep deployment scripts in some ~/deploy directory and run them with a magic set of parameters. In a small project, you do this yourself and this directory is on your own laptop. In a bigger project, there is a \"deployment\" server, that has that magic directory with a set of scripts that can be executed only by a few trusted senior developers. I've seen this setup many times. The biggest issue here is traceability. It's almost impossible to find out who deployed what and why some particular deployment failed. The senior deployment gurus simply SSH to the server and run those magic scripts with magic parameters. Logs are usually lost and problem tracking is very difficult or impossible. Rultor offers something different. With Rultor, there is no SSH access to deployment scripts any more. All scripts stay in the .rultor.yml configuration file, and you start them by posting messages in your issue tracking system (for example Github, JIRA or Trac). Rultor runs the script and publishes its full log right to your ticket. The log stays with your project forever. You can always get back to the ticket you were working with and check why deployment failed and what instructions were actually executed. For example, check out this Github issue, where I was deploying a new version of Rultor itself, and failed a few times: yegor256/rultor#563 . All my failed attempts are protocolled. I can always get back to them and investigate. For a big project this information is vital. Thus, the second benefit of Rultor versus a standalone deployment script is visibility of every single operation. Security of Credentials When you have a custom script sitting in your laptop or in that secret team deployment server, your production credentials stay close to it. There is just no other way. If your software works with a database, it has to know login credentials (user name, password, DB name, port number, etc.). Well, in the worst case, some people just hard code that information right into the source code. We aren't even going to discuss this case, that's how bad it is. But let's say you separate your DB credentials from the source code. You will have something like a db.properties or db.ini file, which will be attached to the application right before deployment. You can also keep that file directly in the production server, which is even better, but not always possible, especially with PaaS deployments, for example. A similar problem exists with deployments of artifacts to repositories. Say, you're regularly deploying to RubyGems.org. Your ~/.gem/credentials will contain your secret API key. So, very often, your deployment scripts are accompanied by some files with sensitive and secure information. And these files have this information in a plain, open format. No encryption, no protection. Just user names, passwords, codes and tokens in plain text. Why is this bad? Well, for a single developer with a single laptop this doesn't sound like a problem. Although, I don't like the idea of losing a laptop somewhere in an airport with all credentials open and ready to be used. You may argue that there are disc protection tools, like FileVault for MacOS or BestCrypt for Windows. Yes, maybe. But let's see what happens when we have a team of developers, working together and sharing those deployment scripts and files with credentials. Once you give access to your deployment scripts to a new member of the team, you have to share all that sensitive data. There is just no way around it. In order to use the scripts he/she has to be able to open files with credentials. This is a problem, if you care about the security of your data. Rultor solves this problem by offering an on-the-fly GPG decryption of your sensitive data, right before they are used by your deployment scripts. In the .rultor.yml configuration file you just say: decrypt : db.ini : \"repo/db.ini.asc\" deploy : script : ftp put db.ini production Then, you encrypt your db.ini using a Rultor GPG key, and fearlessly commit db.ini.asc to the repository. Nobody will be able to open and read that file, except the Rultor server itself, right before running the deployment script. Thus, the third benefit of Rultor versus a standalone deployment script is proper security of sensitive data. "},{"title":"Deploying to Heroku, in One Click","url":"/2014/09/13/deploying-to-heroku.html","tags":["rultor","devops","heroku","java"],"date":"2014-09-13 00:00:00 +0000","categories":[],"body":"There were a few articles already about our usage of Rultor for automating continuous delivery cycles of Java and Ruby projects, including Rubygems , CloudBees and MavenCentral . This one describes how Heroku deployment can be automated. When I need to deploy a new version of an Aintshy web application, all I do is create one message in a Github ticket. I just say @rultor release 0.1.4 and version 0.1.4 gets deployed to Heroku. See Github ticket #5 . You can do the same, with the help of Rultor.com , a free hosted DevOps assistant. Create Heroku Project Create a new project at Heroku.com . Then install their command line toolbelt . Authenticate at Heroku You should authenticate your public SSH key at Heroku, using their command line toolbelt. The process is explained here , but it is not much of a process. You just run heroku login and enter your login credentials. As a result, you will get your existing key (located at ~/.ssh/id_rsa.pub ) authenticated by Heroku. If you didn't have the key before, it will be created automatically. Encrypt SSH Key Now, encrypt id_rsa and id_rsa.pub (they are in the ~/.ssh directory) with a rultor remote : $ gem install rultor $ rultor encrypt -p me/test id_rsa $ rultor encrypt -p me/test id_rsa.pub Instead of me/test use the name of your Github project. You will get two new files id_rsa.asc and id_rsa.pub.asc . Add them to the root directory of your project, commit and push. These files contain your secret information, but only the Rultor server can decrypt them. Create Rultor Config Create a .rultor.yml file in the root directory of your project ( reference page explains this format in detail): decrypt : id_rsa : \"repo/id_rsa.asc\" id_rsa.pub : \"repo/id_rsa.pub.asc\" release : script : | mvn versions:set \"-DnewVersion=${tag}\" git commit -am \"${tag}\" mvn clean install -Pqulice --errors git remote add heroku git@heroku.com:aintshy.git mkdir ~/.ssh mv ../id_rsa ../id_rsa.pub ~/.ssh chmod -R 600 ~/.ssh/* echo -e \"Host *\\n StrictHostKeyChecking no\\n UserKnownHostsFile=/dev/null\" > ~/.ssh/config git push -f heroku $(git symbolic-ref --short HEAD):master You can compare your file with live Rultor configuration of aintshy/hub . Run It! Now it's time to see how it all works. Create a new ticket in the Github issue tracker, and post something like this into it (read more about Rultor commands ): @rultor release, tag is `0.1` You will get a response in a few seconds. The rest will be done by Rultor. Enjoy :) BTW, if something doesn't work as I've explained, don't hesitate to submit a ticket to the Rultor issue tracker . I will try to help you. "},{"title":"Getters/Setters. Evil. Period.","url":"/2014/09/16/getters-and-setters-are-evil.html","tags":["oop","anti-pattern"],"date":"2014-09-16 00:00:00 +0000","categories":["best","jcg"],"body":"There is an old debate, started in 2003 by Allen Holub in this Why getter and setter methods are evil famous article, about whether getters/setters is an anti-pattern and should be avoided or if it is something we inevitably need in object-oriented programming. I'll try to add my two cents to this discussion. The gist of the following text is this: getters and setters is a terrible practice and those who use it can't be excused. Again, to avoid any misunderstanding, I'm not saying that get/set should be avoided when possible. No. I'm saying that you should never have them near your code. Arrogant enough to catch your attention? You've been using that get/set pattern for 15 years and you're a respected Java architect? And you don't want to hear that nonsense from a stranger? Well, I understand your feelings. I felt almost the same when I stumbled upon Object Thinking by David West, the best book about object-oriented programming I've read so far. So please. Calm down and try to understand while I try to explain. Existing Arguments There are a few arguments against \"accessors\" (another name for getters and setters), in an object-oriented world. All of them, I think, are not strong enough. Let's briefly go through them. Tell, Don't Ask Allen Holub says, \"Don't ask for the information you need to do the work; ask the object that has the information to do the work for you\". Violated Encapsulation Principle An object can be teared apart by other objects, since they are able to inject any new data into it, through setters. The object simply can't encapsulate its own state safely enough, since anyone can alter it. Exposed Implementation Details If we can get an object out of another object, we are relying too much on the first object's implementation details. If tomorrow it will change, say, the type of that result, we have to change our code as well. All these justifications are reasonable, but they are missing the main point. Fundamental Misbelief Most programmers believe that an object is a data structure with methods. I'm quoting Getters and Setters Are Not Evil , an article by Bozhidar Bozhanov: But the majority of objects for which people generate getters and setters are simple data holders. This misconception is the consequence of a huge misunderstanding! Objects are not \"simple data holders\". Objects are not data structures with attached methods. This \"data holder\" concept came to object-oriented programming from procedural languages, especially C and COBOL. I'll say it again: an object is not a set of data elements and functions that manipulate them. An object is not a data entity. What is it then? A Ball and A Dog In true object-oriented programming, objects are living creatures, like you and me. They are living organisms, with their own behaviour, properties and a life cycle. Can a living organism have a setter? Can you \"set\" a ball to a dog? Not really. But that is exactly what the following piece of software is doing: Dog dog = new Dog (); dog . setBall ( new Ball ()); How does that sound? Can you get a ball from a dog? Well, you probably can, if she ate it and you're doing surgery. In that case, yes, we can \"get\" a ball from a dog. This is what I'm talking about: Dog dog = new Dog (); Ball ball = dog . getBall (); Or an even more ridiculous example: Dog dog = new Dog (); dog . setWeight ( \"23kg\" ); Can you imagine this transaction in the real world? :) Does it look similar to what you're writing every day? If yes, then you're a procedural programmer. Admit it. And this is what David West has to say about it, on page 30 of his book: Step one in the transformation of a successful procedural developer into a successful object developer is a lobotomy. Do you need a lobotomy? Well, I definitely needed one and received it, while reading West's Object Thinking . Object Thinking Start thinking like an object and you will immediately rename those methods. This is what you will probably get: Dog dog = new Dog (); dog . take ( new Ball ()); Ball ball = dog . give (); Now, we're treating the dog as a real animal, who can take a ball from us and can give it back, when we ask. Worth mentioning is that the dog can't give NULL back. Dogs simply don't know what NULL is :) Object thinking immediately eliminates NULL references from your code. A Fish Called Wanda (1988) by Charles Crichton Besides that, object thinking will lead to object immutability, like in the \"weight of the dog\" example. You would re-write that like this instead: Dog dog = new Dog ( \"23kg\" ); int weight = dog . weight (); The dog is an immutable living organism, which doesn't allow anyone from the outside to change her weight, or size, or name, etc. She can tell, on request, her weight or name. There is nothing wrong with public methods that demonstrate requests for certain \"insides\" of an object. But these methods are not \"getters\" and they should never have the \"get\" prefix. We're not \"getting\" anything from the dog. We're not getting her name. We're asking her to tell us her name. See the difference? We're not talking semantics here, either. We are differentiating the procedural programming mindset from an object-oriented one. In procedural programming, we're working with data, manipulating them, getting, setting, and deleting when necessary. We're in charge, and the data is just a passive component. The dog is nothing to us — it's just a \"data holder\". It doesn't have its own life. We are free to get whatever is necessary from it and set any data into it. This is how C, COBOL, Pascal and many other procedural languages work(ed). On the contrary, in a true object-oriented world, we treat objects like living organisms, with their own date of birth and a moment of death — with their own identity and habits, if you wish. We can ask a dog to give us some piece of data (for example, her weight), and she may return us that information. But we always remember that the dog is an active component. She decides what will happen after our request. That's why, it is conceptually incorrect to have any methods starting with set or get in an object . And it's not about breaking encapsulation, like many people argue. It is whether you're thinking like an object or you're still writing COBOL in Java syntax. PS. Yes, you may ask, — what about JavaBeans, JPA, JAXB, and many other Java APIs that rely on the get/set notation? What about Ruby's built-in feature that simplies the creation of accessors? Well, all of that is our misfortune. It is much easier to stay in a primitive world of procedural COBOL than to truly understand and appreciate the beautiful world of true objects. PPS. Forgot to say, yes, dependency injection via setters is also a terrible anti-pattern. About it, in one of the next posts :) "},{"title":"Remote Programming in Teamed.io","url":"/2014/09/22/remote-programming-interview.html","tags":["mgmt"],"date":"2014-09-22 00:00:00 +0000","categories":[],"body":"Here is an interview taken by Lisette Sutherland from www.CollaborationSuperpowers.com , a few hours ago, which I enjoyed to give :) I answered these questions (approximately): How Teamed.io differs from other software companies (0:50)? How do we control programmers remotely (1:59)? Do we compare ourselves with open source (3:52)? How do we build a network of programmers (5:10)? Why people like to work with us (5:40)? What happens when a programmer fails (7:50)? How can it be financially successful (9:40)? How do we organize \"team building\" (11:50)? What challenges do we have (14:50)? What about micro-management (17:55)? Can this work in a non-IT sector (19:40)? What do you do to manage the team (20:48)? Isn't it difficult to manage so many tasks (24:18)? Do we have cultural issues (25:35)? Is it true that people are not enough result-oriented (27:40)? Are there any other challenges (29:12)? What do I like personally about it (30:40)? How do we scale our teams when we need more programmers (32:01)? What an \"unlimited pool of talents\" means (34:40)? What advice do I have for those who work remotely (37:50)? Where do I work from, personally (39:10)? How do we find clients (42:29)? Enjoy :) "},{"title":"Built-in Fake Objects","url":"/2014/09/23/built-in-fake-objects.html","tags":["testing","java"],"date":"2014-09-23 00:00:00 +0000","categories":["jcg"],"body":"While mock objects are perfect instruments for unit testing, mocking through mock frameworks may turn your unit tests into an unmaintainable mess. Thanks to them we often hear that \"mocking is bad\" and \"mocking is evil\". The root cause of this complexity is that our objects are too big. They have many methods and these methods return other objects, which also have methods. When we pass a mock version of such an object as a parameter, we should make sure that all of its methods return valid objects. This leads to inevitable complexity, which turns unit tests to waste almost impossible to maintain. Object Hierarchy Take the Region interface from jcabi-dynamo as an example (this snippet and all others in this article are simplified, for the sake of brevity): public interface Region { Table table ( String name ); } Its table() method returns an instance of the Table interface, which has its own methods: public interface Table { Frame frame (); Item put ( Attributes attrs ); Region region (); } Interface Frame , returned by the frame() method, also has its own methods. And so on. In order to create a properly mocked instance of interface Region , one would normally create a dozen other mock objects. With Mockito it will look like this: public void testMe () { // many more lines here... Frame frame = Mockito . mock ( Frame . class ); Mockito . doReturn (...). when ( frame ). iterator (); Table table = Mockito . mock ( Table . class ); Mockito . doReturn ( frame ). when ( table ). frame (); Region region = Mockito . mock ( Region . class ); Mockito . doReturn ( table ). when ( region ). table ( Mockito . anyString ()); } And all of this is just a scaffolding before the actual testing. Sample Use Case Let's say, you're developing a project that uses jcabi-dynamo for managing data in DynamoDB. Your class may look similar to this: public class Employee { private final String name ; private final Region region ; public Employee ( String empl , Region dynamo ) { this . name = empl ; this . region = dynamo ; } public Integer salary () { return Integer . parseInt ( this . region . table ( \"employees\" ) . frame () . where ( \"name\" , this . name ) . iterator () . next () . get ( \"salary\" ) . getN () ); } } You can imagine how difficult it will be to unit test this class, using Mockito, for example. First, we have to mock the Region interface. Then, we have to mock a Table interface and make sure it is returned by the table() method. Then, we have to mock a Frame interface, etc. The unit test will be much longer than the class itself. Besides that, its real purpose, which is to test the retrieval of an employee's salary, will not be obvious to the reader. Moreover, when we need to test a similar method of a similar class, we will need to restart this mocking from scratch. Again, multiple lines of code, which will look very similar to what we have already written. Fake Classes The solution is to create fake classes and ship them together with real classes. This is what jcabi-dynamo is doing. Just look at its JavaDoc . There is a package called com.jcabi.dynamo.mock that contains only fake classes, suitable only for unit testing. Even though their sole purpose is to optimize unit testing, we ship them together with production code, in the same JAR package. This is what a test will look like, when a fake class MkRegion is used: public class EmployeeTest { public void canFetchSalaryFromDynamoDb () { Region region = new MkRegion ( new H2Data (). with ( \"employees\" , new String [] { \"name\" }, new String [] { \"salary\" } ) ); region . table ( \"employees\" ). put ( new Attributes () . with ( \"name\" , \"Jeff\" ) . with ( \"salary\" , new AttributeValue (). withN ( 50000 )) ); Employee emp = new Employee ( \"Jeff\" , region ); assertThat ( emp . salary (), equalTo ( 50000 )) } } This test looks obvious to me. First, we create a fake DynamoDB region, which works on top of H2Data storage (in-memory H2 database). The storage will be ready for a single employees table with a hash key name and a single salary attribute. Then, we put a record into the table, with a hash Jeff and a salary 50000 . Finally, we create an instance of class Employee and check how it fetches the salary from DynamoDB. I'm currently doing the same thing in almost every open source library I'm working with. I'm creating a collection of fake classes, that simplify testing inside the library and for its users. BTW, a great article on the same subject: tl;dw: Stop mocking, start testing by Ned Batchelder. "},{"title":"Why Monetary Awards Don't Work?","url":"/2014/09/24/why-monetary-awards-dont-work.html","tags":["mgmt"],"date":"2014-09-24 00:00:00 +0000","categories":[],"body":"Monetary rewards for employees. Do they work? Should we use them? Can money motivate creative minds? Will a programmer work better if he gets paid only when he reaches his goals and objectives? Much research has already been done on this subject, and most of it proves that connecting results with money is a very demotivating approach. For example, Ian Larkin says that the most productive workers \"suffered a 6-8% decrease in productivity after the award was instituted\". I believe this is completely true. Money may become a terrible de-motivator for all modern employees (not just programmers). My question is — why is this so? Why doesn't money work, even when it was invented to be a universal instrument to measure our labor? Why can't an American dollar, which has been used for centuries as a trading tool between working people, be used anymore? Why, in a modern office, do we try to hide monetary motivation and replace it with everything else , like free lunches, team building events, paid vacations, etc. Why don't we just say — \"Jeff completed his task faster than everybody else. This is his $500 check. Whoever completes the next task gets $300,\" out loud in the office?... Sounds uncomfortable, doesn't it? Why does money as a motivator scare us? I have an answer. Money doesn't work when there are no ground rules. When we say that Jeff will get a $500 bonus if he finishes his task on time, but don't say what he should do when someone distracts him — Jeff gets frustrated. He also doesn't understand who his boss is anymore. Does he just work for the bonus, or should he also satisfy a CTO who comes to his desk asking him to do something else urgently? Is Jeff allowed to tell the CTO \"to get lost\" because he's working towards his own personal objective (the bonus)? In all cases I've seen myself and in all research cases I've read about, people keep repeating the same mistake. They create a rewards program (monetary or not) without setting ground rules for the team. By doing so, they encourage people to play wild-wild west, where the fastest gets the cash bag. Obviously, the Bad and the Ugly get to the prize faster, while the Good gets demotivated and depressed. In a clockwise direction from the top left corner: The Good, the Bad and the Ugly (1966) by Sergio Leone; Roger Federer; A Serious Man (2009) by Ethan Coen and Joel Coen; Two and a Half Men (TV Series). What do I mean by ground rules? It should be a simple document ( PMBOK calls it a Staffing Management Plan) that helps me, as a team member, answer at least these basic questions: How my personal results are measured? Who gives me tasks and who do I report to? How should I resolve conflicts between tasks? What are my personal deadlines for every task? Do I have measurable quality expectations for my deliverables? How do my mistakes affect my performance grade? The ground rules document should be superior to your boss. If the document says that your results get an A+ grade, the boss should have no say. If she doesn't like you personally, it doesn't matter. You get an A+ grade, and you are the best. That's it. Does your team have such a document? Can you answer all of these questions? If not, you're not ready for a rewards program. It will only make your management situation worse, just like all the scientific research says. Rewards will motivate the most cunning to take advantage of the most hard working and good-natured. Team spirit will suffer, big time. On the other hand, if you have that \"ground rules\" document and you strictly follow it, giving monetary rewards to your workers will significantly increase their performance and motivation. They will know exactly what needs to be done to get the rewards, and they won't have any distraction. Your team won't be a group of wild west gunslingers anymore, but more like players in a sports arena. The best players will go further, and the worst will know exactly what needs to be done to improve. Fair competition will lead to a better cumulative result. Moreover, if your ground rules are strict and explicit, you can use not only rewards, but also punishments. And your team will gladly accept them, because they will help emphasize what (and who) works best and help get rid of the waste. I'm speaking from experience here. In XDSD we're not only rewarding programmers with money, but we also never pay for anything except delivered results. We manage to do this mostly because our groud rules are very strict and non-ambiguous. And we never break them. "},{"title":"DI Containers are Code Polluters","url":"/2014/10/03/di-containers-are-evil.html","tags":["oop","anti-pattern"],"date":"2014-10-03 00:00:00 +0000","categories":[],"body":"While dependency injection (aka, \"DI\") is a natural technique of composing objects in OOP (known long before the term was introduced by Martin Fowler ), Spring IoC , Google Guice , Java EE6 CDI , Dagger and other DI frameworks turn it into an anti-pattern. I'm not going to discuss obvious arguments against \"setter injections\" (like in Spring IoC ) and \"field injections\" (like in PicoContainer ). These mechanisms simply violate basic principles of object-oriented programming and encourage us to create incomplete, mutable objects, that get stuffed with data during the course of application execution. Remember: ideal objects must be immutable and may not contain setters . Instead, let's talk about \"constructor injection\" (like in Google Guice ) and its use with dependency injection containers . I'll try to show why I consider these containers a redundancy, at least. What is Dependency Injection? This is what dependency injection is (not really different from a plain old object composition): public class Budget { private final DB db ; public Budget ( DB data ) { this . db = data ; } public long total () { return this . db . cell ( \"SELECT SUM(cost) FROM ledger\" ); } } The object data is called a \"dependency\". A Budget doesn't know what kind of database it is working with. All it needs from the database is its ability to fetch a cell, using an arbitrary SQL query, via method cell() . We can instantiate a Budget with a PostgreSQL implementation of the DB interface, for example: public class App { public static void main ( String ... args ) { Budget budget = new Budget ( new Postgres ( \"jdbc:postgresql:5740/main\" ) ); System . out . println ( \"Total is: \" + budget . total ()); } } In other words, we're \"injecting\" a dependency into a new object budget . An alternative to this \"dependency injection\" approach would be to let Budget decide what database it wants to work with: public class Budget { private final DB db = new Postgres ( \"jdbc:postgresql:5740/main\" ); // class methods } This is very dirty and leads to 1) code duplication, 2) inability to reuse, and 3) inability to test, etc. No need to discuss why. It's obvious. Thus, dependency injection via a constructor is an amazing technique. Well, not even a technique, really. More like a feature of Java and all other object-oriented languages. It's expected that almost any object will want to encapsulate some knowledge (aka, a \"state\"). That's what constructors are for. What is a DI Container? So far so good, but here comes the dark side — a dependency injection container. Here is how it works (let's use Google Guice as an example): import javax.inject.Inject ; public class Budget { private final DB db ; @Inject public Budget ( DB data ) { this . db = data ; } // same methods as above } Pay attention: the constructor is annotated with @Inject . Then, we're supposed to configure a container somewhere, when the application starts: Injector injector = Guice . createInjector ( new AbstractModule () { @Override public void configure () { this . bind ( DB . class ). toInstance ( new Postgres ( \"jdbc:postgresql:5740/main\" ) ); } } ); Some frameworks even allow us to configure the injector in an XML file. From now on, we are not allowed to instantiate Budget through the new operator, like we did before. Instead, we should use the injector we just created: public class App { public static void main ( String ... args ) { Injection injector = // as we just did in the previous snippet Budget budget = injector . getInstance ( Budget . class ); System . out . println ( \"Total is: \" + budget . total ()); } } The injection automatically finds out that in order to instantiate a Budget it has to provide an argument for its constructor. It will use an instance of class Postgres , which we instantiated in the injector. This is the right and recommended way to use Guice. There are a few even darker patterns, though, which are possible but not recommended. For example, you can make your injector a singleton and use it right inside the Budget class. These mechanisms are considered wrong even by DI container makers, however, so let's ignore them and focus on the recommended scenario. What Is This For? Let me reiterate and summarize the scenarios of incorrect usage of dependency injection containers: Field injection Setter injection Passing injector as a dependency Making injector a global singleton If we put all of them aside, all we have left is the constructor injection explained above. And how does that help us? Why do we need it? Why can't we use plain old new in the main class of the application? The container we created simply adds more lines to the code base, or even more files, if we use XML. And it doesn't add anything, except an additional complexity. We should always remember this if we have the question: \"What database is used as an argument of a Budget?\" The Right Way Now, let me show you a real life example of using new to construct an application. This is how we create a \"thinking engine\" in rultor.com (full class is in Agents.java ): Impressive? This is a true object composition. I believe this is how a proper object-oriented application should be instantiated. And DI containers? In my opinion, they just add unnecessary noise. "},{"title":"10 Hosted Continuous Integration Services for a Private Repository","url":"/2014/10/05/ten-hosted-continuous-integration-services.html","tags":["ci","devops"],"date":"2014-10-05 00:00:00 +0000","categories":["jcg"],"body":"Every project I'm working with starts with a setup of continuous integration pipeline. I'm a big fan of cloud services, that's why I was always using travis-ci.org. A few of my clients questioned this choice recently, mostly because of the price. So I decided to make a brief analysis of the market. I configured rultor , an open source project, in every CI service I managed to find. All of them are free for open source projects. All of them are hosted and do not require any server installation Here they are, in order of my personal preference (first four are the best and highly recommended): Linux Windows MacOS Pull requests Log compress travis-ci.com $129/mo appveyor.com $39/mo ? wercker.com free! shippable.com $1/mo codeship.io $49/mo ? semaphoreapp.com $29/mo drone.io $25/mo magnum-ci.com ? ? snap-ci.com $30/mo ? circleci.com $19/mo sonolabs.com $15/mo ? hosted-ci.com $49/mo ? ? ship.io free! ? If you know any other good continuous integration services, email me , I'll review and add them to this list. BTW, here is a \"full\" list of continuous integration software and services. travis-ci.org is the best platform I've seen so far. Mostly because it is the most popular. Perfectly integrates with Github and has proper documentation. One important downside is the price of $129 per month. \"With this money you can get a dedicated EC2 instance and install Jenkins there\" — some of my clients say. I strongly disagree, since Jenkins will require a 24x7 administration, which costs way more than $129, but it's always difficult to explain. appveyor.com is the only one that runs Windows builds. Even though I'm working mostly with Java and Ruby, which are expected to be platform independent, they very often appear to be exactly the opposite. When your build succeedes on Linux, there is almost no guarantee it will pass on Windows or Mac. I'm planning to use appveyor in every project, in combination with some other CI service. I'm still testing it though... wercker.com is a European product from Amsterdam, which is still in beta and that's why free for all projects. The platform looks very promissing. It is still free for private repositories and is backed up by investments . They also have an interesting concept of build \"boxes\", which can be pre-configured similar to Docker containers. It works rather stable for the last few months, no complains so far. shippable.com was easy to configure since it understands .travis.yml out of the box. The user interface is easy to navigate since it doesn't have \"settings\" page at all (or I didn't find it). Everything is configured via shippable.yml file in the repository. The service looks stable and robust, no complains so far. semaphoreapp.com is easy to configure and work with. It makes an impression of a light-weight system, which I generally appreciate. As a downside, they don't have any Maven pre-installed have an old version of Maven, but this was solved easily with a short custom script that downloads and unpacks the latest Maven. Another downside is that they are not configurable through a file (like .travis.yml ) — you should do everything through a UI. They also support caching between builds . codeship.io works fine, but their web UI looks a bit out-dated. Besides that, they promise to work with pull requests, but I didn't manage to configure them. They simply don't notify our pull requests in Github, even though they build them. Maybe I'll find a way, so far it's not clear... magnum-ci.com is a very lightweight and young system. It doesn't connect automatically to Github, so you should do some manual operations of adding a web hook. Besides that, works just fine. snap-ci.com is a product of ThoughtWorks, an author of Go , an open source continuous integration server. It looks a bit more complicated than others, giving you an ability to define \"stages\" and combine them into pipelines. I'm not sure yet how these mechanisms may help in small and medium size projects we're mostly working with, but they look \"cool\". There is also a very unfortunate limitation of 2Gb RAM per build — some of my Java projects fail because of that. Besides that, they don't give full access to the build server, for example we can't modify anything in /etc — it is a show-stopper for us. drone.io works fine, but their support didn't reply to me when I asked for a Maven version update (they have an old version pre-installed). Besides that, their badge is not updated correctly in Github README.md — when the build is broken, the bange stays green... very annoying. circleci.com I still don't know why my build fails there. Really difficult to configure and understand what's going on. Trying to figure it out... zeroci.com looks like a one-man project, which definitely needs usability testing. It was rather difficult to configure a project via its web interface. The good thing is that it's free, but its quality is not high enough to recommend it. ship.io is building only mobile applications (for iOS and Android). Besides that, they don't support Maven for Android apps, only Gradle. I'll try to build iOS Swift app with them soon.... solanolabs.com looks rather immature and difficult to configure. They don't even support automatic Github hook configuration when new repository is added. However, their sales spams me rather aggressively :) hosted-ci.com is for iOS/OSX only. They don't give anything for free, even for open source projects. I didn't have a chance to test them yet. cloudbees.com testing now... dploy.io testing now... BTW, if you don't like the idea of keeping continuous integration in cloud, consider these on-premise software packages (in order or preference): Jenkins , TeamCity , Go , Strider , BuildBot . Keep in mind that no matter how good and expensive your continuous integration service is, it won't help you unless you make your master branch read-only . "},{"title":"Project Lifecycle in Teamed.io","url":"/2014/10/06/software-project-lifecycle.html","tags":["mgmt"],"date":"2014-10-06 00:00:00 +0000","categories":["jcg"],"body":"In addition to being a hands-on programmer, I'm also co-founder and CTO of Teamed.io , a custom software development company. I play the role of a technical and management leader in all projects we work with. I wrote this article for those who're interested in hiring me and/or my team. This article will demonstrate what happens from day one until the end of the project, when you choose to work with us . You will see below that our methods of software development seriously differ from what many other teams are using. I personally pay a lot of attention to quality of code and quality of the internal processes that connect our team. There are four phases in every project I work with in Teamed.io : Thinking . Here we're trying to understand: What is the problem that the product is going to solve? We're also investigating the product's boundaries — who will work with the software (actors) and how will they work with it (user stories). Deliverables: specification. Duration: from 2 days up to 3 weeks. Participants: product owner, analyst(s), architect, project manager. Building . Here the software architect is creating a proof-of-concept (aka an MVP or prototype or a skeleton). It is a one-man job that is done almost without any interaction with anyone else. The architect builds the product according to the specification in a very limited time frame. The result will have multiple bugs and open ends, but it will implement the main user story. The architect also configures continuous integration and delivery pipelines. Deliverables: working software. Duration: 2-5 days. Participants: architect. Fixing . At this phase we are adding all the meat to the skeleton. This phase takes most of the time and budget and involves many participants. In some projects we invite up to 50 people to work, at the same time. Since we treat all inconsistencies as bugs, this phase is mostly about finding, reporting and fixing bugs, in order to stabilize the product and get it ready for market launch. We increment and release the software multiple times a day, preferably to its user champions. Deliverables: bug fixes via pull requests. Duration: from weeks to months. Participants: programmer(s), designer(s), tester(s), code reviewer(s), architect, project manager. Using . At this final phase we are launching the product to its end-users, and collecting their feedback (both positive and negative). Everything they are reporting back to us is being registered as a bug. Then, we categorize the bugs and fix them. This phase may take years, but it never involves active implementation of new features. Deliverables: bug fixes via pull requests. Duration: months. Participants: programmer(s), code reviewer(s), project manager. The biggest (i.e., longest and most expensive) phase is, of course, Fixing. It usually takes the majority of time (over 70%). However, the most important and risky phase is the first one — Thinking. A mistake made during Thinking will cost much more than a mistake made later. Thinking Thinking is the first and the most important phase. First, we give a name to the project and create a Github repository. We try to keep all our projects (both open source and commercial) in Github. Mostly because the platform is very popular, very powerful, and really cheap ( $7/mo for a set of 5 private projects). We also keep all communication in the Github issue tracker. Then, we create a simple half-page SRS document (Software Requirements Specification). Usually this is done right inside the source code, but sometimes in the Github wiki. What's important is that the document should be under version control. We will modify it during the course of the project, very intensively. The SRS should briefly identify main \"actors\" of the system and define the product scope. Even though it is only half a page, the creation of this initial SRS document is the most important and the most expensive task in the entire project. We pay a lot of attention to this step. Usually this document is written by myself in a direct communication with the project sponsor. We can't afford a mistake at this step. Then, we invite a few system analysts to the project. These guys are responsible for turning our initial SRS into a more complete and detailed specification. They start by asking questions, submitting them one by one as Github issues. Every question is addressed to the product owner. Using his answers, system analysts modify the SRS document. This article explains how Requs helps us in this process: Incremental Requirements With Requs At the end of the Thinking phase we estimate the size of the project, in lines of code. Using lines of code, we can roughly estimate a budget . I stay personally involved in the project during the entire Thinking phase. Building This is a one-man job for an architect. Every project we work with has an architect who is personally responsible for the quality and all technical decisions made there. I try to play this role in most projects. The Building phase is rather straight forward. I have to implement the solution according to the SRS, in a few working days. No matter how big the idea and how massive the planning development, I still have to create (build from scratch!) the product in, say, three days. Besides building the software itself, I have to configure all basic DevOps processes, including: 1) automated testing and quality control, 2) deploying and releasing pipelines, 3) repository of artifacts, 4) continuous integration service, etc. The result of this phase is a working software package, deployable to its destination and available for testers. Technical quality requirements are also defined at this phase. Fixing Now it's time to build a distributed team of programmers. First, we invite those who've worked in other projects before and have already have proven their quality. Very often we invite new people, finding them through StackOverflow, Github, oDesk, and other sources. An average team size of an average project is 10-20 programmers. At this phase, we understand any inconsistency as a bug. If something is not clear in the documentation, or if something can be refactored for better readability, or if a function can be improved for higher performance — it is a bug to us. And bugs are welcome in our projects. We encourage everybody to report as many bugs as possible. This is how we achieve high quality. That is why the phase is called Fixing, after all. We are reporting bugs and fixing them. Hundreds of bugs. Sometimes thousands. The product grows in front of our very eyes, because after every bug fix we re-deploy the entire product to the production platform. Every bug is reported, classified, discussed, and fixed in its own Github ticket and its own Git branch. We never allow anyone to just commit to the master branch — all changes must pass through our quality controls and be merged into master by rultor.com , our merging bot . Also important to mention is that all communications with the product owner and between programmers happen only through Github issues. We never use any chats , Skype, emails or conferencing software. We communicate only through tickets and comments in Github. Using This is the final phase and it can take quite a long time. By now, the product is ready and is launched to the market. But we still receive bug reports and feature request from the product owner, and we still fix them through the same process flow as in the Fixing phase. We try to keep this phase as quiet as possible, in terms of the amount of bugs reported and fixed. Thanks to our intensive and pro-active bug finding and fixing in the previous phase, we usually have very few problems at the Using phase. And big feature requests? At this phase, we usually try to convert them into new projects and develop them separately, starting again from Thinking. BTW, the illustrations you see above are made by Bárbara Lopes "},{"title":"Stop Chatting, Start Coding","url":"/2014/10/07/stop-chatting-start-coding.html","tags":["xdsd"],"date":"2014-10-07 00:00:00 +0000","categories":["jcg"],"body":" The first principle of eXtremely Distributed Software Development ( XDSD ) states that \"everyone gets paid for verified deliverables\". This literally means that, in order to get paid, every programmer has to write the code, commit it to the repository, pass a code review and make sure the code is merged into the destination branch . Only then, is his result appreciated and paid for. For most of my clients this already sounds extreme. They are used to a traditional scheme of paying per hour or per month. They immediately realize the benefits of XDSD, though, because for them this approach means that project funds are not wasted on activities that don't produce results. But that's not all. Barton Fink (1991) by Joel Coen This principle also means that nobody is paid for anything except tasks explicitly assigned to him/her. Thus, when a programmer has a question about current design, specification, configuration, etc. — nobody will be interested in answering it. Why not? Because there is no payment attached to this. Answering questions in Skype or Hipchat or by email is something that is not appreciated in XDSD in any way. The project simply doesn't pay for this activity. That's why none of our programmers do this. We don't use any (I mean it!) informal communication channels in XDSD projects. We don't do meetings or conference calls. We never discuss any technical issues on Skype or by phone. So, how do we resolve problems and share information? We use task tracking systems for that. When a developer has a question, he submits it as a new \"ticket\". The project manager then picks it up and assigns it to another developer, who is able to answer it. Then, the answer goes back through the tracking system or directly into the source code. The \"question ticket\" gets closed when its author is satisfied with the answer. When the ticket is closed, those who answered it get paid. Using this model, we significantly improve project communications, by making them clean and transparent. We also save a lot of project funds, since every hour spent by a team member is traceable to the line of code he produced. You can see how this happens in action, for example, in this ticket (the project is open source; that's why all communications are open): jcabi/jcabi-github#731 . One Java developer is having a problem with his Git repository. Apparently he did something wrong and couldn't solve the problem by himself. He asked for help by submitting a new bug to the project. He was paid for the bug report. Then, another team member was assigned to help him. He did, through a number of suggestions and instructions. In the end, the problem was solved, and he was also paid for the solution. In total, the project spent 45 minutes, and the problem was solved. "},{"title":"Continuous Integration is Dead","url":"/2014/10/08/continuous-integration-is-dead.html","tags":["mgmt","devops"],"date":"2014-10-08 00:00:00 +0000","categories":["jcg","best"],"body":"A few days ago, my article \"Why Continuous Integration Doesn’t Work\" was published at DevOps.com . Almost the same day I received a few strongly negative critiques on Twitter. Here is my response to the un-asked question: Why the hell shouldn't continuous integration work, being such a brilliant and popular idea? Even though I have some experience in this area, I won't use it as an argument. I'll try to rely only on logic instead. BTW, my experience includes five years of using Apache Continuum, Hudson, CruiseControl, and Jenkins in over 50 open source and commercial projects. Besides that, a few years ago I created a hosted continuous integration service called fazend.com , renamed to rultor.com in 2013. Currently, I'm also an active user of Travis . How Continuous Integration Should Work The idea is simple and obvious. Every time you make a new commit to the master branch (or /trunk in Subversion), a continuous integration server (or service) attempts to build the entire product. \"Build\" means compile, unit test, integration test, quality analysis, etc. The result is either \"success\" or \"failure\". If it is a success, we say that \"the build is clean\". If it is a failure, we say that \"the build is broken\". The build usually gets broken because someone breaks it by commiting new code that turns previously passing unit tests into failing ones. This is the technical side of the problem. It always works. Well, it may have its problems, like hard-coded dependencies, lack of isolation between environments or parallel build collisions, but this article is not about those. If the application is well written and its unit tests are stable, continuous integration is easy. Technically. Let's see the organizational side. Continuous integration is not only a server that builds, but a management/organizational process that should \"work\". Being a process that works means exactly what Jez Humble said in Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation , on page 55: Crucially, if the build fails, the development team stops whatever they are doing and fixes the problem immediately This is what doesn't work and can't work. Who Needs This? As we see, continuous integration is about setting the entire development team on pause and fixing the broken build. Let me reiterate. Once the build is broken, everybody should focus on fixing it and making a commit that returns the build to the stable state. Now, my question is — who, in an actively working team, may need this? A product owner, who is interested in launching new features to the market as soon as possible? Or maybe a project manager, who is responsible for the deadlines? Or maybe programmers, who hate to fix someone else's bugs, especially under pressure. Who likes this continuous integration and who needs it? Nobody. What Happens In Reality? I can tell you. I've seen it multiple times. The scenario is always the same. We just start to ignore that continuous integration build status. Either the build is clean or it is broken, and we continue to do what we were doing before. We don't stop and fix it, as Jez Humble recommends. Instead, we ignore the information that's coming from the continuous integration server. Eventually, maybe tomorrow or on Monday, we'll try to find some spare time and will try to fix the build. Only because we don't like that red button on the dashboard and want to turn it into a green one. What About Discipline? Yes, there is another side of this coin. We can try to enforce discipline in the team. We can make it a strict rule, that our build is always clean and whoever breaks it gets some sort of a punishment. Try doing this and you will get a fear driven development . Programmers will be afraid of committing anything to the repository because they will know that if they cause a build failure they will have to apologize , at least. A strict discipline (which I'm a big fan of) in this case only makes the situation worse. The entire development process slows down and programmers keep their code to themselves for as long as possible, to avoid possibly broken builds. When it's time to commit, their changes are so massive that merging becomes very difficult and sometimes impossible. As a result you get a lot of throw-away code, written by someone but never committed to master , because of that fear factor. OK, What Is The Solution? I wrote about it before; it is called \"read-only master branch\" . It is simple — prohibit anyone from merging anything into master and create a script that anyone can call. The script will merge, test, and commit. The script will not make any exceptions. If any branch breaks at even one unit test, the entire branch will be rejected. In other words: raise the red flag before the code gets into master . This solves all problems. First, the build is always clean. We simply can't break it because nobody can commit unless his code keeps the build clean. Second, there is no fear of breaking anything. Simply because you technically can't do it. All you can do is get a negative response from a merging script. Then you fix your errors and tell the script to try again. Nobody sees these attempts, and you don't need to apologize. Fear factor is gone. BTW, try to use rultor.com to enforce this \"read-only master branch\" principle in your project. "},{"title":"What Does a Software Architect Do?","url":"/2014/10/12/who-is-software-architect.html","tags":["mgmt"],"date":"2014-10-12 00:00:00 +0000","categories":["jcg"],"body":"Do you have a software architect in your project? Do you need one? Well, most agile teams do not define such a role explicitly and work in a democratic mode. Every important technical decision is discussed with the entire team, and the most voted for solution wins. When such a team eventually decides to put a \"software architect\" badge on someone's t-shirt, the most reputable programmer gets it. The badge rarely changes his responsibilities, though. After all, the team stays the same and enjoys having technical discussions together, involving everyone. In the end, a software architect is more of a status than a role with explicitly defined responsibilities. It is a sign of respect, paid by other team players to the oldest and the most authoritative one among them. Right? Absolutely wrong! Obviously, an architect is usually someone who has the most knowledge, skills, experience, and authority. Of course, an architect usually knows more than others and is able to communicate his knowledge with diplomacy and pedagogy when required. An architect is usually one of the smartest guys on the team. This is not, however, what makes him/her an architect. And this is not what the team needs. My definition of a software architect is this: An architect is the one who takes the blame for the quality . You can replace \"blame\" with accountability or responsibility. Although, I prefer to use \"blame\", because it much better emphasizes the fact that every quality issue in the product under development is a personal fault of the architect. Of course, together with the blame he also takes all the credit from happy customers, when the quality is good. This is what the team needs — someone personally responsible for the quality of the software being developed. How this guy will delegate this responsibility to others is his job. Whether he will use his knowledge and skills, or quality control tools, or unit testing frameworks, or authority, or coaching, or corporal punishment — it's his business. A project manager delegates quality control to the software architect, and it is up to the software architect how to delegate it further. The role of a software architect is crucial for every project, even if there are just two coders working at the same desk. One of them has to be the architect. An ideal architect has all the merits mentioned above. He listens to everybody and takes their opinions into account. He is a good coach and a teacher, with a lot of patience. He is an effective communicator and negotiator. He is a diplomat. And he is an expert in the technical domain. But, even if he doesn't have all these merits, his decision is always final . And this is the job of the project manager, to make sure that every technical decision the architect makes is not doubted by anyone. This is what delegation is all about — responsibility should always come with power. As a project manager, you should regularly evaluate the results of your architect. Remember, the quality of the product your team is working on is his personal (!) reponsibility. Any problems you see are his problems. Don't be afraid to blame him and punish him. But, always remember that in order to make your punishments productive you should give your architect full power in his actions. Let me reiterate: his decisions should be final. If you, as a project manager, are not happy with the quality of the product and the architect doesn't improve the situation, replace him. Downgrade him to a programmer and promote one of the programmers to an architect. But always remember that there can only be one architect in the team, and that his decisions are final. That's the only way of having a chance of building a perfect product. "},{"title":"How We Write a Product Vision","url":"/2014/10/20/how-we-write-product-vision.html","tags":["mgmt"],"date":"2014-10-20 00:00:00 +0000","categories":[],"body":" Every software project we work with is started from a Product Vision document. We create it during our Thinking phase. Even though the document is as short as two pages of English text, its development is the most painstaking task in the whole project. There are a few tricks and recommendations which I'd like to share. We usually design a Product Vision in four sections: product statement, stakeholders and needs, features, and quality requirements. Product Statement Product Statement is a one-paragraph declaration of intent, explaining to an absolute stranger what this product is about and what it is for. It is very similar to an elevator pitch . The Statement must answer these questions, preferrably in this specific order: Who is the customer? What does she want? What is the market offering now? What is wrong with existing offers? How will our product fix this? You should answer all these questions in less than 60 words altogether. If you need more words, something is wrong with your understanding of the product under development. If you can answer them in 20 words, your product will conquer the world. By the way, don't confuse a Product Statement with a Mission , which is a much broader declaration of an overall goal of your business. You may have a hundred products but only a single mission. For example, Disney says that its mission is: \"to make people happy\". They've got hundreds of products that help them accomplish this mission. And each product has its own Product Statement. I find these articles helpful: The Product Vision , Agile Artifacts: The Product Vision Statement , The Art of Agile Development: Vision . Stakeholders and Needs This section must list everybody whose life will be affected by the product (positively or negatively). Your list of stakeholders may include: sponsors, developers, users, competitors, government, banks, web hosting providers, Apple Store, hackers, etc. It is very important to list both positive and negative stakeholders. If your product is going to automate some routine manual operations, don't forget that someone will be made redundant because of it. No matter how \"good\" your product is, there is always an \"evil\" side. The invention of the iPhone made millions of people happy, but also caused a lot of trouble for Nokia and Blackberry. An eventual invention of a cancer vaccine will make millions of people healthier, but will also make thousands of oncologists jobless. My point is that any project has both positive and negative stakeholders. Each stakeholder must have a list of needs. They have to be simple and straight forward, like \"earn money\", \"increase profit\", \"share photos\", or \"host a website\". I would recommend defining one or two needs for each stakeholder. If there are more than three, think again — do you really understand what your stakeholders need? Your project will be considered successful if you satisfy all the needs of all your positive stakeholders and neutralize negative ones. This Stakeholder Needs and Requirements article from SEBOK will be helpful. Actors and Features In this section we list actors (entities communicating with the product) and the key functionalities they use. This is the most abstract definition of functional requirements of the product. It doesn't need to be detailed. Instead, it has to be very high-level and abstract. For example, this is how our interaction with a well-known product may be described in two lines: User can post tweets, read tweets of his friends, follow new friends and re-tweet their tweets. Is it clear for a stranger what we're talking about here? Absolutely not — what is a \"tweet\", what does it mean to \"follow\" and what is a \"re-tweet\"? These questions have no answers in the Product Vision document, but it's clear that a user will have four main features available. All other features will be similar to those. Twitter is a multi-billion dollar business with a multi-million dollar product. However, we managed to explain its key features in just two lines of text. You should do the same with your product. If you can't fit all its features into just two-three lines, reconsider your understanding of the product you're going to develop. Also, read about \"feature bloat dilemma\" . Each actor must have at least three and at most six features. If there are more, you should group them somehow. If there are less, break them into smaller and more detailed features. Quality Requirements This section lists all important non-functional requirements. Any product may have hundreds of quality requirements , as well as hundreds of features. However, a Product Vision document must be focused on the most important ones. Consider some examples: Any web page must open in less than 300ms. Total cost of ownership must be less than $5000/mo. Mobile app must be tailored for 10+ popular screen sizes. Mean time to recover must be less than 2 hours. DB must be scalable up to 5Tb without cost increases. It is also very important to keep requirements measurable (like each of these examples). Every line in this section is a message to product developers. They will read this document in order to understand what is most important to the sponsor of the project. For example, these quality requirements are useless: \"user interface must be attractive\", \"web site must be fast\" or \"the system must be stable\". They are not measurable or testable. All they do is distract developers. If you can't make a strict and measurable statement about your quality objectives, don't write anything. It's better to say nothing than set false or ambiguous goals here. Try to keep this section short. There should be six quality requirements, at most. Remove Noise Every section must be no more than twenty lines in length. Even if you're developing a Google killer with a $50 million dollar budget, your Vision document must be as short as two pages. For most of my clients this is a very complex and brain damaging task. They usually come to us with a 50-page document explaining their business ideas with all the important details. From this document, we should only extract information that really makes a difference. The Product Vision document must keep its reader on the highest level of abstraction. The document must take less than a minute to read, from start to finish. If you can't keep it short — you don't understand your product well enough. Example Here is an example of a very simple Product Vision for a Facebook killer: Statement Facebook doesn't allow users to purchase \"likes\", our social network will have this. Stakeholders and Needs Sponsor: to raise investments. Developer: to earn money by programming. Users: to share photos and purchase popularity. Bank: to make commission on every purchase. Government: to protect society against abusive content. Competitors: to wipe us off the market. Actors and Features User can create account, upload photos, share photos, send personal messages, like other photos, purchase likes. Admin can ban user accounts, view summary reports, authorize credit card transactions, configure system parameters, monitor server resource usage. Bank can process credit card transactions. Quality Requirements Any page must open in less than 300ms. User interface must be attractive and simple. Availability must be over 99.999%. Test coverage must be over 80%. Development pipeline must be fully automated. Interfaces must include web site and iOS/Android app. Diplomacy We follow all these recommendations in our projects, in teamed.io . You can use them in your projects as well, but keep in mind that the process of defining a Product Vision could be very painful. You may sometimes offend a client by over-simplifying their \"great\" business idea. 'Really? I'm ready to pay $250,000 for something awesome and you're telling me that you've only got ten lines for it? Huh?' To work around this situation, split the client's documentation into two parts. The first part will fit into the Product Vision document; the second one will be called \"supplementary documentation\" and will contain all that valuable information you've got from the client. You may use that documentation later, during the course of product development. But don't cut corners. Don't allow your client (or anyone else) to force you to bloat the Product Vision. The document has to be very short and explicit. No lyrics, only statements. "},{"title":"Incremental Billing","url":"/2014/10/21/incremental-billing.html","tags":["xdsd","mgmt"],"date":"2014-10-21 00:00:00 +0000","categories":[],"body":"When you hire a software developer (individual or a team), there are basically two types of contracts: fixed price or time-and-material . They are fundamentally different but the truth is that in either case — you lose . In the eXtremely Distributed Software Development ( XDSD ) methodology everything is different, including the way we invoice our clients. Let's see what happens in traditional contracts and what changes in XDSD, which we practice in Teamed.io . The difference between fixed-cost and T&M is in who takes the risk of spending money and getting nothing in return. This risk is huge in the software development industry, especially in outsourcing. Over 80% of all software projects fail to achieve their objectives and about 30% of startups fail by running out of cash. However, very few programmers (if any) fail to get their monthly salaries on time. What does this tell us? I guess it means that in all failures you — the client — will be the loser. Time and Material In T&M you will simply pay and pray. If your programmers appear to be honest workaholics you may get lucky and get something done. As you can see from the numbers above, however, this is rarely the case. Don't fool yourself; there won't be any workaholics in your project. Even if you adopt micro-management and corporal punishment, your overall costs will be much higher than expected and the quality will suffer. This is what a monthly T&M invoice will look like. You will pay for the time spent by programmers pretending to be working on your project. Well, as I said above, some of them will ocassionally do something useful, but overall statistics tell us that most of that time will be wasted. No matter how good or bad the code written during that month — you still have to pay the bill. How many more invoices you will get until the product is done? Nobody knows. In the end — you lose. Fixed Price In Fixed Price you will feel secure at the beginning — \"the statement of work specifies everything and the price is fixed, how can I lose?\" According to the statistics above, however, programmers are much smarter than their clients. You will lose in quality. Yes, you will get something for that fixed price, but it will be a throw-away software. And when you decide to modify it, new costs will bubble up. In the end, the whole project will be ruined and your money will simply be turned into programmers' salaries. This model is even more risky than T&M, where you at least have a chance. Once in a while you will receive an invoice with a list of milestones reached. Every milestone will contain a certain set of features implemented in the product. Keep in mind that the primary motivation of your programmers will be to do less and charge more. Every time you ask for improvements or corrections, there will be a fight about budget. You will either give up and lose a lot of money or your team will significantly jeopardize quality, in order to stay profitable. In either case — you lose. Incremental Billing So, what is the solution? Is it possible to have win-win contracts with programmers? Yes, it is. We call it \"Incremental Billing\". Remember, in XDSD we work with a stream of micro-tasks, usually completed in less than an hour. Each completed task produces a new increment (aka a \"release\" or \"version\") of software. An increment could be a bug fix, a bug report, a new feature or a micro-step towards any of these. By the end of a week you get a bill that lists every single increment delivered during the week, the amount of time spent on its development and its total cost. Every increment costs you 30-60 minutes of a programmer's time (plus our fees). Besides that, by the end of the week, you get an updated version of a project plan, with a re-estimated budget. Thus, you see what was done so far and how much needs to be done, according to our estimate. How does this help you not lose/waste money? Here's how: you fully control your budget you pay only for the work completed you track the progress with few-minutes-granularity you don't pay for meetings, chats, lunches or coffee breaks programmers stay very motivated, since they are paid by result there is no long-term commitment, and you can stop at any time every increment passes all quality checks As you can see, XDSD methodology not only improves the way we develop software but also fixes the flaws in the way you pay for it. Since it is a win-win model, it is beneficial for both programmers and for you — the paying project sponsor. "},{"title":"Paired Brackets","url":"/2014/10/23/paired-brackets-notation.html","tags":["java","oop","programming"],"date":"2014-10-23 00:00:00 +0000","categories":[],"body":"Here is a notation rule I'm using in Java code: a bracket should either start/end a line or be paired on the same line . The notation applies universally to any programming language (incl. Java, Ruby, Python, C++, PHP, etc.) where brackets are used for method/function calls. Here is how your code will look, if you follow this \"Paired Brackets\" notation: 1 2 3 4 5 6 7 8 9 10 11 12 new Foo ( // ends the line Math . max ( 10 , 40 ), // open/close at the same line String . format ( \"hello, %s\" , new Name ( Arrays . asList ( \"Jeff\" , \"Lebowski\" ) ) ) // starts the line ); Obviously, the line with a closing bracket should start at the same indentation level as the line with its opening pair. This is how your IDE will render the code if you follow this notation (IntelliJ IDEA): Sublime Text will also appreciate it: As you see, those light vertical lines at the left side of the code help you to navigate, if you follow the notation. Those multiple closing brackets may look strange to you at the beginning — but give yourself some time and you will get used to them :) Fluent This is how I would recommend formatting fluent method calls (this is Java in NetBeans ): Arrays Here is how you format an array in \"Paired Brackets\" notation (this is Ruby in RubyMine ): As you see, the same principle applies to square and curled brackets. JSON The same principle is applicable to JSON formatting. This is a small JSON document in Coda 2 : JavaScript JavaScript should also follow the same principle. This is how your .js code would look in Atom : Python Finally, here is Python in PyCharm : "},{"title":"Are You a Hacker or a Designer?","url":"/2014/10/26/hacker-vs-programmer-mentality.html","tags":["mgmt","programming"],"date":"2014-10-26 00:00:00 +0000","categories":[],"body":"Twenty years ago, the best programmer was the one capable of fitting an entire application into a 64Kb .COM file. Those who were able to get the most out of that poor Intel 80386 were the icons of programming. That's because twenty years ago computers were expensive and programmers were cheap. That was the time of the \"hacker mentality\". That time is over. That mentality is not appreciated any more, because the market situation is completely opposite. Today, computers are cheap and programmers are expensive. This is the era of the \"designer mentality\", when the readability of our code is much more important than its performance. Prices vs Salaries Look at this graph. It is a comparison of two trends over the last twenty years (1994-2014). The first trend falls down and shows how much cheaper computer memory and HDD storage have become over the last twenty years. The second trend demonstrates how much software developers' salaries escalated over the same period. More accurately, they tripled. I didn't find an official report about that, but I'm sure it's no secret to anyone that the salaries of programmers keep growing — $200,000 per year for a senior developer is not a dream any more... while twenty years ago $60K was the best offer around. I found this article very interesting about this very subject. Basically, this means that in order to create a PHP website in 1994 we had to spend 1000 times more on hardware and three times less on programmers than we do now, in 2014. And we're talking about the same stack of technologies here. The same Linux box with an Apache HTTP Server inside. The difference is that in 1994, if our application had performance problems because of hardware limitations, we paid $35,000 per each additional gigabyte of RAM, while in 2014 we pay $10. In 1994 it was much more efficient to hire more programmers and ask them to optimize the code or even rewrite it, instead of buying new hardware. In 2014 the situation is exactly the opposite. It is now much cheaper to double the size of the server (especially if the server is a virtual cloud one) instead of paying salaries for optimizing the software. In 1994 the best engineers had that \"hacker mentality\", while in 2014 the \"designer mentality\" is much more appreciated. Hacker Mentality Someone with a hacker mentality would call this Fibonacci Java method an \"elegant code\" (would you?): public int f ( int n ) { return n > 2 ? f ( n - 1 )+ f ( n - 2 ): 1 ; } I would highlight these qualities of a good hacker : uses all known (and unknown) features of a programming language discriminates others as hackers and newbies and writes for hackers gets bored and frustrated by rules and standards doesn't write unit tests — juniors will write them later enjoys fire-fighting — that's how his talent manifests prefers talks over docs, since they are much more fun hates to see his code being modified by someone else likes to dedicate himself to one project at a time A hacker is a talented individual. He wants to express his talent in the software he writes. He enjoys coding and does it mostly for fun. I would say, he is married to his code and can't imagine its happy life after an eventual divorce. Code ownership is what a hacker is about — he understands himself as an \"owner\" of the code. When I ask one of my hacker friends — \"How will someone understand what this code does?\" I almost always hear the same answer — \"They will ask me!\" (usually said very proudly, with a sincere smile). Designer Mentality Someone with a designer mentality would refactor the code above to make it easier to read. He would call this Java function an \"elegant code\" (how about you?): public int fibo ( final int pos ) { final int num ; if ( pos > 2 ) { num = fibo ( pos - 1 ) + fibo ( pos - 2 ); } else { num = 1 ; } return num ; } I think these qualities can be attributed to a good designer: tends to use traditional programming techniques assumes everybody is a newbie and writes accordingly enjoys setting rules and following them prefers docs over talks and automation over docs spends most of his coding time on unit tests hates fire-fighting and working over time loves to see his code being modified and refactored works with a few projects at the same time A designer is a talented team player. He contributes to the team processes, standards, rules, education, and discipline as much as he contributes to the source code. He always makes sure that once he leaves the project his code and his ideas stay and work. The highest satisfaction for a good designer is to see his code living its own life — being modified, improved, refactored and eventually retired. A designer sees himself as a parent of the code — once it is old enough to walk and talk, it has to live its own life. The Future If you consider yourself a hacker, I believe it's time to change. The time of hackers is fading out. In the near future we will probably even stop thinking in terms of \"hardware\" and will run our applications in elastic computational platforms with unlimited amounts of memory, CPU power and storage space. We will simply pay for resource utilization and almost any performance issue will just add a few extra dollars to our monthly bills. We won't care about optimization any more. At the same time, good software engineers will become more and more expensive and will charge $500+ per hour just to check out software and give a diagnosis. Just like good lawyers or dentists. That's why, while developing a new software product, those who pay for it will care mostly about its maintainability. Project sponsors will understand that the best solution they can get for their money is the one that is the most readable, maintainable, and automated. Not the fastest. "},{"title":"How Much Do You Cost?","url":"/2014/10/29/how-much-do-you-cost.html","tags":["mgmt"],"date":"2014-10-29 00:00:00 +0000","categories":[],"body":" I'm getting a few emails every day from programmers interested in working with Teamed.io remotely. The first question I usually ask is \"what is your rate?\" (we pay by the hour ) What surprises me is how often people incorrectly estimate themselves, in both directions. I hear very different numbers, from $5 to $500 per hour. I never say no, but usually come up with my own hourly rate estimate. This article explains what factors I do and don't take into account. These are my personal criteria; don't take them as an industry standard. I do find them objective and logical, though — so let me explain. Open Source Contribution This is the first and the most important characteristic of a software developer. Do you contribute to open source projects? Do you have your own open source libraries that are used by some community? Do you write code that is publicly available and used? If you have nothing to show here, I see three posible causes. First, you're too shy to share your code because it's crap . Obviously, this is not a good sign. Not because your code could be bad, but because you're not brave enough to face this fact and improve. In our teams we pay a lot of attention to the quality of code and most of our new team members get surprised by just how high our quality bar is. You will also be surprised. The question is whether you will be able to adapt and improve or if you will give up and quit. If you didn't share your code before and have never dealt with negative feedback, you won't feel comfortable in our projects, where quality requirements are very high. The second possible cause is that you work from nine till five, for food, without passion . Actually, nobody manifests it that way. Instead, I often hear something like \"my company doesn't pay me for open source contribution and at home I want to spend time with my family\". In modern software development, most of the code we're working with is open source — libraries, frameworks, tools, instruments, etc. Almost everything you're using in your commercial projects is open source. By paying your salary your employer does already invest in open source products, because you're an active user of them. The problem is that you are not interested in becoming more active in that contribution. I see this as a lack of passion and self-motivation. Will you be an effective developer in our projects? Not at all, because our entire management model relies on self-motivation . The last possible cause is that you don't know what to write and where to contribute, which means lack of creativity . As I mentioned above, almost everything we're using now is open source, and these tools are full of bugs and not-yet-implemented features. At the same time, you don't see any areas for improvement? You don't know what can be done better? You're not able to at least find, report and fix one bug in some open source product you're using every day? This means that you won't be able to find areas of improvement in our projects either, while we rely on your ability to discover problems creatively . Thus, if your Github account is empty and your CV doesn't position you as \"an active contributor to Linux kernel\" (yeah, why not?), I immediately lose interest. On the other hand, when I see a 100+ stars project in your Github account, I get excited and ready to offer a higher rate. Geographic Location It is a common practice to pay higher rates to those who live in more expensive countries. When I'm getting resumes from San Francisco programmers, their rates are $70+ per hour. The same skills and experience cost $15-20 in Karachi. The reason here is the cost of living — it is much higher in the US than in Pakistan. However, this reason doesn't sound logical to me. If you're driving a more expensive car, we have to pay you a higher salary? The same with the place to stay. You've chosen the country that you live in. You're using all the benefits of a well-developed country and you're paying for them. It's your choice. You decided to spend more money for the quality of your life — what does it have to do with me? Want to pay $30 for a lunch? Become a better engineer. Until then, buy a hot dog for a few bucks. Just saying that \"I'm already here and my lunch costs $30\" is not an argument. Thus, the more expensive the place you live, the less money stays in your pocket. For us this means that $100 will motivate a programmer from Karachi much stronger than the same $100 will motivate the same person, if she lives in San Francisco. Thus, we prefer to work with people whose expenses are lower. Our money will simply work better. StackOverflow.com Reputation We all know what StackOverflow has but very few people (suprisingly few!) actively contributing to it. If your profile there is empty (or you don't have one) I realize that you 1) don't have any questions to ask and 2) you have nothing to answer. First, if you're not asking anything there, you are not growing. Your education process stopped some time ago, probably right after you got an office job. Or maybe you're too shy to ask? Or you can't describe your questions in an accurate and precise format? Or maybe all your questions already have answers? In any case, it's sad. Second, if you're not answering, you simply have nothing to say. In most cases, this means that you're not solving complex and unique problems. You're simply wiring together well-known components and collecting your paychecks. Very often I hear people saying that they solve most of their problems by asking their colleagues sitting next to them in the office. They say they simply don't need StackOverflow (or similar resources, if they exist) because their team is so great that any questions can be answered internally. That's good for the team and bad for you. Why? You don't have a very important skill — finding an answer in a public Internet. In our projects we discourage any horizontal communications between programmers, and you won't be able to get any help from anyone. You will be on your own and you will fail, because you are used to patronizing someone senior, in your office. StackOverflow is not just an indicator of how smart you are and how many upvotes you got for the \"best programming joke\" . It is proof that you can find answers to your questions by communicating with people you don't know. It is a very important skill. Years of Experience \"I've written Java for 10 years!\" — so what? This number means only one thing to me — you managed to survive in some office for ten years. Or maybe in a few offices. You managed to convince someone that he has to pay you for ten years of sitting in his building. Does it mean that you were writing something useful? Does it mean that your code was perfect? It doesn't mean any of that. Years of experience is a false indicator. It actually may play against you, in combination with other indicators mentioned above. If your CV says that you just started to program two years ago and your Github and StackOverflow accounts are empty — there is still a chance you will improve. You're just in the beginning of your career. However, if your CV says that you're a \"10-year seasoned architect\" with zero open source contribution — this means that you're either lying about that ten years or you're absolutely useless as an architect. My point is that the \"years of experience\" argument should be used very carefully. Play this card only if you have other merits. Otherwise, keep it to yourself. Certifications Oracle, Zend, Amazon, IBM, MySQL, etc. — I'm talking about these certifications. In order to get them you should pass an exam. Not an easy one and not online. It is a real exam taken in a certification center, where you're sitting in front of a computer for a few hours, without any books or Internet access, answering questions. Rather humiliating activity for a respected software developer? Indeed. And there is a high probability of failure, which is also rather embarassing. It is a very good sign, if you managed to go through this. If you've done it a few times, even better. However, if you've earned no certifications in your entire career, it is for one of the following reasons: First, you're afraid to lose . A serious certification may cost a few hundred dollars (I paid over $700 for SCEA ) and you will not get a refund if you fail. If you're afraid to lose, you're afraid to fight. This means you'll chicken out in a real-life situation, where a complex problem will need to be solved. Second, you don't invest in your profile. This most probably means that you don't want to change companies and prefer to find a peaceful office, where you can stay forever. I remember I once said to a friend of mine — \"you will greatly improve your CV if you pass this certification\". He answered with a smile — \"I hope I won't need a CV any more, I like this company\". This attitude is very beneficial for the company you're working for, but it definitely works against you. In my experience, the best team players are those who work for themselves. Healthy individualism is a key success factor. If your primary objective is to earn for yourself (money, reputation, skills, or knowledge) — you will be very effective in our projects. Certifications in your profile is an indicator of that healthy individualism we're looking for. Skills Variety The more technologies or programming languages you know, the less you cost. I'm not saying that it's not possible to be an expert in many things at the same time — that's entirely possible. But let me give you a pragmatic reason why you shouldn't — competition. There are thousands of \"Java7 programmers\" on the market — we can easily choose whoever we need. But there are not so many \"Hadoop programmers\" or \"XSLT designers\". If you focus on some specific area and become an expert there, your chances of finding a job are lower, but the payout will be bigger. We usually end up paying more to narrow-skilled specialists, mostly because we have no choice. If a project we're working on needs a Lucene expert, we'll find the right person and do our best to get him/her on board. Doing our best means increasing the price, in most cases. Thus, when I hear that you're \"experienced in MySQL, PostgreSQL, Oracle and Sqlite\" I realize that you know very little about databases. Talks and Publications I think it is obvious that having a blog (about programming, not about your favorite cat) is a positive factor. Even better is to be an occasional speaker at conferences or meetups. When it is a blog, I pay attention to the amount of comments people leave for your articles. If it is a conference, the most important criteria is how difficult it was to get to the list of speakers. Both blog articles and conference presentations make you much more valuable as a specialist. Mostly because these things demonstrate that some people already reviewed your work and your talent. And it was not just a single employer, but a group of other programmers and engineers. This means that we also can rely on your opinions. Besides that, if you write and present regularly, you have a very important skill/talent — you can present your ideas in a \"digestable\" way. In our projects we discourage informal communications and use ticketing systems instead. In those tickets you will have to explain your ideas, questions or concerns so that everybody can understand you. Without enough presentation skills, you won't survive in your projects. BTW, some software developers even file patents in their names — why can't you do this? Previous Employment I usually don't pay much attention to this section of your CV. Our management model is so different from anything you can see anywhere else that it doesn't really matter how many times you were fired before and how senior of a position you have/had with your full-time employer. Even if your title is \"CTO of Twitter\" — it doesn't mean anything to me. My experience tells me that the bigger the company and the higher the position in it — the further away you stay from the source code and from real technical decisions. VPs and CTOs spend most of their time on management meetings and internal politics. I'm much more interested in what you've done over the last few years than in where you've done it and what they called you while you were doing it. Education BSc, MSc, PhD... do we care? Not really. Education is very similar to the \"previous employment\" mentioned above. It doesn't really matter where exactly you've spent those five years after school. What matters is what have you done during that time. If you have nothing to say about your activity in the university than what will the name of it tell me? Well, of course, if it is Stanford or MIT, this will make a difference. In this case I can see that you managed to pass their graduation standards and managed to find money to study there. This is a good sign and will definitely increase your hourly rate. But if it is some mambo-jambo university from nowhere (like the one I graduated from), keep this information to yourself. Rates $100+ per hour we gladly pay to an expert who owns a few popular open source products; has a StackOverflow score above 20K; has certifications, articles, presentations, and maybe even patents. $50+ per hour we pay to a professional programmer who has open source projects on his own or is an active contributor; has a StackOverflow score over 5K; is writing about software development; possesses a few certifications. $30+ per hour we pay to a programmer who regularly contributes to open source code; is present in StackOverflow; has some certifications. $15 per hour we pay to everybody else. Don't get me wrong and don't take these numbers personally. The rate you're getting is a measurable metric of your professional level, not of you as a person. Besides, the level is not static, it is changing every day, and it's entirely in your hands. I wrote this article mostly in order to motivate you to grow. All these criteria are applicable to new members of our teams. Once you start writing some code, we measure your performance and you may get completely different numbers, see How Hourly Rate Is Calculated BTW, illustrations you see above are created by Andreea Mironiuc. "},{"title":"An Empty Line is a Code Smell","url":"/2014/11/03/empty-line-code-smell.html","tags":["design"],"date":"2014-11-03 00:00:00 +0000","categories":[],"body":"The subject may sound like a joke, but it is not. An empty line, used as a separator of instructions in an object method, is a code smell . Why? In short, because a method should not contain \"parts\". A method should always do one thing and its functional decomposition should be done by language constructs (for example, new methods), and never by empty lines. Look at this Java class (it does smell, doesn't it?): final class TextFile { private final File file ; TextFile ( File src ) { this . file = src ; } public int grep ( Pattern regex ) throws IOException { Collection < String > lines = new LinkedList <>(); try ( BufferedReader reader = new BufferedReader ( new FileReader ( this . file ))) { while ( true ) { String line = reader . readLine (); if ( line == null ) { break ; } lines . add ( line ); } } int total = 0 ; for ( String line : lines ) { if ( regex . matcher ( line ). matches ()) { ++ total ; } } return total ; } } This method first loads the content of the file. Second, it counts how many lines match the regular expression provided. So why does method grep smell? Because it does two things instead of one — it loads and it greps. If we make a rule, to avoid empty lines in method bodies, the method will have to be refactored in order to preserve the \"separation of concerns\" introduced by that empty line: final class TextFile { private final File file ; TextFile ( File src ) { this . file = src ; } public int grep ( Pattern regex ) throws IOException { return this . count ( this . lines (), regex ); } private int count ( Iterable < String > lines , Pattern regex ) { int total = 0 ; for ( String line : lines ) { if ( regex . matcher ( line ). matches ()) { ++ total ; } } return total ; } private Iterable < String > lines () throws IOException { Collection < String > lines = new LinkedList <>(); try ( BufferedReader reader = new BufferedReader ( new FileReader ( this . file ))) { while ( true ) { String line = reader . readLine (); if ( line == null ) { break ; } lines . add ( line ); } return lines ; } } } I believe it is obvious that this new class has methods that are much more cohesive and readable. Now every method is doing exactly one thing, and it's easy to understand which thing it is. This idea about avoiding empty lines is also applicable to other languages, not just Java/C++/Ruby, etc. For example, this CSS code is definitely begging for refactoring: .container { width : 80% ; margin-left : auto ; margin-right : auto ; font-size : 2em ; font-weight : bold ; } The empty line here is telling us (screaming at us, actually) that this .container class is too complex and has to be decomposed into two classes: .wide { width : 80% ; margin-left : auto ; margin-right : auto ; } .important { font-size : 2em ; font-weight : bold ; } Unfortunately, using empty lines to separate blocks of code is a very common habit. Moreover, very often I see empty blocks of two or even three lines, which are all playing this evil role of a separator of concerns. Needless to say, a properly designed class must have just a few public methods and a properly designed method must have up to ten instructions (according to Bob Martin). Empty lines inside methods encourage us to break this awesome rule and turn them into multi-page poems. Of course, it's easier to just click enter a few times and continue to code right in the same method, instead of thinking and refactoring first. This laziness will eventually lead to code that is hardly maintainable at all. To prevent this from happening in your projects, stop using empty lines inside methods, completely. Ideally, prohibit them in your automated build. In qulice.com , a static analysis tool we're using in all Java projects, we created a custom Checkstyle check that prohibits empty lines in every method. "},{"title":"How Immutability Helps","url":"/2014/11/07/how-immutability-helps.html","tags":["jcabi","java","oop"],"date":"2014-11-07 00:00:00 +0000","categories":["jcg"],"body":"In a few recent posts, including \"Getters/Setters. Evil. Period.\" , \"Objects Should Be Immutable\" , and \"Dependency Injection Containers are Code Polluters\" , I universally labelled all mutable objects with \"setters\" (object methods starting with set ) evil. My argumentation was based mostly on metaphors and abstract examples. Apparently, this wasn't convincing enough for many of you — I received a few requests asking to provide more specific and practical examples. Thus, in order to illustrate my strongly negative attitude to \"mutability via setters\", I took an existing commons-email Java library from Apache and re-designed it my way, without setters and with \"object thinking\" in mind. I released my library as part of the jcabi family — jcabi-email . Let's see what benefits we get from a \"pure\" object-oriented and immutable approach, without getters. Here is how your code will look, if you send an email using commons-email: Email email = new SimpleEmail (); email . setHostName ( \"smtp.googlemail.com\" ); email . setSmtpPort ( 465 ); email . setAuthenticator ( new DefaultAuthenticator ( \"user\" , \"pwd\" )); email . setFrom ( \"yegor@teamed.io\" , \"Yegor Bugayenko\" ); email . addTo ( \"dude@jcabi.com\" ); email . setSubject ( \"how are you?\" ); email . setMsg ( \"Dude, how are you?\" ); email . send (); Here is how you do the same with jcabi-email : Postman postman = new Postman . Default ( new SMTP ( \"smtp.googlemail.com\" , 465 , \"user\" , \"pwd\" ) ); Envelope envelope = new Envelope . MIME ( new Array < Stamp >( new StSender ( \"Yegor Bugayenko <yegor@teamed.io>\" ), new StRecipient ( \"dude@jcabi.com\" ), new StSubject ( \"how are you?\" ) ), new Array < Enclosure >( new EnPlain ( \"Dude, how are you?\" ) ) ); postman . send ( envelope ); I think the difference is obvious. In the first example, you're dealing with a monster class that can do everything for you, including sending your MIME message via SMTP, creating the message, configuring its parameters, adding MIME parts to it, etc. The Email class from commons-email is really a huge class — 33 private properties, over a hundred methods, about two thousands lines of code. First, you configure the class through a bunch of setters and then you ask it to send() an email for you. In the second example, we have seven objects instantiated via seven new calls. Postman is responsible for packaging a MIME message; SMTP is responsible for sending it via SMTP; stamps ( StSender , StRecipient , and StSubject ) are responsible for configuring the MIME message before delivery; enclosure EnPlain is responsible for creating a MIME part for the message we're going to send. We construct these seven objects, encapsulating one into another, and then we ask the postman to send() the envelope for us. What's Wrong With a Mutable Email? From a user perspective, there is almost nothing wrong. Email is a powerful class with multiple controls — just hit the right one and the job gets done. However, from a developer perspective Email class is a nightmare. Mostly because the class is very big and difficult to maintain. Because the class is so big , every time you want to extend it by introducing a new method, you're facing the fact that you're making the class even worse — longer, less cohesive, less readable, less maintainable, etc. You have a feeling that you're digging into something dirty and that there is no hope to make it cleaner, ever. I'm sure, you're familiar with this feeling — most legacy applications look that way. They have huge multi-line \"classes\" (in reality, COBOL programs written in Java) that were inherited from a few generations of programmers before you. When you start, you're full of energy, but after a few minutes of scrolling such a \"class\" you say — \"screw it, it's almost Saturday\". Because the class is so big , there is no data hiding or encapsulation any more — 33 variables are accessible by over 100 methods. What is hidden? This Email.java file in reality is a big, procedural 2000-line script, called a \"class\" by mistake. Nothing is hidden, once you cross the border of the class by calling one of its methods. After that, you have full access to all the data you may need. Why is this bad? Well, why do we need encapsulation in the first place? In order to protect one programmer from another, aka defensive programming . While I'm busy changing the subject of the MIME message, I want to be sure that I'm not interfered with by some other method's activity, that is changing a sender and touching my subject by mistake. Encapsulation helps us narrow down the scope of the problem, while this Email class is doing exactly the opposite. Because the class is so big , its unit testing is even more complicated than the class itself. Why? Because of multiple inter-dependencies between its methods and properties. In order to test setCharset() you have to prepare the entire object by calling a few other methods, then you have to call send() to make sure the message being sent actually uses the encoding you specified. Thus, in order to test a one-line method setCharset() you run the entire integration testing scenario of sending a full MIME message through SMTP. Obviously, if something gets changed in one of the methods, almost every test method will be affected. In other words, tests are very fragile, unreliable and over-complicated. I can go on and on with this \" because the class is so big \", but I think it is obvious that a small, cohesive class is always better than a big one. It is obvious to me, to you, and to any object-oriented programmer. But why is it not so obvious to the developers of Apache Commons Email? I don't think they are stupid or un-educated. What is it then? How and Why Did It Happen? This is how it always happens. You start to design a class as something cohesive, solid, and small. Your intentions are very positive. Very soon you realize that there is something else that this class has to do. Then, something else. Then, even more. The best way to make your class more and more powerful is by adding setters that inject configuration parameters into the class so that it can process them inside, isn't it? This is the root cause of the problem! The root cause is our ability to insert data into mutable objects via configuration methods, also known as \"setters\". When an object is mutable and allows us to add setters whenever we want, we will do it without limits. Let me put it this way — mutable classes tend to grow in size and lose cohesiveness . If commons-email authors made this Email class immutable in the beginning, they wouldn't have been able to add so many methods into it and encapsulate so many properties. They wouldn't be able to turn it into a monster. Why? Because an immutable object only accepts a state through a constructor. Can you imagine a 33-argument constructor? Of course, not. When you make your class immutable in the first place, you are forced to keep it cohesive, small, solid and robust. Because you can't encapsulate too much and you can't modify what's encapsulated. Just two or three arguments of a constructor and you're done. How Did I Design An Immutable Email? When I was designing jcabi-email I started with a small and simple class: Postman . Well, it is an interface, since I never make interface-less classes. So, Postman is... a post man. He is delivering messages to other people. First, I created a default version of it (I omit the ctor, for the sake of brevity): import javax.mail.Message ; @Immutable class Postman . Default implements Postman { private final String host ; private final int port ; private final String user ; private final String password ; @Override void send ( Message msg ) { // create SMTP session // create transport // transport.connect(this.host, this.port, etc.) // transport.send(msg) // transport.close(); } } Good start, it works. What now? Well, the Message is difficult to construct. It is a complex class from JDK that requires some manipulations before it can become a nice HTML email. So I created an envelope, which will build this complex object for me (pay attention, both Postman and Envelope are immutable and annotated with @Immutable from jcabi-aspects ): @Immutable interface Envelope { Message unwrap (); } I also refactor the Postman to accept an envelope, not a message: @Immutable interface Postman { void send ( Envelope env ); } So far, so good. Now let's try to create a simple implementation of Envelope : @Immutable class MIME implements Envelope { @Override public Message unwrap () { return new MimeMessage ( Session . getDefaultInstance ( new Properties ()) ); } } It works, but it does nothing useful yet. It only creates an absolutely empty MIME message and returns it. How about adding a subject to it and both To: and From: addresses (pay attention, MIME class is also immutable): @Immutable class Envelope . MIME implements Envelope { private final String subject ; private final String from ; private final Array < String > to ; public MIME ( String subj , String sender , Iterable < String > rcpts ) { this . subject = subj ; this . from = sender ; this . to = new Array < String >( rcpts ); } @Override public Message unwrap () { Message msg = new MimeMessage ( Session . getDefaultInstance ( new Properties ()) ); msg . setSubject ( this . subject ); msg . setFrom ( new InternetAddress ( this . from )); for ( String email : this . to ) { msg . setRecipient ( Message . RecipientType . TO , new InternetAddress ( email ) ); } return msg ; } } Looks correct and it works. But it is still too primitive. How about CC: and BCC: ? What about email text? How about PDF enclosures? What if I want to specify the encoding of the message? What about Reply-To ? Can I add all these parameters to the constructor? Remember, the class is immutable and I can't introduce the setReplyTo() method. I have to pass the replyTo argument into its constructor. It's impossible, because the constructor will have too many arguments, and nobody will be able to use it. So, what do I do? Well, I started to think: how can we break the concept of an \"envelope\" into smaller concepts — and this what I invented. Like a real-life envelope, my MIME object will have stamps. Stamps will be responsible for configuring an object Message (again, Stamp is immutable, as well as all its implementors): @Immutable interface Stamp { void attach ( Message message ); } Now, I can simplify my MIME class to the following: @Immutable class Envelope . MIME implements Envelope { private final Array < Stamp > stamps ; public MIME ( Iterable < Stamp > stmps ) { this . stamps = new Array < Stamp >( stmps ); } @Override public Message unwrap () { Message msg = new MimeMessage ( Session . getDefaultInstance ( new Properties ()) ); for ( Stamp stamp : this . stamps ) { stamp . attach ( msg ); } return msg ; } } Now, I will create stamps for the subject, for To: , for From: , for CC: , for BCC: , etc. As many stamps as I like. The class MIME will stay the same — small, cohesive, readable, solid, etc. What is important here is why I made the decision to refactor while the class was relatively small. Indeed, I started to worry about these stamp classes when my MIME class was just 25 lines in size. That is exactly the point of this article — immutability forces you to design small and cohesive objects . Without immutability, I would have gone the same direction as commons-email. My MIME class would grow in size and sooner or later would become as big as Email from commons-email. The only thing that stopped me was the necessity to refactor it, because I wasn't able to pass all arguments through a constructor. Without immutability, I wouldn't have had that motivator and I would have done what Apache developers did with commons-email — bloat the class and turn it into an unmaintainable monster. That's jcabi-email . I hope this example was illustrative enough and that you will start writing cleaner code with immutable objects. "},{"title":"Hits-of-Code Instead of SLoC","url":"/2014/11/14/hits-of-code.html","tags":["mgmt"],"date":"2014-11-14 00:00:00 +0000","categories":[],"body":"Lines-of-Code (aka SLoC) is a metric with a terrible reputation. Try to google it yourself and you'll find tons of articles bad-mouthing about its counter-effectiveness and destructiveness for a software development process. The main argument is that we can't measure the progress of programming by the number of lines of code written. Probably the most famous quote is attributed to Bill Gates : Measuring programming progress by lines of code is like measuring aircraft building progress by weight Basically, this means that certain parts of the aircraft will take much more effort at the same time being much lighter than others (like a central computer, for example). Instead of measuring the weight of the aircraft we should measure the effort put into it... somehow. So, here is the idea. How about we measure the amount of times programmers touch the lines. Instead of counting the number of lines we'll count how many times they were actually modified — we can get this information from Git (or any other SCM). The more you touch that part of the aircraft — the more effort you spent on it, right? I called it Hits-of-Code (HoC) and created a small tool to help us calculate this number in just one line. It's a Ruby gem , install it and run: $ gem install hoc $ hoc 54687 The number 54687 is a total number of Hits-of-Code in your code base. The principle behind this number is primitive — every time a line of code is modified, created or deleted in a Git commit, the counter increments. The main reason why this metric is better than LoC is that it is much better aligned with the actual effort invested into the code base. Here is why. It Always Increments The HoC metric always goes up. Today it can not be lower than it was yesterday — just like the effort, it always increments. Lines-of-Code is not acting like this. You may have a huge code base today, but after refactoring it will become much smaller. The number of lines of code is decreased. Does it mean you are less effective? Definitely not, but the LoC metric says so, to a non-programmer. A project manager, for example, may decide that since the size of the code base stayed the same over the last month, the team is not working. HoC doesn't have this counter-intuitive effect. Instead, HoC grows together with your every commit. The more you work on the code base, the bigger the HoC. It doesn't matter how big or small the absolute size of the your product. What matters is how much effort you put into it. That's why HoC is very intuitive and may be used as a measurement of software development progress. Look at this 18-month graph; it shows both metrics together. I used the same Java code base of rultor , a DevOps assistant . The code base experienced a major refactoring a few months ago, as you see on the graph. I think it is obvious which metric on this graph tells us more about the efforts being invested into the product. It Is Objective For HoC it doesn't matter how big the absolute size of the code base, but only how big your relative contribution to it. Let's say, you have 300K lines of code and 95% of them were copy-pasted from some third-party libraries (by the way, it is a very common and terrible practice — to keep third-party code inside your own repository). The amount of lines of code will be big, but the actual custom code part will be relatively small. Thus, the LoC metric will be misleading — it will always show 300K with small increments or decrements around it. Everybody will have a feeling that the team is working with 300K lines code base. On the other hand, HoC will always take into account the part of code that is actually being modified. The value of HoC will be objectively correlated with the actual effort of programmers working with the code base. It Exposes Complexity of Lines LoC is usually critized for its neutrality towards code complexity. An auto-generated ORM class or a complex sorting algorithm may have the same size in terms of lines of code, but the first takes seconds to write, while the second may take weeks or months. That's why lines of code is usually considered a false metric. Hits-of-Code takes complexity into account, because the longer you work with that sorting algorithm the more modifications you make to its lines. Well, this statement is true if you use Git regularly and commit your changes frequently — that is how you tell Git about your work progress. Conclusion Finally, look at this list of open projects completed by our team over the last few years. Every project has two metrics: Lines-of-Code and Hits-of-Code. It is interesting to see how relatively small projects have very big (over a million) HoC numbers. This immediately reminds me how much time we invested into it and how old they are. I used the HoC metric in this analysis: How much do you pay per line of code? . That post compares a traditional project that paid $3.98 per HoC and an open source one, managed by Teamed.io, that paid ¢13. My conclusion is that this Hits-of-Code metric can be used as a tool of progress tracking in a software development project. Moreover, it can be used for estimations of team size, project budget, development schedule and so forth. Obviously, LoC can't be the only metric, but in combination with others it may greatly help in estimating, planning and tracking. "},{"title":"Seven Virtues of a Good Object","url":"/2014/11/20/seven-virtues-of-good-object.html","tags":["oop"],"date":"2014-11-20 00:00:00 +0000","categories":["best"],"body":"Martin Fowler says : A library is essentially a set of functions that you can call, these days usually organized into classes. Functions organized into classes? With all due respect, this is wrong. And it is a very common misconception of a class in object-oriented programming. Classes are not organizers of functions. And objects are not data structures. So what is a \"proper\" object? Which one is not a proper one? What is the difference? Even though it is a very polemic subject, it is very important. Unless we understand what an object is, how can we write object-oriented software? Well, thanks to Java, Ruby, and others, we can. But how good will it be? Unfortunately, this is not an exact science, and there are many opinions. Here is my list of qualities of a good object. Class vs. Object Before we start talking about objects, let's define what a class is. It is a place where objects are being born (a.k.a. instantiated ). The main responsibility of a class is to construct new objects on demand and destruct them when they are not used anymore. A class knows how its children should look and how they should behave. In other words, it knows what contracts they should obey. Sometimes I hear classes being called \"object templates\" (for example, Wikipedia says so ). This definition is not correct because it places classes into a passive position. This definition assumes that someone will get a template and build an object by using it. This may be true, technically speaking, but conceptually it's wrong. Nobody else should be involved — there are only a class and its children. An object asks a class to create another object, and the class constructs it; that's it. Ruby expresses this concept much better than Java or C++: photo = File . new ( ' / tmp / photo . png ' ) The object photo is constructed by the class File ( new is an entry point to the class). Once constructed, the object is acting on its own. It shouldn't know who constructed it and how many more brothers and sisters it has in the class. Yes, I mean that reflection is a terrible idea, but I'll write more about it in one of the next posts :) Now, let's talk about objects and their best and worst sides. 1. He Exists in Real Life First of all, an object is a living organism . Moreover, an object should be anthropomorphized , i.e. treated like a human being (or a pet, if you like them more). By this I basically mean that an object is not a data structure or a collection of functions. Instead, it is an independent entity with its own life cycle, its own behavior, and its own habits. An employee, a department, an HTTP request, a table in MySQL, a line in a file, or a file itself are proper objects — because they exist in real life, even when our software is turned off. To be more precise, an object is a representative of a real-life creature. It is a proxy of that real-life creature in front of all other objects. Without such a creature, there is — obviously — no object. photo = File . new ( ' / tmp / photo . png ' ) puts photo . width () In this example, I'm asking File to construct a new object photo , which will be a representative of a real file on disk. You may say that a file is also something virtual and exists only when the computer is turned on. I would agree and refine the definition of \"real life\" as follows: It is everything that exists aside from the scope of the program the object lives in. The disk file is outside the scope of our program; that's why it is perfectly correct to create its representative inside the program. A controller, a parser, a filter, a validator, a service locator, a singleton, or a factory are not good objects (yes, most GoF patterns are anti-patterns!). They don't exist apart from your software, in real life. They are invented just to tie other objects together. They are artificial and fake creatures. They don't represent anyone. Seriously, an XML parser — who does it represent? Nobody. Some of them may become good if they change their names; others can never excuse their existence. For example, that XML parser can be renamed to \"parseable XML\" and start to represent an XML document that exists outside of our scope. Always ask yourself, \"What is the real-life entity behind my object?\" If you can't find an answer, start thinking about refactoring. 2. He Works by Contracts A good object always works by contracts. He expects to be hired not because of his personal merits but because he obeys the contracts. On the other hand, when we hire an object, we shouldn't discriminate and expect some specific object from a specific class to do the work for us. We should expect any object to do what our contract says. As long as the object does what we need, we should not be interested in his class of origin, his sex, or his religion. For example, I need to show a photo on the screen. I want that photo to be read from a file in PNG format. I'm contracting an object from class DataFile and asking him to give me the binary content of that image. But wait, do I care where exactly the content will come from — the file on disk, or an HTTP request, or maybe a document in Dropbox? Actually, I don't. All I care about is that some object gives me a byte array with PNG content. So my contract would look like this: interface Binary { byte [] read (); } Now, any object from any class (not just DataFile ) can work for me. All he has to do, in order to be eligible, is to obey the contract — by implementing the interface Binary . The rule here is simple: every public method in a good object should implement his counterpart from an interface. If your object has public methods that are not inherited from any interface, he is badly designed. There are two practical reasons for this. First, an object working without a contract is impossible to mock in a unit test. Second, a contractless object is impossible to extend via decoration . 3. He Is Unique A good object should always encapsulate something in order to be unique. If there is nothing to encapsulate, an object may have identical clones, which I believe is bad. Here is an example of a bad object, which may have clones: class HTTPStatus implements Status { private URL page = new URL ( \"http://www.google.com\" ); @Override public int read () throws IOException { return HttpURLConnection . class . cast ( this . page . openConnection () ). getResponseCode (); } } I can create a few instances of class HTTPStatus , and all of them will be equal to each other: first = new HTTPStatus (); second = new HTTPStatus (); assert first . equals ( second ); Obviously utility classes, which have only static methods, can't instantiate good objects. More generally, utility classes don't have any of the merits mentioned in this article and can't even be called \"classes\". They are simply terrible abusers of an object paradigm and exist in modern object-oriented languages only because their inventors enabled static methods. 4. He Is Immutable A good object should never change his encapsulated state. Remember, an object is a representative of a real-life entity, and this entity should stay the same through the entire life of the object. In other words, an object should never betray those whom he represents. He should never change owners. :) Be aware that immutability doesn't mean that all methods always return the same values. Instead, a good immutable object is very dynamic. However, he never changes his internal state. For example: @Immutable final class HTTPStatus implements Status { private URL page ; public HTTPStatus ( URL url ) { this . page = url ; } @Override public int read () throws IOException { return HttpURLConnection . class . cast ( this . page . openConnection () ). getResponseCode (); } } Even though the method read() may return different values, the object is immutable. He points to a certain web page and will never point anywhere else. He will never change his encapsulated state, and he will never betray the URL he represents. Why is immutability a virtue? This article explains in detail: Objects Should Be Immutable . In a nutshell, immutable objects are better because: Immutable objects are simpler to construct, test, and use. Truly immutable objects are always thread-safe. They help avoid temporal coupling. Their usage is side-effect free (no defensive copies). They always have failure atomicity. They are much easier to cache. They prevent NULL references . Of course, a good object doesn't have setters , which may change his state and force him to betray the URL. In other words, introducing a setURL() method would be a terrible mistake in class HTTPStatus . Besides all that, immutable objects will force you to make more cohesive, solid, and understandable designs, as this article explains: How Immutability Helps . 5. His Class Doesn't Have Anything Static A static method implements a behavior of a class, not an object. Let's say we have class File , and his children have method size() : final class File implements Measurable { @Override public int size () { // calculate the size of the file and return } } So far, so good; the method size() is there because of the contract Measurable , and every object of class File will be able to measure his size. A terrible mistake would be to design this class with a static method instead (this design is also known as a utility class and is very popular in Java, Ruby, and almost every OOP language): // TERRIBLE DESIGN, DON'T USE! class File { public static int size ( String file ) { // calculate the size of the file and return } } This design runs completely against the object-oriented paradigm. Why? Because static methods turn object-oriented programming into \"class-oriented\" programming. This method, size() , exposes the behavior of the class, not of his objects. What's wrong with this, you may ask? Why can't we have both objects and classes as first-class citizens in our code? Why can't both of them have methods and properties? The problem is that with class-oriented programming, decomposition doesn't work anymore. We can't break down a complex problem into parts, because only a single instance of a class exists in the entire program. The power of OOP is that it allows us to use objects as an instrument for scope decomposition. When I instantiate an object inside a method, he is dedicated to my specific task. He is perfectly isolated from all other objects around the method. This object is a local variable in the scope of the method. A class, with his static methods, is always a global variable no matter where I use him. Because of that, I can't isolate my interaction with this variable from others. Besides being conceptually against object-oriented principles, public static methods have a few practical drawbacks: First, it's impossible to mock them (Well, you can use PowerMock , but this will then be the most terrible decision you could make in a Java project ... I made it once, a few years ago). Second, they are not thread-safe by definition, because they always work with static variables, which are accessible from all threads. You can make them thread-safe, but this will always require explicit synchronization. Every time you see a public static method, start rewriting immediately. I don't even want to mention how terrible static (or global) variables are. I think it is just obvious. 6. His Name Is Not a Job Title The name of an object should tell us what this object is , not what it does , just like we name objects in real life: book instead of page aggregator, cup instead of water holder, T-shirt instead of body dresser. There are exceptions, of course, like printer or computer, but they were invented just recently and by those who didn't read this article. :) For example, these names tell us who their owners are: an apple, a file, a series of HTTP requests, a socket, an XML document, a list of users, a regular expression, an integer, a PostgreSQL table, or Jeffrey Lebowski. A properly named object is always possible to draw as a small picture. Even a regular expression can be drawn. In the opposite, here is an example of names that tell us what their owners do: a file reader, a text parser, a URL validator, an XML printer, a service locator, a singleton, a script runner, or a Java programmer. Can you draw any of them? No, you can't. These names are not suitable for good objects. They are terrible names that lead to terrible design. In general, avoid names that end with \"-er\" — most of them are bad. \"What is the alternative of a FileReader ?\" I hear you asking. What would be a better name? Let's see. We already have File , which is a representative of a real-world file on disk. This representative is not powerful enough for us, because he doesn't know how to read the content of the file. We want to create a more powerful one that will have that ability. What would we call him? Remember, the name should say what he is, not what he does. What is he? He is a file that has data; not just a file, like File , but a more sophisticated one, with data. So how about FileWithData or simply DataFile ? The same logic should be applicable to all other names. Always think about what it is rather than what it does. Give your objects real, meaningful names instead of job titles. 7. His Class Is Either Final or Abstract A good object comes from either a final or abstract class. A final class is one that can't be extended via inheritance. An abstract class is one that can't have children. Simply put, a class should either say, \"You can never break me; I'm a black box for you\" or \"I'm broken already; fix me first and then use\". There is nothing in between. A final class is a black box that you can't modify by any means. He works as he works, and you either use him or throw him away. You can't create another class that will inherit his properties. This is not allowed because of that final modifier. The only way to extend such a final class is through decoration of his children. Let's say I have the class HTTPStatus (see above), and I don't like him. Well, I like him, but he's not powerful enough for me. I want him to throw an exception if HTTP status is over 400. I want his method, read() , to do more that it does now. A traditional way would be to extend the class and overwrite his method: class OnlyValidStatus extends HTTPStatus { @Override public int read () throws IOException { int code = super . read (); if ( code > 400 ) { throw new RuntimException ( \"unsuccessful HTTP code\" ); } return code ; } } Why is this wrong? It is very wrong because we risk breaking the logic of the entire parent class by overriding one of his methods. Remember, once we override the method read() in the child class, all methods from the parent class start to use his new version. We're literally injecting a new \"piece of implementation\" right into the class. Philosophically speaking, this is an offense. On the other hand, to extend a final class, you have to treat him like a black box and decorate him with your own implementation (a.k.a. Decorator Pattern ): final class OnlyValidStatus implements Status { private final Status origin ; public OnlyValidStatus ( Status status ) { this . origin = status ; } @Override public int read () throws IOException { int code = this . origin . read (); if ( code > 400 ) { throw new RuntimException ( \"unsuccessful HTTP code\" ); } return code ; } } Make sure that this class is implementing the same interface as the original one: Status . The instance of HTTPStatus will be passed into him through the constructor and encapsulated. Then every call will be intercepted and implemented in a different way, if necessary. In this design, we treat the original object as a black box and never touch his internal logic. If you don't use that final keyword, anyone (including yourself) will be able to extend the class and ... offend him :( So a class without final is a bad design. An abstract class is the exact oposite case — he tells us that he is incomplete and we can't use him \"as is\". We have to inject our custom implementation logic into him, but only into the places he allows us to touch. These places are explicitly marked as abstract methods. For example, our HTTPStatus may look like this: abstract class ValidatedHTTPStatus implements Status { @Override public final int read () throws IOException { int code = this . origin . read (); if (! this . isValid ()) { throw new RuntimException ( \"unsuccessful HTTP code\" ); } return code ; } protected abstract boolean isValid (); } As you see, the class doesn't know how exactly to validate the HTTP code, and he expects us to inject that logic through inheritance and through overloading the method isValid() . We're not going to offend him with this inheritance, since he defended all other methods with final (pay attention to the modifiers of his methods). Thus, the class is ready for our offense and is perfectly guarded against it. To summarize, your class should either be final or abstract — nothing in between. "},{"title":"Five Principles of Bug Tracking","url":"/2014/11/24/principles-of-bug-tracking.html","tags":["mgmt"],"date":"2014-11-24 00:00:00 +0000","categories":[],"body":"A team working remotely requires much stronger discipline than a co-located crew sitting in the same office. First of all, I mean discipline of communications. At teamed.io , we have developed software remotely for the last five years. We manage tasks strictly through ticketing systems (like Github, JIRA, Trac, Basecamp, etc.) and discourage any informal communications, like Skype, HipChat, emails, or phone calls. Every ticket for us is an isolated task with its own life cycle, its own participants, and its own goal. Over these years, we've learned a few lessons that I want to share. If you also work remotely with your team, you may find them useful. Monty Python Flying Circus, TV Series (1969-1974) 1. Keep It One-on-One Each ticket (aka \"bug\") is a link between two people: problem specifier and problem solver. If it is a bug, I'm reporting it — you're solving it. If it is a question, I'm asking for an explanation — you're explaining. If it is a task, I'm ordering you to do it — you're doing it. In any case, there are two main characters. No matter how many people are involved in the ticket resolution, only these two characters have formal roles. The responsibility of the ticket reporter is to defend the problem . When I report a bug, I have to insist that it exists — this is my job. Others may tell me that I'm wrong and the bug is not there. They may tell me that they can't reproduce it. They may say that my description of a task is too vague and nobody understands it. There may be many issues of that kind. My job is to do the best I can in order to keep the ticket alive . Obviously, if the bug is not reproducible, I'll be forced to close the ticket. However, until the ticket is closed, I'm its guardian angel. :) On the other hand, the responsibility of the ticket solver is to defend the solution . When a ticket is assigned to me and I have to resolve it, my job is to convince the reporter that my solution is good enough. He may tell me that my solution is not sufficient, not the most efficient, or incomplete. My job is to insist that I'm right and he is wrong. Well, of course, in a reasonable way. And in order to create a solution that will be accepted as sufficient enough, I have to understand the problem first, investigate all possible options, and propose the most elegant implementation. But all this is secondary. The first thing I will be focused on is how to convince the reporter. I will always remember that my primary goal is to close the ticket . My point here is that no matter how many people are involved in the ticket discussion, always remember what is happening there — one person is selling his solution to another person. Everybody else around them is help or distraction (see below). 2. Close It! Remember that a ticket is not a chat. You're not there to talk. You're there to close . When the ticket is assigned to you, focus on closing it as soon as possible. Also, keep in mind that the sooner you close the ticket, the better job you will do for the project. Long-living tickets are a management nightmare. It is difficult to track them and control them. It's difficult to understand what's going on. Have you seen those two-year-old tickets in open source projects that have hundreds of comments and no deliverables? It is a mistake by their project managers and ticket participants. Each ticket should be short and focused — 1) a problem, 2) a refinement question, 3) a short explanation, 4) a solution, 5) closed, thanks everybody. This is an ideal scenario. As soon as you realize that your ticket is turning into a long discussion, try to close it even faster. How can I close it if the reporter doesn't like my solution? Find a temporary solution that will satisfy the reporter and allow you to close the ticket. Use \"TODO\" in your code or dirty workarounds — they are all better than a ticket hovering for a long time. Once you see that the solution is provided and is sufficient enough to close the ticket, ask its reporter to close it. Explicitly ask for that; don't dance arround with \"looks like this solution may be accepted, if you don't mind\". Be explicit in your intention to close the ticket and move on. Try this: \"@jeff, please close the ticket if you don't have any further questions.\" 3. Don't Close It! Every time you raise a bug and create a new ticket, you consume project resources. Every bug report means money spent on the project: 1) money for your time spent finding the problem and reporting it; 2) for the time of the project manager who is working with the ticket and finding who will fix it; 3) for the time of the ticket solver, who is trying to understand your report and provide a solution; and also 4) for the time of everybody else who will participate in the discussion. If you close the ticket without a problem being properly solved, you put this money into the trash bin. Once the ticket is started, there is no way back. We can't just say, \"Nah, ignore it; it's not important anymore.\" Your ticket already consumed project time and budget resources, and in order to turn them into something useful, you have to make sure that some solution is delivered. It can be a temporary solution. It can be a single line change in the project documentation. It can be a TODO marker in the code saying that \"we are aware of the problem but won't fix it because we're lazy\". Anything would work, but not nothing. Look at it from a different perspective. When you started that ticket, you had something in mind. Something was not right with the product. That's why you reported a bug. If you close the ticket without anyone even touching that place of code, someone else will have the same concern in a few days or a few years. And then the project will have to pay again for a similar ticket or discussion of the same problem. Even if you're convinced that the issue you found in the code is not really an issue, ask a ticket resolver to document it right in the source code in order to prevent such confusion from happening again in the future. 4. Avoid Noise — Address Your Comments Every time you post a message to the ticket, address it to someone. Otherwise, if you post just because you want to express your opinion, your comments become communication noise. Remember, a ticket is a conversation between two people — one of them reported an issue and the other one is trying to fix it. Comments like, \"How about we try another approach\" or \"I remember I had a similar issue some time ago\" are very annoying and distracting. Let's be honest, nobody really needs or cares about \"opinions\". All we need in a ticket is a solution(s). If you think the ticket should be closed because the introduced solution is good enough, address your comment to the ticket reporter. And start it with \"@jeff, I think the solution you've got already is good enough, because ...\" This way, you will help the assignee to close the ticket and move on. If you think the solution is wrong, address your comment to the assignee of the ticket, starting with \"@jeff, I believe your solution is not good enough because ...\" This way, you will help the ticket reporter keep the ticket open until a proper solution comes up. Again, don't pollute the air with generic opinions. Instead, be very specific and take sides — you either like the solution and want the ticket to be closed, or you don't like it and want the ticket to stay open. Everything in between is just making the situation more complex and isn't helping the project at all. 5. Report When It Is Broken I think it is obvious, but I will reiterate: Every bug has to be reproducible. Every time you report a bug, you should explain how exactly the product is broken. Yes, it is your job to prove that the software doesn't work as intended, or is not documented properly, or doesn't satisfy the requirements, etc. Every bug report should follow the same simple formula: \"This is what we have , this is what we should have instead, so fix it\". Every ticket, be it a bug, a task, a question, or a suggestion, should be formatted in this way. By submitting it, you're asking the project to move from point A to point B. Something is not right at point A, and it will be much better for all of us to be at that point B. So it's obvious that you have to explain where these points A and B are. It is highly desirable if you can explain how to get there — how to reproduce a problem and how to fix it. Even when you have a question, you should also follow that format. If you have a question, it means the project documentation is not sufficient enough for you to find an answer there. This is what is broken. You should ask for a fix. So instead of reporting, \"How should I use class X?\", say something like, \"The current documentation is not complete; it doesn't explain how I should use class X. Please fix.\" If you can't explain how to get there, say so in the ticket: \"I see that this class doesn't work as it should, but I don't know how to reproduce the problem and how to fix it.\" This will give everybody a clear message that you are aware that your bug report is not perfect. The first step for its resolver will be to refine the problem and find a way to reproduce it. If such a replica can't be found, obviously your bug will be forced into closing. Let me reiterate again: Every ticket is dragging the project from point A, where something is not right, to point B, where it is fixed. Your job, as a ticket reporter, is to draw that line — clearly and explicitly. "},{"title":"ORM Is an Offensive Anti-Pattern","url":"/2014/12/01/orm-offensive-anti-pattern.html","tags":["oop"],"date":"2014-12-01 00:00:00 +0000","categories":[],"body":"TL;DR ORM is a terrible anti-pattern that violates all principles of object-oriented programming, tearing objects apart and turning them into dumb and passive data bags. There is no excuse for ORM existence in any application, be it a small web app or an enterprise-size system with thousands of tables and CRUD manipulations on them. What is the alternative? SQL-speaking objects . Vinni-Pukh (1969) by Fyodor Khitruk How ORM Works Object-relational mapping (ORM) is a technique (a.k.a. design pattern) of accessing a relational database from an object-oriented language (Java, for example). There are multiple implementations of ORM in almost every language; for example: Hibernate for Java, ActiveRecord for Ruby on Rails, Doctrine for PHP, and SQLAlchemy for Python. In Java, the ORM design is even standardized as JPA . First, let's see how ORM works, by example. Let's use Java, PostgreSQL, and Hibernate. Let's say we have a single table in the database, called post : +-----+------------+--------------------------+ | id | date | title | +-----+------------+--------------------------+ | 9 | 10/24/2014 | How to cook a sandwich | | 13 | 11/03/2014 | My favorite movies | | 27 | 11/17/2014 | How much I love my job | +-----+------------+--------------------------+ Now we want to CRUD-manipulate this table from our Java app (CRUD stands for create, read, update, and delete). First, we should create a Post class (I'm sorry it's so long, but that's the best I can do): @Entity @Table ( name = \"post\" ) public class Post { private int id ; private Date date ; private String title ; @Id @GeneratedValue public int getId () { return this . id ; } @Temporal ( TemporalType . TIMESTAMP ) public Date getDate () { return this . date ; } public Title getTitle () { return this . title ; } public void setDate ( Date when ) { this . date = when ; } public void setTitle ( String txt ) { this . title = txt ; } } Before any operation with Hibernate, we have to create a session factory: SessionFactory factory = new AnnotationConfiguration () . configure () . addAnnotatedClass ( Post . class ) . buildSessionFactory (); This factory will give us \"sessions\" every time we want to manipulate with Post objects. Every manipulation with the session should be wrapped in this code block: Session session = factory . openSession (); try { Transaction txn = session . beginTransaction (); // your manipulations with the ORM, see below txn . commit (); } catch ( HibernateException ex ) { txn . rollback (); } finally { session . close (); } When the session is ready, here is how we get a list of all posts from that database table: List posts = session . createQuery ( \"FROM Post\" ). list (); for ( Post post : ( List < Post >) posts ){ System . out . println ( \"Title: \" + post . getTitle ()); } I think it's clear what's going on here. Hibernate is a big, powerful engine that makes a connection to the database, executes necessary SQL SELECT requests, and retrieves the data. Then it makes instances of class Post and stuffs them with the data. When the object comes to us, it is filled with data, and we should use getters to take them out, like we're using getTitle() above. When we want to do a reverse operation and send an object to the database, we do all of the same but in reverse order. We make an instance of class Post , stuff it with the data, and ask Hibernate to save it: Post post = new Post (); post . setDate ( new Date ()); post . setTitle ( \"How to cook an omelette\" ); session . save ( post ); This is how almost every ORM works. The basic principle is always the same — ORM objects are anemic envelopes with data. We are talking with the ORM framework, and the framework is talking to the database. Objects only help us send our requests to the ORM framework and understand its response. Besides getters and setters, objects have no other methods. They don't even know which database they came from. This is how object-relational mapping works. What's wrong with it, you may ask? Everything! What's Wrong With ORM? Seriously, what is wrong? Hibernate has been one of the most popular Java libraries for more than 10 years already. Almost every SQL-intensive application in the world is using it. Each Java tutorial would mention Hibernate (or maybe some other ORM like TopLink or OpenJPA) for a database-connected application. It's a standard de-facto and still I'm saying that it's wrong? Yes. I'm claiming that the entire idea behind ORM is wrong. Its invention was maybe the second big mistake in OOP after NULL reference . Actually, I'm not the only one saying something like this, and definitely not the first. A lot about this subject has already been published by very respected authors, including OrmHate by Martin Fowler, Object-Relational Mapping Is the Vietnam of Computer Science by Jeff Atwood, The Vietnam of Computer Science by Ted Neward, ORM Is an Anti-Pattern by Laurie Voss, and many others. However, my argument is different than what they're saying. Even though their reasons are practical and valid, like \"ORM is slow\" or \"database upgrades are hard\", they miss the main point. You can see a very good, practical answer to these practical arguments given by Bozhidar Bozhanov in his ORM Haters Don’t Get It blog post. The main point is that ORM, instead of encapsulating database interaction inside an object, extracts it away, literally tearing a solid and cohesive living organism apart. One part of the object keeps the data while another one, implemented inside the ORM engine (session factory), knows how to deal with this data and transfers it to the relational database. Look at this picture; it illustrates what ORM is doing. I, being a reader of posts, have to deal with two components: 1) the ORM and 2) the \"obtruncated\" object returned to me. The behavior I'm interacting with is supposed to be provided through a single entry point, which is an object in OOP. In the case of ORM, I'm getting this behavior via two entry points — the ORM and the \"thing\", which we can't even call an object. Because of this terrible and offensive violation of the object-oriented paradigm, we have a lot of practical issues already mentioned in respected publications. I can only add a few more. SQL Is Not Hidden . Users of ORM should speak SQL (or its dialect, like HQL ). See the example above; we're calling session.createQuery(\"FROM Post\") in order to get all posts. Even though it's not SQL, it is very similar to it. Thus, the relational model is not encapsulated inside objects. Instead, it is exposed to the entire application. Everybody, with each object, inevitably has to deal with a relational model in order to get or save something. Thus, ORM doesn't hide and wrap the SQL but pollutes the entire application with it. Difficult to Test . When some object is working a list of posts, it needs to deal with an instance of SessionFactory . How can we mock this dependency? We have to create a mock of it? How complex is this task? Look at the code above, and you will realize how verbose and cumbersome that unit test will be. Instead, we can write integration tests and connect the entire application to a test version of PostgreSQL. In that case, there is no need to mock SessionFactory , but such tests will be rather slow, and even more important, our having-nothing-to-do-with-the-database objects will be tested against the database instance. A terrible design. Again, let me reiterate. Practical problems of ORM are just consequences. The fundamental drawback is that ORM tears objects apart, terribly and offensively violating the very idea of what an object is . SQL-Speaking Objects What is the alternative? Let me show it to you by example. Let's try to design that class, Post , my way. We'll have to break it down into two classes: Post and Posts , singular and plural. I already mentioned in one of my previous articles that a good object is always an abstraction of a real-life entity. Here is how this principle works in practice. We have two entities: database table and table row. That's why we'll make two classes; Posts will represent the table, and Post will represent the row. As I also mentioned in that article , every object should work by contract and implement an interface. Let's start our design with two interfaces. Of course, our objects will be immutable. Here is how Posts would look: @Immutable interface Posts { Iterable < Post > iterate (); Post add ( Date date , String title ); } This is how a single Post would look: @Immutable interface Post { int id (); Date date (); String title (); } Here is how we will list all posts in the database table: Posts posts = // we'll discuss this right now for ( Post post : posts . iterate ()){ System . out . println ( \"Title: \" + post . title ()); } Here is how we will create a new post: Posts posts = // we'll discuss this right now posts . add ( new Date (), \"How to cook an omelette\" ); As you see, we have true objects now. They are in charge of all operations, and they perfectly hide their implementation details. There are no transactions, sessions, or factories. We don't even know whether these objects are actually talking to the PostgreSQL or if they keep all the data in text files. All we need from Posts is an ability to list all posts for us and to create a new one. Implementation details are perfectly hidden inside. Now let's see how we can implement these two classes. I'm going to use jcabi-jdbc as a JDBC wrapper, but you can use something else or just plain JDBC if you like. It doesn't really matter. What matters is that your database interactions are hidden inside objects. Let's start with Posts and implement it in class PgPosts (\"pg\" stands for PostgreSQL): @Immutable final class PgPosts implements Posts { private final Source dbase ; public PgPosts ( DataSource data ) { this . dbase = data ; } public Iterable < Post > iterate () { return new JdbcSession ( this . dbase ) . sql ( \"SELECT id FROM post\" ) . select ( new ListOutcome < Post >( new ListOutcome . Mapping < Post >() { @Override public Post map ( final ResultSet rset ) { return new PgPost ( rset . getInteger ( 1 )); } } ) ); } public Post add ( Date date , String title ) { return new PgPost ( this . dbase , new JdbcSession ( this . dbase ) . sql ( \"INSERT INTO post (date, title) VALUES (?, ?)\" ) . set ( new Utc ( date )) . set ( title ) . insert ( new SingleOutcome < Integer >( Integer . class )) ); } } Next, let's implement the Post interface in class PgPost : @Immutable final class PgPost implements Post { private final Source dbase ; private final int number ; public PgPost ( DataSource data , int id ) { this . dbase = data ; this . number = id ; } public int id () { return this . number ; } public Date date () { return new JdbcSession ( this . dbase ) . sql ( \"SELECT date FROM post WHERE id = ?\" ) . set ( this . number ) . select ( new SingleOutcome < Utc >( Utc . class )); } public String title () { return new JdbcSession ( this . dbase ) . sql ( \"SELECT title FROM post WHERE id = ?\" ) . set ( this . number ) . select ( new SingleOutcome < String >( String . class )); } } This is how a full database interaction scenario would look like using the classes we just created: Posts posts = new PgPosts ( dbase ); for ( Post post : posts . iterate ()){ System . out . println ( \"Title: \" + post . title ()); } Post post = posts . add ( new Date (), \"How to cook an omelette\" ); System . out . println ( \"Just added post #\" + post . id ()); You can see a full practical example here . It's an open source web app that works with PostgreSQL using the exact approach explained above — SQL-speaking objects. What About Performance? I can hear you screaming, \"What about performance?\" In that script a few lines above, we're making many redundant round trips to the database. First, we retrieve post IDs with SELECT id and then, in order to get their titles, we make an extra SELECT title call for each post. This is inefficient, or simply put, too slow. No worries; this is object-oriented programming, which means it is flexible! Let's create a decorator of PgPost that will accept all data in its constructor and cache it internally, forever: @Immutable final class ConstPost implements Post { private final Post origin ; private final Date dte ; private final String ttl ; public ConstPost ( Post post , Date date , String title ) { this . origin = post ; this . dte = date ; this . ttl = title ; } public int id () { return this . origin . id (); } public Date date () { return this . dte ; } public String title () { return this . ttl ; } } Pay attention: This decorator doesn't know anything about PostgreSQL or JDBC. It just decorates an object of type Post and pre-caches the date and title. As usual, this decorator is also immutable. Now let's create another implementation of Posts that will return the \"constant\" objects: @Immutable final class ConstPgPosts implements Posts { // ... public Iterable < Post > iterate () { return new JdbcSession ( this . dbase ) . sql ( \"SELECT * FROM post\" ) . select ( new ListOutcome < Post >( new ListOutcome . Mapping < Post >() { @Override public Post map ( final ResultSet rset ) { return new ConstPost ( new PgPost ( rset . getInteger ( 1 )), Utc . getTimestamp ( rset , 2 ), rset . getString ( 3 ) ); } } ) ); } } Now all posts returned by iterate() of this new class are pre-equipped with dates and titles fetched in one round trip to the database. Using decorators and multiple implementations of the same interface, you can compose any functionality you wish. What is the most important is that while functionality is being extended, the complexity of the design is not escalating, because classes don't grow in size. Instead, we're introducing new classes that stay cohesive and solid, because they are small. What About Transactions? Every object should deal with its own transactions and encapsulate them the same way as SELECT or INSERT queries. This will lead to nested transactions, which is perfectly fine provided the database server supports them. If there is no such support, create a session-wide transaction object that will accept a \"callable\" class. For example: final class Txn { private final DataSource dbase ; public < T > T call ( Callable < T > callable ) { JdbcSession session = new JdbcSession ( this . dbase ); try { session . sql ( \"START TRANSACTION\" ). exec (); T result = callable . call (); session . sql ( \"COMMIT\" ). exec (); return result ; } catch ( Exception ex ) { session . sql ( \"ROLLBACK\" ). exec (); throw ex ; } } } Then, when you want to wrap a few object manipulations in one transaction, do it like this: new Txn ( dbase ). call ( new Callable < Integer >() { @Override public Integer call () { Posts posts = new PgPosts ( dbase ); Post post = posts . add ( new Date (), \"How to cook an omelette\" ); posts . comments (). post ( \"This is my first comment!\" ); return post . id (); } } ); This code will create a new post and post a comment to it. If one of the calls fail, the entire transaction will be rolled back. This approach looks object-oriented to me. I'm calling it \"SQL-speaking objects\", because they know how to speak SQL with the database server. It's their skill, perfectly encapsulated inside their borders. "},{"title":"Synchronization Between Nodes","url":"/2014/12/04/synchronization-between-nodes.html","tags":["stateful"],"date":"2014-12-04 00:00:00 +0000","categories":[],"body":"When two or more software modules are accessing the same resource, they have to be synchronized. This means that only one module at a time should be working with the resource. Without such synchronization, there will be collisions and conflicts. This is especially true when we're talking about \"resources\" that do not support atomic transactions. To solve this issue and prevent conflicts, we have to introduce one more element into the picture. All software modules, before accessing the resource, should capture a lock from a centralized server. Once the manipulations with the resource are complete, the module should release the lock. While the lock is being captured by one module, no other modules will be able to capture it. The approach is very simple and well-known. However, I didn't find any cloud services that would provide such a locking and unlocking service over a RESTful API. So I decided to create one — stateful.co . No Retreat, No Surrender (1986) by Corey Yuen Here is a practical example. I have a Java web app that is hosted at Heroku. There are three servers (a.k.a. \"dynos\") running the same .war application. Why three? Because the web traffic is rather active, and one server is not powerful enough. So I have to have three of them. They all run exactly the same applications. Each web app works with a table in Amazon DynamoDB. It updates the table, puts new items into it, deletes some items sometimes, and selects them. So far, so good, but conflicts are inevitable. Here is an example of a typical interaction scenario between the web app and DynamoDB (I'm using jcabi-dynamo): Table table = region . table ( \"posts\" ); Item item = table . frame () . where ( \"name\" , \"Jeff\" ) . iterator (). next (); String salary = item . get ( \"salary\" ); item . put ( \"salary\" , this . recalculate ( salary )); The logic is obvious here. First, I retrieve an item from the table posts , then read its salary , and then modify it according to my recalculation algorithm. The problem is that another module may start to do the same while I'm recalculating. It will read the same initial value from the table and will start exactly the same recalculation. Then it will save a new value, and I will save one too. We will end up having Jeff's salary modified only once, while users will expect a double modification since two of them initiated two transactions with two different web apps. The right approach here is to \"lock\" the DynamoDB table first, even before reading the salary. Then do the modifications and eventually unlock it. Here is how stateful.co helps me. All I need to do is create a new named lock in the stateful.co web panel, get my authentication keys, and modify my Java code: Sttc sttc = new RtSttc ( new URN ( \"urn:github:526301\" ), // my Github ID \"9FF3-4320-73FB-EEAC\" // my secret key! ); Locks locks = sttc . locks (); Lock lock = locks . get ( \"posts-table-lock\" ); Table table = region . table ( \"posts\" ); Item item = table . frame () . where ( \"name\" , \"Jeff\" ) . iterator (). next (); new Atomic ( lock ). call ( new Callable < Void >() { @Override public void call () { String salary = item . get ( \"salary\" ); item . put ( \"salary\" , this . recalculate ( salary )); return null ; } } ); As you see, I wrap that critical transaction into Callable , which will be executed in isolation. This approach, obviously, doesn't guarantee atomicity of transaction — if part of the transaction fails, there won't be any automatic rollbacks and the DynamoDB table will be left in a \"broken\" state. Locks from stateful.co guarantee isolation in resource usage, and you can use any type of resources, including NoSQL tables, files, S3 objects, embedded software, etc. I should not forget to add this dependency to my pom.xml : <dependency> <groupId> co.stateful </groupId> <artifactId> java-sdk </artifactId> </dependency> Of course, you can do the same; the service is absolutely free of charge. And you can use any other languages, not just Java. BTW, if interested, contribute with your own SDK in your preferred language; I'll add it to the Github collection . "},{"title":"How an Immutable Object Can Have State and Behavior?","url":"/2014/12/09/immutable-object-state-and-behavior.html","tags":["oop"],"date":"2014-12-09 00:00:00 +0000","categories":[],"body":"I often hear this argument against immutable objects : \"Yes, they are useful when the state doesn't change. However, in our case, we deal with frequently changing objects. We simply can't afford to create a new document every time we just need to change its title .\" Here is where I disagree: object title is not a state of a document, if you need to change it frequently. Instead, it is a document's behavior . A document can and must be immutable, if it is a good object , even when its title is changed frequently. Let me explain how. Once Upon a Time in the West (1968) by Sergio Leone Identity, State, and Behavior Basically, there are three elements in every object: identity, state, and behavior. Identity is what distinguishes our document from other objects, state is what a document knows about itself (a.k.a. \"encapsulated knowledge\"), and behavior is what a document can do for us on request. For example, this is a mutable document: class Document { private int id ; private String title ; Document ( int id ) { this . id = id ; } public String getTitle () { return this . title ; } public String setTitle ( String text ) { this . title = text ; } @Override public String toString () { return String . format ( \"doc #%d about '%s'\" , this . id , this . text ); } } Let's try to use this mutable object: Document first = new Document ( 50 ); first . setTitle ( \"How to grill a sandwich\" ); Document second = new Document ( 50 ); second . setTitle ( \"How to grill a sandwich\" ); if ( first . equals ( second )) { // FALSE System . out . println ( String . format ( \"%s is equal to %s\" , first , second ) ); } Here, we're creating two objects and then modifying their encapsulated states. Obviously, first.equals(second) will return false because the two objects have different identities, even though they encapsulate the same state. Method toString() exposes the document's behavior — the document can convert itself to a string. In order to modify a document's title, we just call its setTitle() once again: first . setTitle ( \"How to cook pasta\" ); Simply put, we can reuse the object many times, modifying its internal state. It is fast and convenient, isn't it? Fast, yes. Convenient, not really. Read on. Immutable Objects Have No Identity As I've mentioned before , immutability is one of the virtues of a good object, and a very important one. A good object is immutable, and good software contains only immutable objects. The main difference between immutable and mutable objects is that an immutable one doesn't have an identity and its state never changes. Here is an immutable variant of the same document: @Immutable class Document { private final int id ; private final String title ; Document ( int id , String text ) { this . id = id ; this . title = text ; } public String title () { return this . title ; } public Document title ( String text ) { return new Document ( this . id , text ); } @Override public boolean equals ( Object doc ) { return doc instanceof Document && Document . class . cast ( doc ). id == this . id && Document . class . cast ( doc ). title . equals ( this . title ); } @Override public String toString () { return String . format ( \"doc #%d about '%s'\" , this . id , this . text ); } } This document is immutable, and its state ( id ad title ) is its identity. Let's see how we can use this immutable class (by the way, I'm using @Immutable annotation from jcabi-aspects ): Document first = new Document ( 50 , \"How to grill a sandwich\" ); Document second = new Document ( 50 , \"How to grill a sandwich\" ); if ( first . equals ( second )) { // TRUE System . out . println ( String . format ( \"%s is equal to %s\" , first , second ) ); } We can't modify a document any more. When we need to change the title, we have to create a new document: Document first = new Document ( 50 , \"How to grill a sandwich\" ); first = first . title ( \"How to cook pasta\" ); Every time we want to modify its encapsulated state, we have to modify its identity too, because there is no identity. State is the identity. Look at the code of the equals() method above — it compares documents by their IDs and titles. Now ID+title of a document is its identity! What About Frequent Changes? Now I'm getting to the question we started with: What about performance and convenience? We don't want to change the entire document every time we have to modify its title. If the document is big enough, that would be a huge obligation. Moreover, if an immutable object encapsulates other immutable objects, we have to change the entire hierarchy when modifying even a single string in one of them. The answer is simple. A document's title should not be part of its state . Instead, the title should be its behavior . For example, consider this: @Immutable class Document { private final int id ; Document ( int id ) { this . id = id ; } public String title () { // read title from storage } public void title ( String text ) { // save text to storage } @Override public boolean equals ( Object doc ) { return doc instanceof Document && Document . class . cast ( doc ). id == this . id ; } @Override public String toString () { return String . format ( \"doc #%d about '%s'\" , this . id , this . title ()); } } Conceptually speaking, this document is acting as a proxy of a real-life document that has a title stored somewhere — in a file, for example. This is what a good object should do — be a proxy of a real-life entity. The document exposes two features: reading the title and saving the title. Here is how its interface would look like: @Immutable interface Document { String title (); void title ( String text ); } title() reads the title of the document and returns it as a String , and title(String) saves it back into the document. Imagine a real paper document with a title. You ask an object to read that title from the paper or to erase an existing one and write new text over it. This paper is a \"copy\" utilized in these methods. Now we can make frequent changes to the immutable document, and the document stays the same. It doesn't stop being immutable, since it's state ( id ) is not changed. It is the same document, even though we change its title, becuase the title is not a state of the document. It is something in the real world, outside of the document. The document is just a proxy between us and that \"something\". Reading and writing the title are behaviors of the document, not its state. Mutable Memory The only question we still have unanswered is what is that \"copy\" and what happens if we need to keep the title of the document in memory? Let's look at it from an \"object thinking\" point of view. We have a document object, which is supposed to represent a real-life entity in an object-oriented world. If such an entity is a file, we can easily implement title() methods. If such an entity is an Amazon S3 object, we also implement title reading and writing methods easily, keeping the object immutable. If such an entity is an HTTP page, we have no issues in the implementation of title reading or writing, keeping the object immutable. We have no issues as long as a real-world document exists and has its own identity. Our title reading and writing methods will communicate with that real-world document and extract or update its title. Problems arise when such an entity doesn't exist in a real world. In that case, we need to create a mutable object property called title , read it via title() , and modify it via title(String) . But an object is immutable, so we can't have a mutable property in it — by definition! What do we do? Think. How could it be that our object doesn't represent a real-world entity? Remember, the real world is everything around the living environment of an object. Is it possible that an object doesn't represent anyone and acts on its own? No, it's not possible. Every object is a representantive of a real-world entity. So, who does it represent if we want to keep title inside it and we don't have any file or HTTP page behind the object? It represents computer memory . The title of immutable document #50, \"How to grill a sandwich\", is stored in the memory, taking up 23 bytes of space. The document should know where those bytes are stored, and it should be able to read them and replace them with something else. Those 23 bytes are the real-world entity that the object represents. The bytes have nothing to do with the state of the object. They are a mutable real-world entity, similar to a file, HTTP page, or an Amazon S3 object. Unfortunately, Java (and many other modern languages) do not allow direct access to computer memory. This is how we would design our class if such direct access was possible: @Immutable class Document { private final int id ; private final Memory memory ; Document ( int id ) { this . id = id ; this . memory = new Memory (); } public String title () { return new String ( this . memory . read ()); } public void title ( String text ) { this . memory . write ( text . getBytes ()); } } That Memory class would be implemented by JDK natively, and all other classes would be immutable. The class Memory would have direct access to the memory heap and would be responsible for malloc and free operations on the operating system level. Having such a class would allow us to make all Java classes immutable, including StringBuffer , ByteArrayOutputStream , etc. The Memory class would explicitly emphasize the mission of an object in a software program, which is to be a data animator . An object is not holding data; it is animating it. The data exists somewhere, and it is anemic, static, motionless, stationary, etc. The data is dead while the object is alive . The role of an object is to make a piece of data alive, to animate it but not to become a piece of data. An object needs some knowledge in order to gain access to that dead piece of data. An object may need a database unique key, an HTTP address, a file name, or a memory address in order to find the data and animate it. But an object should never think of itself as data. What Is the Practical Solution? Unfortunately, we don't have such a memory-representing class in Java, Ruby, JavaScript, Python, PHP, and many other high-level languages. It looks like language designers didn't get the idea of alive objects vs. dead data, which is sad. We're forced to mix data with object states using the same language constructs: object variables and properties. Maybe someday we'll have that Memory class in Java and other languages, but until then, we have a few options. Use C++ . In C++ and similar low-level languages, it is possible to access memory directly and deal with in-memory data the same way we deal with in-file or in-HTTP data. In C++, we can create that Memory class and use it exactly the way we explained above. Use Arrays . In Java, an array is a data structure with a unique property — it can be modified while being declared as final . You can use an array of bytes as a mutable data structure inside an immutable object. It's a surrogate solution that conceptually resembles the Memory class but is much more primitive. Avoid In-Memory Data . Try to avoid in-memory data as much as possible. In some domains, it is easy to do; for example, in web apps, file processing, I/O adapters, etc. However, in other domains, it is much easier said than done. For example, in games, data manipulation algorithms, and GUI, most of the objects animate in-memory data mostly because memory is the only resource they have. In that case, without the Memory class, you end up with mutable objects :( There is no workaround. To summarize, don't forget that an object is an animator of data. It is using its encapsulated knowledge in order to reach the data. No matter where the data is stored — in a file, in HTTP, or in memory — it is conceptually very different from an object state, even though they may look very similar. A good object is an immutable animator of mutable data. Even though it is immutable and data is mutable, it is alive and data is dead in the scope of the object's living environment. "},{"title":"How Much Your Objects Encapsulate?","url":"/2014/12/15/how-much-your-objects-encapsulate.html","tags":["oop"],"date":"2014-12-15 00:00:00 +0000","categories":[],"body":"Which line do you like more, the first or the second: new HTTP ( \"http://www.google.com\" ). read (); new HTTP (). read ( \"http://www.google.com\" ); What is the difference? The first class HTTP encapsulates a URL, while the second one expects it as an argument of method read() . Technically, both objects do exactly the same thing: they read the content of the Google home page. Which one is the right design? Usually I hate to say this, but in this case I have to — it depends. The Truman Show (1998) by Peter Weir As we discussed before , a good object is a representative of a real-life entity. Such an entity exists outside of the object's living environment. The object knows how to access it and how to communicate with it. What is that real-life entity in the example above? Each class gives its own answer. And the answer is given by the list of arguments its constructors accept. The first class accepts a single URL as an argument of its constructor. This tells us that the object of this class, after being constructed, will represent a web page. The second class accepts no arguments, which tells us that the object of it will represent ... the Universe. I think this principle is applicable to all classes in object-oriented programming — in order to understand what real-life entity an object represents, look at its constructor. All arguments passed into the constructor and encapsulated by the object identify a real-life entity accessed and managed by the object. Of course, I'm talking about good objects , which are immutable and don't have setters and getters . Pay attention that I'm talking about arguments encapsulated by the object. The following class doesn't represent the Universe, even though it does have a no-arguments constructor: class Time { private final long msec ; public Time () { this ( System . currentTimeMillis ()); } public Time ( long time ) { this . msec = time ; } } This class has two constructors. One of them is the main one, and one is supplementary. We're interested in the main one, which implements the encapsulation of arguments. Now, the question is which is better: to represent a web page or the Universe? It depends, but I think that in general, the smaller the real-life entity we represent, the more solid and cohesive design we give to the object. On the other hand, sometimes we have to have an object that represents the Universe. For example, we may have this: class HTTP { public String read ( String url ) { // read via HTTP and return } public boolean online () { // check whether we're online } } This is not an elegant design, but it demonstrates when it may be necessary to represent the entire Universe. An object of this HTTP class can read any web page from the entire web (it is almost as big as the Universe, isn't it?), and it can check whether the entire web is accessible by it. Obviously, in this case, we don't need it to encapsulate anything. I believe that objects representing the Universe are not good objects, mostly because there is only one Universe; why do we need many representatives of it? :) "},{"title":"You Do Need Independent Technical Reviews!","url":"/2014/12/18/independent-technical-reviews.html","tags":["mgmt"],"date":"2014-12-18 00:00:00 +0000","categories":["jcg"],"body":"Do you have a team of brilliant and enthusiastic programmers? Of course! You've carefully chosen them from a hundred candidates! Are they passionate about the product? Absolutely! They use cutting-edge technologies, never sleep, and hardly eat or drink anything except coffee! Do they believe in your business success? No doubts about it; they live and breathe all those features, releases, continuous delivery, user experience, etc. Are you sure they are developing the product correctly? Well, yes, you're pretty sure; why wouldn't they? ... Does this sound familiar? I can't count how many times I've heard these stories told by startup founders. Most of them are in love with their teams ... until that day when it's time to hire a new one. There could be many possible reasons for such a fiasco, but one of them is a lack of regular, systematic, and independent technical reviews . Nothing demotivates a development team more than a lack of attention to their deliverables. On the other hand, a regular reconciliation of their results and your quality expectations is one of the key factors that will guarantee technical success for your startup. Below I summarize my experience with organizing such technical reviews. Arizona Dream (1992) by Emir Kusturica An independent review is when you ask someone outside of your team to look at your source code and other technical resources and give you an objective opinion about them. Every modern software team should also use internal code reviews, which is is something else entirely. An internal review occurs when one programmer shows his code to other peers on the team and asks their opinion. This usually happens as a daily activity and has nothing to do with independent reviews. An independent review is performed by a programmer who knows nothing about your team. He comes on board, checks out the code from your repository, spends a few hours (or days) looking at it and trying to understand what it does. Then, he tells you what is wrong and where. He explains how he would do it better, where he would change it, and what he would do instead. Then, you pay him and he leaves. You may never see him again, but his conclusions and suggestions help you check the reality of your code and evaluate how your team is really doing. We, at teamed.io , do independent reviews with every project of ours, and this is a list of principles we use: Make Independent Reviews Systematic . This is the first and most important rule — organize such technical reviews regularly. Moreover, inform your team about the schedule, and let them be prepared for the reviews. Once a month is a good practice, according to my experience. Depending on your source code size, a full review should take from two to eight hours . Don't spend more than eight hours; there is no point in going too deep into the code during independent reviews. Pay for Bugs Found . We always pay for bugs, not for the time spent finding them. We ask our reviewers to look at the code and report as many bugs as we think we need. For each bug, we pay 15 minutes for their time. In other words, we assume that a good reviewer can find and report approximately four problems in one hour. For example, a reviewer charges $150 per hour. We hire him and ask him to find and report the 20 most criticial issues he can discover. Our estimate is that he should spend five hours on this work. Thus, he will get $750 when we have 20 bugs in our tracking system reported by him. If he finds fewer, he gets proportionally less money. This payment schedule will help you focus your reviewer on the main objective of the review process — finding and reporting issues. There are no other goals. The only thing you're interested in is knowing what the issues with your current technical solution are. That's what you're paying for. Hire the Best and Pay Well . My experience tells me that the position of an independent reviewer is a very important one. He is not just a programmer but more of an architect who is capable of looking at the solution from a very high level of abstraction, while at the same time paying a lot of attention to details; he should be very good at designing similar systems; he should know how to report a bug correctly and with enough detail; he should understand your business domain; etc. Besides all that, he should be well motivated to help you. You're not hiring him for full-time work but rather just for a few-hour gig. My advice is to try to get the best guys , and pay them as much as they ask, usually over $100 per hour. Don't negotiate, just pay. It's just a few hundred dollars for you, but the effect of their contribution will be huge. Ask For and Expect Criticism . It is a very common mistake to ask a reviewer, \"Do you like our code?\" Don't expect him to tell you how great your code is. This is not what you're paying him for! You already have a full team of programmers for cheering you up; they can tell you a lot about the code they are creating and how awesome it is. You don't want to hear this again from the reviewer. Instead, you want to know what is wrong and needs to be fixed. So your questions should sound like, \"What problems do you think we should fix first?\" Some reviewers will try to please you with positive comments, but ignore that flattery and bring them back to the main goal — bugs. The payment schedule explained above should help. Regularly Change Reviewers . Try not to use the same reviewer more than once on the same project (I mean the same code base). I believe the reason here is obvious, but let me re-iterate: You don't need your reviewer to be nice to you and tell you how great your code is. You want him to be objective and focused on problems, not on bright sides. If you hire the same person again and again, psychologically you make him engaged to the source code. He's seen it once; now he has to see it again. He already told you about some problem, and now he has to repeat it again. He won't feel comfortable doing it. Instead, he will start feeling like a member of the team and will feel responsible for the source code and its mistakes. He, as any other team member, will start hiding issues instead of revealing them. Thus, for every independent technical review, get a new person. Be Polite and Honest With Your Team . Independent reviews can be rather offensive to your programmers. They may think that you don't trust them. They may feel that you don't respect them as technical specialists. They may even decide that you're getting ready to fire them all and are currently looking for new people. This is a very possible and very destructive side effect of an independent review. How do you avoid it? I can't give you universal advice, but the best suggestion I can give is this: be honest with them. Tell them that the quality of the product is critical for you and your business. Explain to them that the business is paying them for their work and that in order to keep paychecks coming, you have to stress quality control — regularly, objectively, independently, and honestly. In the end, if you manage to organize reviews as this article explains, the team will be very thankful to you. They will gain a lot of new ideas and thoughts from every review and will ask you to repeat them regularly. Review From Day One . Don't wait until the end of the project! I've seen this mistake many times. Very often startup founders think that until the product is done and ready for the market, they shouldn't distract their team. They think they should let the team work toward project milestones and take care of quality later, \"when we have a million visitors per day\". This day will never come if you let your team run without control! Start conducting independent reviews from the moment your Git repository has its first file. Until the repository is big enough, you may only spend $300 once a month to receive an objective, independent opinion about its quality. Will this ruin your budget? Prohibit Discussions, and Ask for Formal Reporting . Don't let your reviewers talk to the team. If you do, the entire idea of a review being independent falls apart. If a reviewer is able to ask informal questions and discuss your system design with your programmers, their answers will satisfy him, and he will move on. But you, the owner of the business, will stay exactly where you were before the review. The point of the review is not to make the reviewer happy. It is exactly the opposite. You want to make him confused! If he is confused, your design is wrong and he feels the need to report a bug. The source code should speak for itself, and it should be easy enough for a stranger (the reviewer) to understand how it works. If this is not the case, there is something wrong that should be fixed. Treat Any Question as a Bug . Don't expect a review to produce any bugs in functionality, like \"I click this button and the system crashes\". This will happen rarely, if ever. Your team is very good at discovering these issues and fixing them. Independent reviews are not about that kind of bugs. The main goal of an independent review is to discover bugs in the architecture and design. Your product may work, but its architecture may have serious design flaws that won't allow you, for example, to handle exponential growth in web traffic. An independent reviewer will help you find those flaws and address them sooner than later. In order to get bugs of that kind from the reviewer, you should encourage him to report anything he doesn't like — unmotivated use of a technology, lack of documentation, unclear purpose of a file, absence of a unit test, etc. Remember, the reviewer is not a member of your team and has his own ideas about the technologies you're using and software development in general. You're interested in matching his vision with your team's. Then, you're interested in fixing all critical mismatches. Review Everything, Not Just Source Code . Let your reviewer look at all technical resources you have, not just source code files ( .java , .rb , .php , etc.) Give him access to the database schema, continuous integration panel, build environment, issue tracking system, plans and schedules, work agendas, uptime reports, deployment pipeline, production logs, customer bug reports, statistics, etc. Everything that could help him understand how your system works, and more importantly, where and how it breaks, is very useful. Don't limit the reviewer to the source code only — this is simply not enough! Let him see the big picture, and you will get a much more detailed and professional report. Track How Inconsistencies Are Resolved . Once you get a report from the reviewer, make sure that the most important issues immediately get into your team's backlog. Then, make sure they are addressed and closed. That doesn't mean you should fix them all and listen to everything said by the reviewer. Definitely not! Your architect runs the show, not the reviewer. Your architect should decide what is right and what is wrong in the technical implementation of the product. But it's important to make him resolve all concerns raised by the reviewer. Very often you will get answers like these from him: \"We don't care about it now\", \"we won't fix it until the next release\", or \"he is wrong; we're doing it better\". These answers are perfectly valid, but they have to be given (reviewers are people and they also make mistakes). The answers will help you, the founder, understand what your team is doing and how well they understand their business. If you can offer more suggestions, based on your experience, please post them below in the comments, and I'll add them to the list. I'm still thinking that I may have forgotten something important :) "},{"title":"Immutable Objects Are Not Dumb","url":"/2014/12/22/immutable-objects-not-dumb.html","tags":["oop"],"date":"2014-12-22 00:00:00 +0000","categories":[],"body":"After a few recent posts about immutability, including \"Objects Should Be Immutable\" and \"How an Immutable Object Can Have State and Behavior?\" , I was surprised by the number of comments saying that I badly misunderstood the idea. Most of those comments stated that an immutable object must always behave the same way — that is what immutability is about. What kind of immutability is it, if a method returns different results each time we call it? This is not how well-known immutable classes behave. Take, for example, String , BigInteger , Locale , URI , URL , Inet4Address , UUID , or wrapper classes for primitives, like Double and Integer . Other comments argued against the very definition of an immutable object as a representative of a mutable real-world entity. How could an immutable object represent a mutable entity? Huh? I'm very surprised. This post is going to clarify the definition of an immutable object. First, here is a quick answer. How can an immutable object represent a mutable entity? Look at an immutable class, File , and its methods, for example length() and delete() . The class is immutable, according to Oracle documentation, and its methods may return different values each time we call them. An object of class File , being perfectly immutable, represents a mutable real-world entity, a file on disk. The Usual Suspects (1995) by Bryan Singer In this post , I said that \"an object is immutable if its state can't be modified after it is created.\" This definition is not mine; it's taken from Java Concurrency in Practice by Goetz et al. , Section 3.4 (by the way, I highly recommend you read it). Now look at this class (I'm using jcabi-http to read and write over HTTP): @Immutable class Page { private final URI uri ; Page ( URI addr ) { this . uri = addr ; } public String load () { return new JdkRequest ( this . uri ) . fetch (). body (); } public void save ( String content ) { return new JdkRequest ( this . uri ) . method ( \"PUT\" ) . body (). set ( content ). back () . fetch (); } } What is the \"state\" in this class? That's right, this.uri is the state. It uniquely identifies every object of this class, and it is not modifiable. Thus, the class makes only immutable objects. And each object represents a mutable entity of the real world, a web page with a URI. There is no contradiction in this situation. The class is perfectly immutable, while the web page it represents is mutable. Why do most programmers I have talked to believe that if an underlying entity is mutable, an object is mutable too? I think the answer is simple — they think that objects are data structures with methods. That's why, from this point of view, an immutable object is a data structure that never changes. This is where the fallacy is coming from — an object is not a data structure . It is a living organism representing a real-world entity inside the object's living environment (a computer program). It does encapsulate some data, which helps to locate the entity in the real world. The encapsulated data is the coordinates of the entity being represented. In the case of String or URL , the coordinates are the same as the entity itself, but this is just an isolated incident, not a generic rule. An immutable object is not a data structure that doesn't change, even though String , BigInteger , and URL look like one. An object is immutable if and only if it doesn't change the coordinates of the real-world entity it represents. In the Page class above, this means that an object of the class, once instantiated, will never change this.uri . It will always point to the same web page, no matter what. And the object doesn't guarantee anything about the behavior of that web page. The page is a dynamic creature of a real world, living its own life. Our object can't promise anything about the page. The only thing it promises is that it will always stay loyal to that page — it will never forget or change its coordinates. Conceptually speaking, immutability means loyalty, that's all. "},{"title":"How to Be Honest and Keep a Customer","url":"/2015/01/05/how-to-be-honest-and-keep-customer.html","tags":["mgmt"],"date":"2015-01-05 00:00:00 +0000","categories":["jcg"],"body":"Most of our clients are rather surprised when we explain to them that they will have full access to the source code from the first day of the project. We let them see everything that is happening in the project, including the Git repository, bug reports, discussions between programmers, continuous integration fails, etc. They often tell me that other software development outsourcing teams keep this information in-house and deliver only final releases, rarely together with the source code. I understand why other developers are trying to hide as much as possible. Giving a project sponsor full access to the development environment is not easy at all. Here is a summary of problems we've been having and our solutions. I hope they help you honestly show your clients all project internals and still keep them on board. 99 francs (2007) by Jan Kounen He Is Breaking Our Process This is the most popular problem we face with our new clients. Once they gain access to the development environment, they try to give instructions directly to programmers, walking around our existing process . \"I'm paying these guys; why can't I tell them what to do?\" is a very typical mindset. Instead of submitting requests through our standard change management mechanism, such a client goes directly to one of the programmers and tells him what should be fixed, how, and when. It's micro-management in its worst form. We see it very often. What do we do? First, we try to understand why it's happening. The simplest answer is that the client is a moron. Sometimes this is exactly the case, but it's a rare one. Much more often, our clients are not that bad. What is it, then? Why can't they follow the process and abide by the rules? There are a few possible reasons. Maybe the rules are not explained well . This is the most popular root cause — the rules of work are not clear enough for the client. He just doesn't know what he is supposed to do in order to submit a request and get it implemented. To prevent this, we try to educate our clients at the beginning of a new project. We even write guidance manuals for clients. Most of them are happy to read them and learn the way we work, because they understand that this is the best way to achieve success while working with us. Maybe our management is chaotic , and the client is trying to \"organize\" us by giving explicit instructions regarding the most important tasks. We've seen it before, and we are always trying to learn from this. As soon as we see that the client is trying to micro-manage us, we ask ourselves: \"Is our process transparent enough? Do we give enough information to the client about milestones, risks, plans, costs, etc.?\" In most cases, it's our own fault, and we're trying to learn and improve. If so, it's important to react fast, before the client becomes too agressive in his orders and instructions. It will be very difficult to escort him back to the normal process once he gets \"micro-management\" in his blood. Maybe the client is not busy enough and has a lot of free time , which he is happy to spend by giving orders and distracting your team. I've seen this many times. A solution? Keep him busy. Turn him into a member of the team and assign him some tasks related to documentation and research. In my experience, most clients would be happy to do this work and help the project. He Is Asking Too Much A technically-savvy client can turn the life of an architect into a nightmare by constantly asking him to explain every single technical decision made, from \"Why PostgreSQL instead of MySQL?\" to \"Why doesn't this method throw a checked exception?\" Constantly answering such questions can turn a project into a school of programming. Even though he is paying for our time, that doesn't mean we should teach him how to develop software, right? On the other hand, he is interested in knowing how his software is developed and how it works. It's a fair request, isn't it? I believe there is a win-win solution to this problem. Here is how we manage it. First of all, we make all his requests formal. We ask a client to create a new ticket for each request, properly explaining what is not clear and how much detail is expected in the explanation. Second, we look at such requests positively — they are good indicators of certain inconsistencies in the software. If it's not clear for the client why PostgreSQL is used and not MySQL, it's a fault of our architect . He didn't document his decision and didn't explain how it was made, what other options were considered, what selection criteria were applied, etc. Thus, a request from a client is a bug we get for free. So, we look at it positively. Finally, we charge our clients for the answers given. Every question, submitted as a ticket, goes through the full flow and gets billed just as any other ticket. This approach prevents the client from asking for too much. He realizes that we're ready to explain anything he wants, but he will pay for it. He Is Telling Too Much This problem is even bigger than the previous one. Some clients believe they are savvy enough to argue with our architect and our programmers about how the software should be developed. They don't just ask why PostgreSQL is used, they tell us that we should use MySQL, because \"I know that it's a great database; my friend is using it, and his business is growing!\" Sometimes it gets even worse, when suggestions are directed at every class or even a method, like \"You should use a Singleton pattern here!\" Our first choice is to agree and do what he wants. But it's a road to nowhere. Once you do it, your project is ruined, and you should start thinking about a divorce with this client. Your entire team will quickly turn into a group of coding monkeys, micro-managed by someone with some cash. It's a very wrong direction; don't even think about going there. The second choice is to tell the client to mind his own business and let us do ours. He hired us because we're professional enough to develop the software according to his requirements. If he questions our capabilities, he is free to change the contractor. But until then, he has to trust our decisions. Will this work? I doubt it. It's the same as giving him the finger. He will get offended, and you won't get anything. The solution here is to turn the client's demands into project requirements. Most of them will be lost in the process, because they won't be sane enough to form a good requirement. Others will be documented, estimated, and crossed-out by the client himself, becuase he will realize they are pointless or too expensive. Only a few of them will survive, since they will be reasonable enough. And they will help the project. So it is also a win-win solution. For example, he says that \"you should use MySQL because it's great\". You tell him that the project requirements document doesn't limit you to choose whichever database you like. Should it? He says yes, of course! OK, let's try to document such a requirement. How will it sound? How about, \"We should only use great databases?\" Sound correct? If so, then PostgreSQL satisfies this requirement. Problem solved; let us continue to do our work. He will have a hard time figuring out how to write a requirement in a way that disallows PostgreSQL but allows MySQL. It is simply not possible in most cases. Sometimes, though, it will make sense; for example, \"We should use a database server that understands our legacy data in MySQL format\". This is a perfectly sane requirement, and the only way to satisfy it is to use MySQL. Thus, my recommendation is to never take a client's demands directly to execution, but rather use them first to amend the requirements documentation. Even if you don't have such documentation, create a simple one-page document. Agree with the client that you work against this document, and when anyone wants to change something, you first have to amend the document and then have your team ensure the software satisfies it. This kind of discipline will be accepted by any client and will protect you against sudden and distracting corrections. He Is Questioning Our Skills When source code is open to the client, and he is technically capable of reading it, it is very possible that one day he will tell us that our code is crap and we have to learn how to program better. It has not happened in our projects for many years, but it has happened before, when we weren't using static analysis as a mandatory step in our continuous integration pipeline. Another funny possibility is when the client shows the source code to a \"friend\", and he gives a \"professional\" opinion, which sounds like, \"They don't know what they are doing.\" Once such an opinion hits your client's ears, the project is at a significant risk of closure. It'll be very difficult, almost impossible, to convince the client not to listen to the \"friend\" and continue to work with you. That's why most outsourcers prefer to keep their sources private until the very end of the project, when the final invoice is paid. I think that an accidental appearance of a \"friend\" with a negative opinion is un-preventable. If it happens, it happens. You can't avoid it. On the other hand, if you think your code is perfect and your team has only talented programmers writing beautiful software, this is not going to protect you either. An opinion coming from a \"friend\" won't be objective; it will just be very personal, and that's why it's very credible. He is a friend of a client, and he doesn't send him bills every week. Why would he lie? Of course, he is speaking from the heart! (I'm being sarcastic.) So, no matter how beautiful your architecture and your source code is, the \"friend\" will always be right. In my opinion, the only way to prevent such a situation or minimize its consequences is to organize regular and systematic independent technical reviews . They will give confindence to the client that the team is not lying to him about the quality of the product and key technical decisions made internally. To conclude, I strongly believe it is important to be honest and open with each client, no matter how difficult it is. Try to learn from every conflict with each client, and improve your management process and your principles of work. Hiding source code is not professional and makes you look bad in the eyes of your clients and the entire industry. "},{"title":"Daily Stand-Up Meetings Are a Good Tool for a Bad Manager","url":"/2015/01/08/morning-standup-meetings.html","tags":["mgmt"],"date":"2015-01-08 00:00:00 +0000","categories":[],"body":"A stand-up meeting (or simply \"stand-up\") is \"a daily team-meeting held to provide a status update to the team members\", according to Wikipedia . In the next few paragraphs, I attempt to explain why these meetings, despite being so popular in software development teams, are pure evil and should never be used by good managers. I'm not saying they can be done right or wrong; there are plenty of articles about that. I'm not trying to give advice about how to do them properly so they work, either. I'm saying that a good manager should never have daily stand-ups. Because they not only \"don't work\" but also do very bad, sometimes catastrophic, things to your management process, whether it's agile or not. On the other hand, a bad manager will always use daily stand-ups as his or her key management instrument. Cool Hand Luke (1967) by Stuart Rosenberg To explain what I mean, let's look at management from a few different angles and compare how good and bad managers would organize their work. Information A Bad Manager Asks How Things Are Going . Strolling around the office asking how things are going is a great habit of a terrible manager. He doesn't know what his team is doing because he is not smart enough to organize the process and information flow correctly. However, he needs to know what's going on because his boss is also asking him from time to time. So the only way to collect the required information is to ask the team, \"What are you working on right now?\" Morning stand-up is a perfect place to ask this annoying question officially without being marked as a manager who doesn't know what he is doing. A Good Manager Is Being Told When Necessary . Managing a project involves management of communications. When information flows are organized correctly, every team member knows when and how he or she has to report to the manager. When something goes wrong, everybody knows how such a situation has to be reported: immediately and directly. When a backlog task is completed, everybody understands how to inform a project manager if he needs this information. A perfect project manager never asks his people. Instead, they tell him when necessary. And when someone does stop to tell him something, a good project manager fixes such a broken communication channel. But he never uses daily meetings to collect information. As a good manager, inform your team what your goals are and what's important to you as a project manager (or Scrum master). They should know what's important for you to know about their progress, risks, impediments, and failures. They should understand what trouble you will get into if they let you down. It is your job, as a good manager, to inform them about the most important issues the project and the team are working through. It's their job, as a good team, to inform you immediately when they have some important information. This is what perfect management is about. If you manage to organize teamwork like that, you won't need to wait until the next morning to ask your developers what they were doing yesterday and what problems they experienced. You would have seen this information earlier, exactly when you needed it. You would stay informed about your project affairs even outside of the office. Actually, you would not need an office at all, but that's a subject for another discussion :) Someone may say that daily stand-ups are a perfect place and time to exchange information among programmers, not just to inform the Scrum master and get his feedback. Again, we have the same argument here — why can't they exchange information when it's required, during the day? Why do we need to put 10 people together every morning to discuss something that concerns only five of them? I can answer. Bad managers, who don't know how else to organize the exchange of information between team members, use morning stand-ups as a replacement for a correct communication model. These morning meetings give the impression that the manager is working hard and well deserves his overblown salary. To the contrary, a good manager would never have any regular status update meetings, becuase he knows how to use effective communication instruments, like issue tracking tools, emails, code reviews, decision-making meetings, pair programming, etc. Responsibility A Bad Manager Micro-Manages . This guy knows very little about project management, and that's why he feels very insecure. He is afraid of losing control of the team; he doesn't trust his own people; and he always feels under-informed and shakes when his own boss asks him, \"What's going on?\" Because of all this, he uses his people as anti-depressant pills — when they are doing what he says, he feels more secure and stable. A daily stand-up meeting is a great place where he can ask each of us what we're doing and then tell us what we should do instead. This manager forces us to disclose our personal goals and plans in order to correct them when he feels necessary. How many times have you heard something like this: \" I'm planning to test X. ... No, next week; today you work with Y \" This is micro-management. Daily stand-ups are the perfect tool for a micro-manager. A Good Manager Delegates Responsibility . Ideal management involves four steps: 1) Breaking a complex task into smaller sub-tasks; 2) Delegating them to subordinates; 3) Declaring awards, penalties, and rules; and 4) Making sure that awards are generous, penalties are inevitable, and rules are strictly followed. A perfect manager never tells his people what to do every day and how to organize their work time. He trusts and controls. He never humiliates his people by telling them how to do their work. A great manager would say: \" You're planning to test X today? It's your decision, and I fully respect it. Just remember that if Y isn't ready by the end of the week, you lose the project, as we agreed. \" Why would such a manager need daily stand-ups? Why would he need to ask his people what they are doing? He is not meddling in their plans. Instead, he trusts them and controls their results only. Let me reiterate: I strongly believe that responsibility must be delegated, and this delegation consists of three components: awards, penalties, and rules . In a modern Western culture, it may be rather difficult to define them — we have long-term contracts and monthly salaries. But a good manager has to find a way. Each task has to be delegated and isolated. This means that the programmer working on the task has to be personally responsible for its success or failure. And he or she has to know the consequences. A good manager understands that any team member inevitably tries to avoid personal responsibility. Everybody is trying to put a responsibility monkey back on the shoulders of the manager. It is natural and inevitable. And daily stand-up meetings only help everybody do this trick. When you ask me in the morning how things are going, I'll say that there are some problems and I'm not sure that I will be able to finish the task by the end of the week. That's it! I'm not responsible for the task anymore. It's not my fault if I fail. I told you that I may fail, remember? From now, the responsibility is yours. A good manager knows about this trick and prevents it by explicitly defining awards, penalties, and rules. When I tell you that I may fail, you remind me that I'm going to lose my awards and will get penalties instead: - I'm not sure I can meet the deadline ... - Sorry to hear that you're going to lose your $200 weekend bonus because of that :( Have you seen many project managers or Scrum masters saying such a thing? Not so many, I believe. Yes, a good manager is a rare creature. But only a good manager is capable of defining awards, penalties, and rules so explicitly and strictly. When this triangle is defined, nobody needs status update meetings every morning. Everything is clear as it is. We all know our goals and our objectives. We know what will happen if we fail, and we also understand how much we're going to get if we succeed. We don't need a manager to remind us about that every morning. And we don't need a manager to check our progress. He already gave us a very clear definition of our objectives. Why would we talk about them again every morning? A bad manager isn't capable of defining objectives; that's why he wants to micro-manage us every morning. Actually, a bad manager is doing it during the day too. He is afraid that without well-known goals and rules, the team will do something wrong or won't do anything at all. That's why he has \"to keep his hand on the pulse\". In reality, he keeps his hand on the neck of the team. Motivation A Bad Manager De-Motivates by Public Embarassment . He doesn't know how to organize a proper motivational system within the team; that's why he relies on a natural fear of public embarassment. It's only logical that no one would feel comfortable saying, \" I forgot it \" in front of everybody. So the daily stand-up meeting is where he puts everybody in a line and asks, \" What did you do yesterday? \" This fearful moment is a great motivator for the team, isn't it? I don't think so. A Good Manager Motivates by Objectives . Ideal management defines objectives and lets people achieve them using their skills, resources, knowledge, and passion. A properly defined objective always has three components: awards, penalties, and rules. A great manager knows how to translate corporate objectives into personal ones: \" If we deliver this feature before the weekend, the company will generate extra profit. You, Sally, will personally get $500. If you fail, you will be moved to another, less interesting project. \" This is a perfectly defined objective. Do we need to ask Sally every morning, in front of everybody, if she forgot to implement the feature? If she is working hard? Will this questioning help her? Absolutely not! She already knows what she is working for, and she is motivated enough. When she finishes on time, organize a meeting and give her a $500 check in front of everybody. This is what a good manager uses meetings for. There's more to this, too, as daily status updates in front of everybody motivate the best team players to backslide and become the same as the worst ones. Well, this is mostly because they don't want to offend anyone by their super performance. It is in our nature to try to look similar to everybody else while being in a group. When everybody reports, \" I still have nothing to show \", it would be strange to expect a good programmer to say, \" I finished all my tasks and want to get more \". Well, this may happen once, but after a few times, this A player will either stop working hard or will change the team. He will see that his performance is standing out and that this can't be appreciated by the group, no matter what the manager says. A good manager understands that each programmer has his or her own speed, quality, and salary. A good manager gives different tasks to different people and expects different results from them. Obviously, lining everybody up in the morning and expecting similar reports from them is a huge mistake. The mistake will have a catastrophic effect on A players, who are interested in achieving super results and expect to be super-appreciated and compensated. A bad manager can't manage different people differently, just because he doesn't know how. That's why he needs daily stand-ups, where everybody reports almost the same, and it's easy to compare their results to each other. Also, it's easier to blame or to cheer up those who don't report similar to others. In other words, a bad manager uses daily stand-ups as an instrument of equality, which in this case only ruins the entire team's motivation. Daily stand-ups, as well as any status update meetings, are a great instrument to hide and protect a lazy and stupid manager. To hide his inability to manage people. To hide his lack of competence. To hide his fear of problems, challenges, and risks. If you're a good manager, don't embarrass yourself with daily stand-ups. "},{"title":"Continuous Integration on Windows, with Appveyor and Maven","url":"/2015/01/10/windows-appveyor-maven.html","tags":["devops"],"date":"2015-01-10 00:00:00 +0000","categories":[],"body":" The purpose of Continuous Integration is to tell us, the developers, when the product we're working on is not \"packagable\" any more. The sooner we get the signal, the better. Why? Because the damage will be younger if we find it sooner. The younger the damage, the easier it is to fix. There are many modern and high-quality hosted continuous integration services , but only one of them (to my knowledge) supports Windows as a build platform — appveyor.com . My experience tells me that it's a good practice to continuously integrate on different platforms at the same time, especially when developing an open source library. That's why, in teamed.io we're using AppVeyor in combination with Travis . This is how I managed to configure AppVeyor to build my Java Maven projects (this is appveyor.yml configuration file you're supposed to place in the root directory of your Github repository): version : '{build}' os : Windows Server 2012 install : - ps : | Add-Type -AssemblyName System.IO.Compression.FileSystem if (!(Test-Path -Path \"C:\\maven\" )) { (new-object System.Net.WebClient).DownloadFile( 'http://www.us.apache.org/dist/maven/maven-3/3.2.5/binaries/apache-maven-3.2.5-bin.zip', 'C:\\maven-bin.zip' ) [System.IO.Compression.ZipFile]::ExtractToDirectory(\"C:\\maven-bin.zip\", \"C:\\maven\") } - cmd : SET PATH=C:\\maven\\apache-maven-3.2.5\\bin;%JAVA_HOME%\\bin;%PATH - cmd : SET MAVEN_OPTS=-XX:MaxPermSize=2g -Xmx4g - cmd : SET JAVA_OPTS=-XX:MaxPermSize=2g -Xmx4g build_script : - mvn clean package --batch-mode -DskipTest test_script : - mvn clean install --batch-mode cache : - C:\\maven\\ - C:\\Users\\appveyor\\.m2 It was not that easy at all, so I decided to share. You can see how this configuration works in these projects: jcabi-aspects , jcabi-email , jcabi-dynamo , and rultor . "},{"title":"A Compound Name Is a Code Smell","url":"/2015/01/12/compound-name-is-code-smell.html","tags":["oop"],"date":"2015-01-12 00:00:00 +0000","categories":[],"body":"Do you name variables like textLength , table_name , or current-user-email ? All three are compound names that consist of more than one word. Even though they look more descriptive than name , length , or email , I would strongly recommend avoiding them. I believe a variable name that is more complex than a noun is a code smell. Why? Because we usually give a variable a compound name when its scope is so big and complex that a simple noun would sound ambiguous. And a big, complex scope is an obvious code smell. The Meaning of Life (1983) by Terry Jones and Terry Gilliam The scope of a variable is the place where it is visible, like a method, for example. Look at this Ruby class: class CSV def initialize ( csvFileName ) @fileName = csvFileName end def readRecords () File . readLines ( @fileName ) . map | csvLine | csvLine . split ( ',' ) end end end The visible scope of variable csvFileName is method initialize() , which is a constructor of the class CSV . Why does it need a compound name that consists of three words? Isn't it already clear that a single-argument constructor of class CSV expects the name of a file with comma-separated values? I would rename it to file . Next, the scope of @fileName is the entire CSV class. Renaming a single variable in the class to just @file won't introduce any confusion. It's still clear what file we're dealing with. The same situation exists with the csvLine variable. It is clear that we're dealing with CSV lines here. The csv prefix is just a redundancy. Here is how I would refactor the class: class CSV def initialize ( file ) @file = file end def records () File . readLines ( @file ) . map | line | line . split ( ',' ) end end end Now it looks clear and concise. If you can't perform such a refactoring, it means your scope is too big and/or too complex. An ideal method should deal with up to five variables, and an ideal class should encapsulate up to five properties. If we have five variables, can't we find five nouns to name them? Adam and Eve didn't have second names. They were unique in Eden, as were many other characters in the Old Testament. Second and middle names were invented later in order to resolve ambiguity. To keep your methods and classes clean and solid, and to prevent ambiguity, try to give your variables and methods unique single-word names, just like Adam and Eve were named by you know who :) "}]}