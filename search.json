{"entries":[{"title":"Yegor Bugayenko About Programming","url":"/index.html","date":null,"categories":[],"body":" page of "},{"title":"Unsubscribe","url":"/unsubscribe.html","date":null,"categories":[],"body":"I'm sorry to see you leaving :( Your email Unsubscribe You can always subscribe again. "},{"title":"First Post","url":"/2014/04/06/introduction.html","date":"2014-04-06 00:00:00 +0000","categories":["2014","apr"],"body":"This is the first post on my new blog. Therefore, it's not about anything in particular - just an introduction and my way of saying hello. This blog will be primarily about software development ideas. As my About Me page says, I'm passionate about software quality, and will write solely about my ideas and views on it. Anyway, welcome to my new blog. Together, let's see how this works out! :) ps. BTW, I purchased the Cambria font just for this new blog. It cost, €98. Nevertheless, I think its a good investment for this new venture. "},{"title":"Movies for Thanasis","url":"/2014/04/06/movies-for-thanasis.html","date":"2014-04-06 00:00:00 +0000","categories":["2014","apr"],"body":"Sometime ago, I recommended a list of movies to a friend of mine after he told me was losing all interest in \"Hollywood.\" Level C titles are supposed to be impossible to understand unless you've seen (and understood) their prequels -- listed in sections A and B. So, start browsing the lists in sections A and post your comments if you have any. :) Level A True Romance (1993) Pulp Fiction (1994) Kill Bill (2003) Doberman (1997) La fille sur le pont (1999) Irreversible (2002) Fear and Loathing in Las Vegas (1998) Perdita Durango (1997) Golden Balls (1993) The Million Dollar Hotel (2000) Y Tu Mamá También (2001) Reservoir Dogs (1992) Trainspotting (1996) Fight Club (1999) Arizona Dream (1992) Black Cat, White Cat (1998) Buffalo '66 (1998) Jamon Jamon (1992) Natural Born Killers (1994) Thursday (1998) Bullet (1996) Level B Delicatessen (1991) Bernie (1996) Hardmen (1996) Revolver (2005) Science of Sleep (2006) Cashback (2006) El Crimen Perfecto (2004) El día de la bestia (1995) La comunidad (2000) The Happiness of the Katakuris (2001) 99 francs (2007) Combien tu m'aimes? (2005) Ying xiong (2002) Brutti, sporchi e cattivi (1976) Level C What Just Happened (2008) Happiness (1998) Before the Devil Knows You Are Dead (2007) No Country for Old Men (2007) A Serious Man (2009) "},{"title":"Phantomjs as an HTML Validator","url":"/2014/04/06/phandom.html","date":"2014-04-06 00:00:00 +0000","categories":["2014","apr"],"body":" I created phandom.org a few months ago, but yesterday finally found the time to make some needed changes to it. So, now is a good time to explain how I'm using Phandom in some of my unit tests. Before I get started, though, I should say a few words about phantomjs , which is a JavaScript interface for WebKit. WebKit, on the other hand, is a web browser without a user interface. WebKit is a C++ library that enables manipulation of HTML content, through DOM calls. For example, this is a simple JavaScript located code in example.js : 1 2 3 4 5 6 7 8 var page = require ( 'webpage' ). create (); page . open ( 'http://google.com' , function () { console . log ( 'loaded!' ); phantom . exit ( 0 ); } ); We run phantomjs from the command line with the following code: $ phantomjs example.js Phantomjs creates a page object (provided by webpage module inside phantomjs), and then asks it to open() a Web page. The object communicates with WebKit and converts this call into DOM instructions. After which, the page loads. The phantomjs engine then terminates on line 6. WebKit renders a web page with all neccessary components such as CSS, JavaScript, ActionScript, etc, just as any standard Web browser would. So far so good, and this is the traditional way of using phantomjs. Now, on to giving you an idea of how Phandom (which stands for \"PhantomJS DOM\") works inside Java unit tests: To test this, let's give phantomjs an HTML page and ask him to render it. When the page is ready, we'll ask phantomjs to show us how this HTML looks in WebKit. If we see the elements we need and desire, — we're good to go. Let's use the following example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import com.rexsl.test.XhtmlMatchers ; import org.hamcrest.MatcherAssert ; import org.phandom.Phandom ; public class DocumentTest { @Test public void rendersValidHtml () { Document doc = new Document (); // This is the method we're testing. It is supposed // to return a valid HTML without broken JavaScript // and with all required HTML elements. String html = doc . html (); MatcherAssert . assertThat ( XhtmlMatchers . xhtml ( new Phandom ( html ). dom ()), XhtmlMatchers . hasXPath ( \"//p[.='Hello, world!']\" ) ); } } When we use the above code, here is what happens. First, we get HTML html as a String from doc object, and then pass it to Phandom as an argument. Then, on line 13, we call the Phandom.dom() method to get an instance of the class org.w3c.dom.Document . If our HTML contains any broken JavaScript code, method dom() produces a runtime exception and the unit test faila. If HTML is clean and WebKit is able to render it without problems, the test passes. I'm using this mechanism in a few different projects,and it works quite well. Therefore, I highly recommend it. Of course, you shouldn't forget that you must have phantomjs installed on your build machine. In order to avoid unit test failures when phantomjs is not available or present, I've created the following supplementary method: public class DocumentTest { @Test public void rendersValidHtml () { Assume . assumeTrue ( Phandom . installed ()); // the rest of the unit test method body... } } Enjoy and feel free to report any bugs or problems you encounter to: Github issues :) "},{"title":"Xembly, an Assembly for XML","url":"/2014/04/09/xembly-intro.html","date":"2014-04-09 00:00:00 +0000","categories":["2014","apr"],"body":" I use XML in almost every one of my projects. And, despite all the fuss about JSON/YAML, I honestly believe that XML is one of the greatest languages ever invented. Also, I believe that the beauty of XML reveals itself when used in combination with related technologies. For example, you can expose your data in XML and render it for the end-user using XSL stylesheet . Another example would be when you validate the same data, before rendering, to ensure that the structure is correct. You can do this with the XSD schema. Alternatively, you can pick specific data elements from the entire document by using XPath queries. Essentially, these three technologies, XSL, XSD schema and XPath, are what makes XML so powerful. However, there can be times when XML falls short. For instance, imagine you have an existing document that needs to be modified just slightly. For example, let's use the following: <accounts> [...] <acc id= '34' > <name> Jeffrey </name> <balance> 305 </balance> </acc> <acc id= '35' > <name> Walter </name> <balance> 50090 </balance> </acc> [...] </accounts> The above code represents a list of accounts. Each account has its own id and several child elements. In our example, we need to find the account belonging to Jeffrey and increase its balance by 500 . How would we do this? Well, there are a few possible solutions: SAX-parse the document, change the balance and save the stream; DOM-parse it, find the element with XPath, change the value and then print it; apply a parametrized XSL stylesheet; apply XQuery small script to make changes All of these methods have their own drawbacks. However, all of them have one particular problem in common — they are very verbose. With each of the above methods, you need at least a page of code to perform this rather simple operation. Furthermore, if the logic of the operation becomes more complex, the amount of needed code grows much faster than you may expect. Simply put, XML lacks a tool for primitive data manipulations within a document. Perhaps, it is this shortcoming that makes XML unpopular with some. Anyway, here is a tool I created a few month ago: Xembly . It is an imperative language with a few simple directives and resembles Assembly in style. Thus, the name - Xembly. With Xembly, there are no loops, conditions or variables - just a sequence of directives with arguments. Let's create a simple example. Say, for instance, we want to add a new account number 36 to our list document. The code would look like: 1 2 3 4 5 6 7 8 XPATH '/ accounts '; ADD ' account '; ATTR ' id ' , ' 36 '; ADD ' name '; SET ' Donny '; UP ; ADD ' balance '; SET ' 3400 '; The above should be intuitively clear, but I'll explain just in case. First, the XPATH directive points us to the element found by the \"/accounts\" XPath query. This will be our root element. We assume here that it exists in the document. Therefore, if it is absent, our Xembly script will fail with a runtime exception. Next, the ADD directive on line 2 creates a new XML element without any children or attributes. Then, the ATTR directive sets an attribute for this element. The code then adds the new child element name and sets its text value to \"Donny\" using the SET directive. Finally, we move our pointer back to account element using UP , add the balance child element and set its value to \"3400\" . Our balance changing task can be expressed in Xembly with the following code: XPATH '/ accounts / account [ name =\" Jeffrey \"]/ balance '; XSET ' . + 500 '; The XSET directive sets the element text value, similar to SET , but calculates it beforehand using the provided XPath expression . + 500 . Xembly performs all manipulations through DOM. Consequently, Xembly can be implemented inside any language that has a built-in DOM implementation. In the meantime, there is only one implementation of Xembly language — in Java. Here is how it works: 1 2 3 4 5 6 7 Iterable < Directive > directives = new Directives () . xpath ( \"/accounts\" ) . add ( \"account\" ) . attr ( \"id\" , \"36\" ) . add ( \"name\" ). set ( \"Donny\" ). up () . add ( \"balance\" ). set ( \"3400\" ); new Xembler ( directives ). apply ( document ); In this snippet, I'm using a supplementary script builder, Directives , which enables generation of directives in a fluent way. Then, I use Xembler class, which is similar to \"assembler\", to apply all specified directives to the document object of class org.w3c.dom.Document . Additionally, Xembly can be used to build XML documents from scratch and as a replacement for traditional DOM building. A quick example: System . out . println ( new Xembler ( new Directives (). add ( \"html\" ) . add ( \"head\" ) . add ( \"title\" ) . set ( \"Hello, world!\" ) ). xml () ); The above snippet produces the following output: <html> <head> <title> Hello, world! </title> </head> </html> For me, this appears to be more simple and compact. As usual, your bug reports and suggestions are always welcomed. Please send to Github issues :) "},{"title":"Fluent Java Http Client","url":"/2014/04/11/jcabi-http-intro.html","date":"2014-04-11 00:00:00 +0000","categories":["2014","apr"],"body":" In the world of Java, there are plenty of HTTP clients from which to choose. Nevertheless, I decided to create a new one because none of the other clients satisfied fully all of my requirements. Maybe, I'm too demanding. Still, this is how my jcabi-http client interacts when you make an HTTP request and expect a successful HTML page in return: 1 2 3 4 5 6 7 8 String html = new JdkRequest ( \"https://www.google.com\" ) . uri (). path ( \"/users\" ). queryParam ( \"id\" , 333 ). back () . method ( Request . GET ) . header ( \"Accept\" , \"text/html\" ) . fetch () . as ( RestResponse . class ) . assertStatus ( HttpURLConnection . HTTP_OK ) . body (); I designed this new client with the following requirements in mind: Simplicity For me, this was the most important requirement. The client must be simple and easy to use. In most cases, I need only to make an HTTP request and parse the JSON response to return a value. For example, this is how I use the new client to return a current EUR rate: 1 2 3 4 5 6 String rate = new JdkRequest ( \"http://www.getexchangerates.com/api/latest.json\" ) . header ( \"Accept\" , \"application/json\" ) . fetch () . as ( JsonResponse . class ) . json (). readArray (). getJsonObject ( 0 ) . getString ( \"EUR\" ); I assume that the above is easy to understand and maintain. Fluent Interface The new client has to be fluent, which means that the entire server interaction fits into one Java statement. Why is this important? I think that fluent interface is the most compact and expressive way to perform multiple imperative calls. To my knowledge, none of the existing libraries enable this type of fluency. Testable and Extendable I'm a big fan of interfaces, mostly because they make your designs both cleaner and highly extendable at the same time. In jcabi-http , there are five interfaces extended by 20 classes. Request is an interface, as well as Response , RequestURI , and RequestBody exposed by it. Use of interfaces makes the library highly extendable. For example, we have JdkRequest and ApacheRequest , which make actual HTTP calls to the server using two completely different technologies: (JDK HttpURLConnection and Apache Http Client, respectively). In the future, it will be possible to introduce new implementations without breaking existing code. Say, for instance, I want to fetch a page and then do something with it. These two calls perform the task differently, but the end results are the same: String uri = \"http://www.google.com\" ; Response page ; page = new JdkRequest ( uri ). fetch (); page = new ApacheRequest ( uri ). fetch (); XML and JSON Out-of-the-Box There are two common standards that I wanted the library to support right out of the box. In most cases, the response retrieved from a server is in either XML or JSON format. It has always been a hassle, and extra work, for me to parse the output to take care of formatting issues. jcabi-http client supports them both out of the box, and it's possible to add more formats in the future as needed. For example, you can fetch XML and retrieve a string value from its element: String name = new JdkRequest ( \"http://my-api.example.com\" ) . header ( \"Accept\" , \"text/xml\" ) . fetch () . as ( XmlResponse . class ) . xml (). xpath ( \"/root/name/text()\" ). get ( 0 ); Basically, the response produced by fetch() is decorated by XmlResponse . This then exposes the xml() method that returns an instance of the XML interface. The same can be done with JSON through the Java JSON API ( JSR-353 ). None of the libraries that I'm aware of or worked with offer this feature. Immutable The last requirement, but certainly not the least important, is that I need all interfaces of the library to be annotated with @Immutable . This is important because I need to be able to encapsulate an instance of Request in other immutable classes. "},{"title":"Object-Oriented DynamoDB API","url":"/2014/04/14/jcabi-dynamo-java-api-of-aws-dynamodb.html","date":"2014-04-14 00:00:00 +0000","categories":["2014","apr"],"body":" I'm a big fan of cloud computing in general and of Amazon Web Services in particular. I honestly believe that in a few years big providers will host all, or almost all, computing and storage resources. When this is the case, we won't have to worry too much anymore about downtime, backups and system administrators. DynamoDB is one of the steps towards this future. This looks cool - jcabi-dynamo - a #Java Object layer atop the #DynamoDB SDK - http://t.co/khRFR2joKX #aws — Jeff Barr (@jeffbarr) September 19, 2013 DynamoDB is a NoSQL database accessible through RESTful JSON API. Its design is relatively simple. There are tables, which basically are collections of data structs, or in AWS terminology, \"items.\" Every item has a mandatory \"hash,\" an optional \"range\" and a number of other optional attributes. For instance, take the example table depts : +------+--------+---------------------------+ | dept | worker | Attributes | +------+--------+---------------------------+ | 205 | Jeff | job=\"manager\", sex=\"male\" | | 205 | Bob | age=43, city=\"Chicago\" | | 398 | Alice | age=27, job=\"architect\" | +------+--------+---------------------------+ For Java, Amazon provides an SDK , which mirrors all RESTful calls to Java methods. The SDK works fine, but is designed in a pure procedural style. Let's say we want to add a new item to the table above. RESTful call putItem looks like (in essence): putItem: tableName: depts item: dept: 435 worker: \"William\" job: \"programmer\" This is what the Amazon server needs to know in order to create a new item in the table. This is how you're supposed to make this call through the AWS Java SDK: 1 2 3 4 5 6 7 8 9 10 11 12 13 PutItemRequest request = new PutItemRequest (); request . setTableName ( \"depts\" ); Map < String , AttributeValue > attributes = new HashMap <>(); attributes . put ( \"dept\" , new AttributeValue ( 435 )); attributes . put ( \"worker\" , new AttributeValue ( \"William\" )); attributes . put ( \"job\" , new AttributeValue ( \" programmer )); request . setItem ( attributes ); AmazonDynamoDB aws = // instantiate it with credentials try { aws . putItem ( request ); } finally { aws . shutdown (); } The above script works fine, but there is one major drawback — it is not object oriented. It is a perfect example of an imperative procedural programming . To allow you to compare, let me show what I've done with jcabi-dynamo . Here is my code, which does exactly the same thing, but in an object-oriented way: 1 2 3 4 5 6 7 8 Region region = // instantiate it with credentials Table table = region . table ( \"depts\" ); Item item = table . put ( new Attributes () . with ( \"dept\" , 435 ) . with ( \"worker\" , \"William\" ) . with ( \"job\" , \"programmer\" ) ); My code is not only shorter, but it also employs encapsulation and separates responsibilities of classes. Table class (actually it is an interface internally implemented by a class) encapsulates information about the table, while Item encapsulates item details. We can pass an item as an argument to another method and all DynamoDB related implementation details will be hidden from it. For example, somewhere later in the code: void sayHello ( Item item ) { System . out . println ( \"Hello, \" + item . get ( \"worker\" )); } In this script, we don't know anything about DynamoDB or how to deal with its RESTful API. We interact solely with an instance of Item class. By the way, all public entities in jcabi-dynamo are Java interfaces. Thanks to that, you can test and mock the library completely. Let's consider a more complex example, which would take a page of code if we were to use a bare AWS SDK. Let's say that we want to remove all workers from our table who work as architects: Region region = // instantiate it with credentials Iterator < Item > workers = region . table ( \"depts\" ). frame () . where ( \"job\" , Condition . equalTo ( \"architect\" )); while ( workers . hasNext ()) { workers . remove (); } jcabi-dynamo has saved a lot of code lines in a few of my projects. You can see it in action at rultor-users . The library ships as a JAR dependency in Maven Central (get its latest versions from Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamo </artifactId> </dependency> "},{"title":"Mocking of HTTP Server in Java","url":"/2014/04/18/jcabi-http-server-mocking.html","date":"2014-04-18 00:00:00 +0000","categories":["2014","apr"],"body":" Recently, I explained a fluent Java HTTP client created (mostly) to make HTTP interactions more object-oriented than with other available clients,including: Apache Client , Jersey Client and plain old HttpURLConnection . This client ships in the jcabi-http Maven artifact. However, the client part is not the only benefit of using jcabi-http . Jcabi also includes a server component that can help you in unit and integration testing of your HTTP clients. Let me show you an example first. In the example, I'm using hamcrest for assertions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 MkContainer container = new MkGrizzlyContainer () . next ( new MkAnswer . Simple ( \"hello, world!\" )) . start (); try { new JdkRequest ( container . home ()) . header ( \"User-agent\" , \"Myself\" ) . fetch () . assertBody ( Matchers . containsString ( \"hello\" )); } finally { container . stop (); } MkQuery query = container . take (); MatcherAssert . assertThat ( query . headers (). get ( \"User-agent\" ), Matchers . hasItem ( \"Myself\" ) ); Now, let's discover what happens here. In the first few lines, I create an instance of MkContainer , which literally has four methods: next(MkAnswer) , start() , stop() , and home() . It works as an HTTP server with a \"first-in-first-out\" queue for HTTP answers. We add answers, and the server returns them in response to HTTP requests. The server starts on start() call and stops on stop() . Its method home() returns a URL of its \"home page\". The server then binds itself to a randomly allocated TCP port. The container finds the first available and unoccupied port. In the example above, I added just one answer. This means that the container will reply only to the first HTTP request with that answer and that all consecutive requests will cause HTTP responses with status \"internal server error 500 .\" In lines 5 through 8, I make an HTTP request to the already started server. Also, I make an assertion that the body of the HTTP response contains the text \"hello\" . Obviously, this assertion will pass because the server will return \"hello, world!\" to my first request: new JdkRequest ( container . home ()) . header ( \"User-agent\" , \"Myself\" ) . fetch () . assertBody ( Matchers . containsString ( \"hello\" )); As you can see, I use container.home() in order to get the URL of the server. It is recommended that you allow the container to find the first unoccupied TCP port and bind itself to it. Nevertheless, if you need to specify your own port, you can do it with a one-argument method start(int) in MkContainer . I use try/finally to stop the container safely. In unit tests, this is not critical, as you can simplify your code and never stop the container. Besides, the container will be killed together with the JVM. However, for the sake of clarity, I would recommend you stop the container in the finally block. On line 12, I ask the stopped container to give me the first request it received. This mechanism is similar conceptually to the \"verify\" technology of mocking frameworks. For example, Mockito . MkQuery query = container . take (); MatcherAssert . assertThat ( query . headers (). get ( \"User-agent\" ), Matchers . hasItem ( \"Myself\" ) ); An instance of MkQuery exposes information about the query made. In this example, I get all headers of the HTTP request and making an assertion that the \"User-Agent\" header was there and had at least one value equal to \"Myself\" . This mocking technology is used actively in unit and integration tests of jcabi-github , which is a Java client to Github API. In its development, the technology is very important in checking which requests are being sent to the server and validating whether they comply with our requirements. Here, we are using jcabi-http mocking. As with the client, you need the jcabi-http.jar dependency (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-http </artifactId> </dependency> Besides the above, you need to add one more dependency, which is a Grizzly HTTP server. MkGrizzlyContainer is based on it. <dependency> <groupId> com.sun.grizzly </groupId> <artifactId> grizzly-servlet-webserver </artifactId> <scope> test </scope> </dependency> If you have any questions or suggestions, please submit them through Github issues . As always, bugs are welcome :) "},{"title":"Basic HTTP Auth for S3 Buckets","url":"/2014/04/21/s3-http-basic-auth.html","date":"2014-04-21 00:00:00 +0000","categories":["2014","apr"],"body":" Amazon S3 is a simple and very useful storage of binary objects (aka \"files\"). To use it, you create a \"bucket\" there with a unique name and upload your objects. Afterwards, AWS guarantees your object will be available for download through their RESTful API . A few years ago, AWS introduced a S3 feature S3 called static website hosting . With static website hosting, you simply turn on the feature and all objects in your bucket become available through public HTTP. This is an awesome feature for hosting static content, such as images, JavaScript files, video and audio content. When using the hosting, you need to change the CNAME record in your DNS so that it points to www.example.com.aws.amazon.com . After changing the DNS entry, your static website is available at www.example.com just as it would be normally. When using Amazon S3, though, it is not possible to protect your website because the content is purely static. This means you can't have a login page on the front end. With the service, you can either make your objects either absolutely public — so that anyone can see them online — or assign access rights to them — but only for users connected through RESTful API. My use case with the service was a bit more complex, though. I wanted to host my static content as S3 objects. However, I wanted to do this while ensuring only a few people had access to the content using their Web browsers. HTTP Basic Authentication The HTTP protocol offers a nice \"basic access authentication\" feature that doesn't require any extra site pages. When an HTTP request arrives at the server, it doesn't deliver the content but replies with a 401 status response. This response means literally \"I don't know who you are, please authenticate yourself.\" The browser shows its native login screen and prompts for a user name and password. After entering the login credentials, they are concatenated, Base64 encoded, and added to the next request in Authorization HTTP header. Now, the browser tries to make another attempt to fetch the same webpage. But, this time, the HTTP request contains a header: Authorization: Basic am9lOnNlY3JldA== The above is just an example. In the example, the Base64 encoded part means joe:secret , where joe is the user name and secret the password entered by the user. This time the server has authentication information and can make a decision whether this user is authenticated (his password matches the server's records) and authorized (he has permission to access the request webpage). s3auth.com Since Amazon doesn't provide this feature, I decided to create a simple web service, s3auth.com , which stays in front of my Amazon S3 buckets and implements the HTTP-native authentication and authorization mechanism. Instead of making my objects public, though, I make them private and point my CNAME record to relay.s3auth.com . HTTP requests from Web browsers then arrive at my server, connect to Amazon S3, retrieve my objects and deliver them back in HTTP responses. The server implements authentication and authorization using a special file .htpasswd in the root of my bucket. The format of the \".htpasswd\" file is identical to the one used by Apache HTTP Server — one user per line. Every line has the name of a user and a hash version of his password. Implementation I made this software open source mostly to guarantee to my users that the server doesn't store their private data anywhere, but rather acts only as a pass-through service. As a result, the software is on Github . For the sake of privacy and convenience, I use only OAuth2 for user accounts. This means that I don't know who my users are. I don't possess their names or emails, but only their account numbers in Facebook, Google Plus or Github. Of course, I can find their names using these numbers, but this information is public anyway. The server is implemented in Java6. For its hosting, I'm using a single Amazon EC2 m1.small Ubuntu server. These days, the server seems to work properly and is stable. Extra Features Besides authentication and authorization, the s3auth.com server can render lists of pages — just like Apache HTTP Server. If you have a collection of objects in your bucket — but the index.html file is missing — Amazon S3 delivers a \"page not found\" result. Conversely, my server displays a list of objects in the bucket, when no \"index.html\" is present, and makes it possible to navigate up or down one folder. When your bucket has the versioning feature turned on, you are able to list all versions of any object in the browser. To do this, just add ?all-versions to the end of the URL to display the list. Next, click a version to have s3auth.com retrieve and render it. Traction I created this service mostly for myself, but apparently I'm not the only with the problems described above. At the moment, s3auth.com hosts over 300 domains and sends through more than 10Mb of data each hour. "},{"title":"Java XML Parsing Made Easy","url":"/2014/04/24/java-xml-parsing-and-traversing.html","date":"2014-04-24 00:00:00 +0000","categories":["2014","apr"],"body":"Unlike with many other modern languages, parsing XML in Java requires more than one line of code. XML traversing using XPath takes even more code, and I find this is unfair and annoying. I'm a big fan of XML and use it it in almost every Java application. Some time ago, I decided to put all of that XML-to-DOM parsing code into a small library — jcabi-xml . Put simply, the library is a convenient wrapper for JDK-native DOM manipulations. That's why it is small and dependency-free. With the following example, you can see just how simple XML parsing can be: import com.jcabi.xml.XML ; import com.jcabi.xml.XMLDocument ; XML xml = new XMLDocument ( \"<root><a>hello</a><b>world!</b></root>\" ); Now, we have an object of interface XML that can traverse the XML tree and convert it back to text. For example: // outputs \"hello\" System . out . println ( xml . xpath ( \"/root/a/text()\" ). get ( 0 )); // outputs the entire XML document System . out . println ( xml . toString ()); Method xpath() allows you to find a collection of text nodes or attributes in the document, and then convert them to a collection of strings, using XPath query : // outputs \"hello\" and \"world\" for ( String text : xml . xpath ( \"/root/*/text()\" )) { System . out . println ( text ); } Method nodes() enables the same XPath search operation, but instead returns a collection of instances of XML interface: // outputs \"<a>hello</a>\" and \"<b>world</b>\" for ( XML node : xml . xpath ( \"/root/*\" )) System . out . println ( node ); } Besides XML parsing, printing and XPath traversing, jcabi-xml also provides XSD validation and XSL transformations. I'll write about those features in the next post :) "},{"title":"Typical Mistakes in Java Code","url":"/2014/04/27/typical-mistakes-in-java-code.html","date":"2014-04-27 00:00:00 +0000","categories":["2014","apr"],"body":"This page contains most typical mistakes I see in the Java code of people working with me. Static analysis (we're using qulice can't catch all of the mistakes for obvious reasons, and that's why I decided to list them all here. Let me know if you want to see something else added here, and I'll be happy to oblige. All of the listed mistakes are related to object-oriented programming in general and to Java in particular. Class Names Read this short \"What is an Object?\" article. Your class should be an abstraction of a real life entity with no \"validators\", \"controllers\", \"managers\", etc. If your class name ends with an \"-er\" — it's a bad design . And, of course, utility classes are anti-patterns, like StringUtils , FileUtils , and IOUtils from Apache. The above are perfect examples of terrible designs. Read this follow up post: OOP Alternative to Utility Classes Of course, never add suffixes or prefixes to distinguish between interfaces and classes . For example, all of these names are terribly wrong: IRecord , IfaceEmployee , or RecordInterface . Usually, interface name is the name of a real-life entity, while class name should explain its implementation details. If there is nothing specific to say about an implementation, name it Default, Simple , or something similar. For example: class SimpleUser implements User {}; class DefaultRecord implements Record {}; class Suffixed implements Name {}; class Validated implements Content {}; Method Names Methods can either return something or return void . If a method returns something, then its name should explain what it returns , for example (don't use the get prefix ever): boolean isValid ( String name ); String content (); int ageOf ( File file ); If it returns void, then its name should explain what it does . For example: void save ( File file ); void process ( Work work ); void append ( File file , String line ); There is only one exception to the rule just mentioned — test methods for JUnit. They are explained below. Test Method Names Method names in JUnit tests should be created as English sentences without spaces. It's easier to explain by example: /** * HttpRequest can return its content in Unicode. * @throws Exception If test fails */ public void returnsItsContentInUnicode () throws Exception { } It's important to start the first sentence of your JavaDoc with the name of the class you're testing followed by can . So, your first sentence should always be similar to \"somebody can do something\". The method name will state exactly the same, but without the subject. If I add a subject at the beginning of the method name, I should get a complete English sentence, as in above example: \"HttpRequest returns its content in unicode\". Pay attention that the test method doesn't start with can .Only JavaDoc comments start with 'can.' Additionally, method names shouldn’t start with a verb. It's a good practice to always declare test methods as throwing Exception . Variable Names Avoid composite names of variables, like timeOfDay , firstItem , or httpRequest . I mean with both — class variables and in-method ones. A variable name should be long enough to avoid ambiguity in its scope of visibility, but not too long if possible. A name should be a noun in singular or plural form, or an appropriate abbreviation. For example: List < String > names ; void sendThroughProxy ( File file , Protocol proto ); private File content ; public HttpRequest request ; Sometimes, you may have collisions between constructor parameters and in-class properties if the constructor saves incoming data in an instantiated object. In this case, I recommend to create abbreviations by removing vowels (see how USPS abbreviates street names ). Another example: public class Message { private String recipient ; public Message ( String rcpt ) { this . recipient = rcpt ; } } In many cases, the best hint for a name of a variable can ascertained by reading its class name. Just write it with a small letter, and you should be good: File file ; User user ; Branch branch ; However, never do the same for primitive types, like Integer number or String string . You can also use an adjective, when there are multiple variables with different characteristics. For instance: String contact(String left, String right); Constructors Without exceptions, there should be only one constructor that stores data in object variables. All other constructors should call this one with different arguments. For example: public class Server { private String address ; public Server ( String uri ) { this . address = uri ; } public Server ( URI uri ) { this ( uri . toString ()); } } One-time Variables Avoid one-time variables at all costs. By \"one-time\" I mean variables that are used only once. Like in this example: String name = \"data.txt\" ; return new File ( name ); This above variable is used only once and the code should be refactored to: return new File ( \"data.txt\" ); Sometimes, in very rare cases — mostly because of better formatting — one-time variables may be used. Nevertheless, try to avoid such situations at all costs. Exceptions Needless to say, you should never swallow exceptions, but rather let them bubble up as high as possible. Private methods should always let checked exceptions go out. Never use exceptions for flow control. For example this code is wrong: int size ; try { size = this . fileSize (); } catch ( IOException ex ) { size = 0 ; } Seriously, what if that IOException says \"disk is full\"? Will you still assume that the size of the file is zero and move on? Indentation For indentation, the main rule is that a bracket should either end a line or be closed on the same line (reverse rule applies to a closing bracket). For example, the following is not correct because the first bracket is not closed on the same line and there are symbols after it. The second bracket is also in trouble because there are symbols in front of it and it is not opened on the same line: final File file = new File(directory, \"file.txt\"); Correct indentation should look like: StringUtils.join( Arrays.asList( \"first line\", \"second line\", StringUtils.join( Arrays.asList(\"a\", \"b\") ) ), \"separator\" ); The second important rule of indentation says that you should put as much as possible on one line - within the limit of 80 characters. The example above is not valid since it can be compacted: StringUtils.join( Arrays.asList( \"first line\", \"second line\", StringUtils.join(Arrays.asList(\"a\", \"b\")) ), \"separator\" ); Redundant Constants Class constants should be used when you want to share information between class methods, and this information is a characteristic (!) of your class. Don't use constants as a replacement of string or numeric literals — very bad practice that leads to code pollution. Constants (as with any object in OOP) should have a meaning in a real world. What meaning do these constants have in the real world: class Document { private static final String D_LETTER = \"D\" ; // bad practice private static final String EXTENSION = \".doc\" ; // good practice } Another typical mistake is to use constants in unit tests to avoid duplicate string/numeric literals in test methods. Don't do this! Every test method should work with its own set of input values. Use new texts and numbers in every new test method. They are independent. So, why do they have to share the same input constants? Test Data Coupling This is an example of data coupling in a test method: User user = new User ( \"Jeff\" ); // maybe some other code here MatcherAssert . assertThat ( user . name (), Matchers . equalTo ( \"Jeff\" )); On the last line, we couple \"Jeff\" with the same string literal from the first line. If, a few months later, someone wants to change the value on the third line, he/she has to spend extra time finding where else \"Jeff\" is used in the same method. To avoid this data coupling, you should introduce a variable. "},{"title":"XML/XPath Matchers for Hamcrest","url":"/2014/04/28/xml-xpath-hamcrest-matchers.html","date":"2014-04-28 00:00:00 +0000","categories":["2014","apr"],"body":" Hamcrest is my favorite instrument in unit testing. It replaces the JUnit procedural assertions of org.junit.Assert with an object-oriented mechanism. However, I will discuss that subject in more detail sometime later. Now, though, I want to demonstrate a new library published today on Github and Maven Central: jcabi-matchers . jcabi-matchers is a collection of Hamcrest matchers to make XPath assertions in XML and XHTML documents. Let's say, for instance, a class that is undergoing testing produces an XML that needs to contain a single <message> element with the content \"hello, world!\" This is how that code would look in a unit test: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import com.jcabi.matchers.XhtmlMatchers ; import org.hamcrest.MatcherAssert ; import org.junit.Test ; public class FooTest { @Test public void hasWelcomeMessage () { MatcherAssert . assertThat ( new Foo (). createXml (), XhtmlMatchers . hasXPaths ( \"/document[count(message)=1]\" , \"/document/message[.='hello, world!']\" ) ); } } There are two alternatives to the above that I'm aware of, which are do almost the same thing: xml-matchers by David Ehringer and hasXPath() method in Hamcrest itself. I have tried them both, but faced a number of problems. First, Hamcrest hasXPath() works only with an instance of Node . With this method, converting a String into Node becomes a repetitive and routine task in every unit test. The above is a very strange limitation of Hamcrest in contrast to jcabi-matchers , which works with almost anything, from a String to a Reader and even an InputStream . Second, `XmlMatchers from xml-matchers provides a very inconvenient way for working with namespaces. Before you can use an XPath query with a non-default namespace, you should create an instance of NamespaceContext. The library provides a simple implementation of this interface, but, still, it is requires extra code in every unit test. jcabi-matchers simplifies namespace handling problems even further, as it pre-defines most popular namespaces, including xtml , xs , xsl , etc. The following example works right out-of-the-box — without any extra configuration: 1 2 3 4 MatcherAssert . assertThat ( new URL ( \"http://www.google.com\" ). getContent (), XhtmlMatchers . hasXPath ( \"//xhtml:body\" ) ); To summarize, my primary objective with the library was its simplicity of usage. "},{"title":"W3C Java Validators","url":"/2014/04/29/w3c-java-validators.html","date":"2014-04-29 00:00:00 +0000","categories":["2014","apr"],"body":" A few years ago, I created two Java wrappers for W3C validators: ( HTML and CSS ). Both wrappers seemed to be working fine and were even listed by W3C on their website in the API section. Until recently, these wrappers have always been part of ReXSL library. A few days ago, though, I took the wrappers out of ReXSL and published them as a standalone library — jcabi-w3c . Consequently, now seems to be a good time to write a few words about them. Below is an example that demonstrates how you can validate an HTML document against W3C compliancy rules: 1 2 3 4 import com.jcabi.w3c.ValidatorBuilder ; assert ValidatorBuilder . html () . validate ( \"<html>hello, world!</html>\" ) . valid (); The valid() method is a black or white indicator that returns false when the document is not valid. Additionally, you can obtain more information through a list of \"defects\" returned by the W3C server: 1 2 3 Collection < Defect > defects = ValidatorBuilder . html () . validate ( \"<html>hello, world!</html>\" ) . errors (); The same can be done with CSS: 1 2 3 Collection < Defect > defects = ValidatorBuilder . css () . validate ( \"body { font-family: Arial; }\" ) . errors (); Personally, I think it is a good practice to validate all of HTML pages produced by your application against W3C during integration testing. It's not a matter of seeking perfection, but rather of preventing bigger problems later. These dependencies are mandatory when using jcabi-w3c (get their latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-w3c </artifactId> </dependency> <dependency> <groupId> org.glassfish </groupId> <artifactId> javax.json </artifactId> </dependency> <dependency> <groupId> com.sun.jersey </groupId> <artifactId> jersey-client </artifactId> </dependency> <dependency> <groupId> org.hamcrest </groupId> <artifactId> hamcrest-core </artifactId> </dependency> "},{"title":"DynamoDB Local Maven Plugin","url":"/2014/05/01/dynamodb-local-maven-plugin.html","date":"2014-05-01 00:00:00 +0000","categories":["2014","may"],"body":" DynamoDB Local is a locally running copy of Amazon DynamoDB server. Amazon developed the tool and based it on SQLite. It acts as a real DynamoDB service through the RESTful API. I guess, DynamoDB Local is meant to be used in integration testing and this is how we're going to use it below. I use Maven to run all of my Java integration testing using maven-failsafe-plugin . The philosophy of integration testing with Maven is that you start all your supplementary test stubs during the pre-integration-test phase, run your tests in the integration-test phase and then shutdown all stubs during the post-integration-test . It would be great if it were possible to use DynamoDB Local that way. I didn't find any Maven plugins for that purpose, so I decided to create my own — jcabi-dynamodb-maven-plugin . Full usage details for the plugin are explained on its website . However, here is a simple example (get its latest versions in Maven Central ): <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamodb-maven-plugin </artifactId> <executions> <execution> <goals> <goal> start </goal> <goal> stop </goal> </goals> <configuration> <port> 10500 </port> <dist> ${project.build.directory}/dynamodb-dist </dist> </configuration> </execution> </executions> </plugin> The above configuration will start DynamoDB Local right before running integration tests, and then stop it immediately afterwards. The server will listen at TCP port 10500. While the number is used in the example, you're supposed to use a randomly allocated port instead. When the DynamoDB Local server is up and running, we can create an integration test for it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import com.amazonaws.auth.BasicAWSCredentials ; import com.amazonaws.services.dynamodbv2.AmazonDynamoDB ; import com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient ; import com.amazonaws.services.dynamodbv2.model.ListTablesResult ; public class FooITCase { @Test public void worksWithAwsDynamoDb () { AmazonDynamoDB aws = new AmazonDynamoDBClient ( new BasicAWSCredentials ( \"\" , \"\" ) ); aws . setEndpoint ( \"http://localhost:10500\" ); ListTablesResult list = aws . listTables (); for ( String name : list . getTableNames ()) { System . out . println ( \"table found: \" + name ); } } } Of course, there won't be any output because the server starts without any tables. Since the server is empty, you should create tables before every integration test, using createTable() from DynamoDB SDK . To avoid this type of extra hassle, in the latest version 0.6 of jcabi-dynamodb-maven-plugin we introduced a new goal create-tables : <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-dynamodb-maven-plugin </artifactId> <executions> <execution> <goals> <goal> create-tables </goal> </goals> <configuration> <tables> <table> ${basedir}/src/test/dynamodb/foo.json </table> </tables> </configuration> </execution> </executions> </plugin> The foo.json file used above should contain a JSON request that is sent to DynamoDB Local right after it is up and running. The request should comply with the specification of CreateTable request. For example: { \"AttributeDefinitions\" : [ { \"AttributeName\" : \"id\" , \"AttributeType\" : \"N\" } ], \"KeySchema\" : [ { \"AttributeName\" : \"id\" , \"KeyType\" : \"HASH\" } ], \"ProvisionedThroughput\" : { \"ReadCapacityUnits\" : \"1\" , \"WriteCapacityUnits\" : \"1\" }, \"TableName\" : \"foo\" } The table will be created during the pre-integration-test phase and dropped at the post-integration-test phase. Now, we can make our integration test much more meaningful with the help of jcabi-dynamo : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import com.jcabi.dynamo.Attributes ; import com.jcabi.dynamo.Conditions ; import com.jcabi.dynamo.Credentials ; import com.jcabi.dynamo.Region ; import com.jcabi.dynamo.Table ; import org.hamcrest.MatcherAssert ; import org.hamcrest.Matchers ; public class FooITCase { @Test public void worksWithAwsDynamoDb () { Region region = new Region . Simple ( new Credentials . Simple ( \"\" , \"\" )); Table table = region . table ( \"foo\" ); table . put ( new Attributes () . with ( \"id\" , 123 ) . with ( \"name\" , \"Robert DeNiro\" ) ); MatcherAssert . assertThat ( table . frame (). where ( \"id\" , Conditions . equalTo ( 123 )), Matchers . notEmpty () ); } } The above test will put a new item into the table and then assert that the item is there. The plugin was tested with three operating systems, and proved to work without problems: Mac OS X 10.8.5, Windows 7 SP1 and Ubuntu Linux 12.04 Desktop. "},{"title":"OOP Alternative to Utility Classes","url":"/2014/05/05/oop-alternative-to-utility-classes.html","date":"2014-05-05 00:00:00 +0000","categories":["2014","may"],"body":"A utility class (aka helper class) is a \"structure\" that has only static methods and encapsulates no state. StringUtils , IOUtils , FileUtils from Apache Commons ; Iterables and Iterators from Guava , and Files from JDK7 are perfect examples of utility classes. This design idea is very popular in the Java world (as well as C#, Ruby, etc.) because utility classes provide common functionality used everywhere. Here, we want to follow the DRY principle and avoid duplication. Therefore, we place common code blocks into utility classes and reuse them when necessary: // This is a terrible design, don't reuse public class NumberUtils { public static int max ( int a , int b ) { return a > b ? a : b ; } } Indeed, this a very convenient technique!? Utility Classes Are Evil However, in an object-oriented world, utility classes are considered a very bad (some even may say \"terrible\") practice. There have been many discussions of this subject; to name a few: Are Helper Classes Evil? by Nick Malik, Why helper, singletons and utility classes are mostly bad by Simon Hart, Avoiding Utility Classes by Marshal Ward, Kill That Util Class! by Dhaval Dalal, Helper Classes Are A Code Smell by Rob Bagby. Additionally, there are a few questions on StackExchange about utility classes: If a “Utilities” class is evil, where do I put my generic code? , Utility Classes are Evil . A dry summary of all their arguments is that utility classes are not proper objects; therefore, they don't fit into object-oriented world. They were inherited from procedural programming, mostly because most were used to a functional decomposition paradigm back then. Assuming you agree with the arguments and want to stop using utility classes, I'll show by example how these creatures can be replaced with proper objects. Procedural Example Say, for instance, you want to read a text file, split it into lines, trim every line and then save the results in another file. This is can be done with FileUtils from Apache Commons: 1 2 3 4 5 6 7 8 void transform ( File in , File out ) { Collection < String > src = FileUtils . readLines ( in , \"UTF-8\" ); Collection < String > dest = new ArrayList <>( src . size ()); for ( String line : src ) { dest . add ( line . trim ()); } FileUtils . writeLines ( out , dest , \"UTF-8\" ); } The above code may look clean; however, this is procedural programming, not object-oriented. We are manipulating data (bytes and bits) and explicitly instructing the computer from where to retrieve them and then where to put them on every single line of code. We're defining a procedure of execution . Object-Oriented Alternative In an object-oriented paradigm, we should instantiate and compose objects, thus letting them manage data when and how they desire. Instead of calling supplementary static functions, we should create objects that are capable of exposing the behaviour we are seeking: public class Max implements Number { private final int a ; private final int b ; public Max ( int x , int y ) { this . a = x ; this . b = y ; } @Override public int intValue () { return this . a > this . b ? this . a : this . b ; } } This procedural call: int max = NumberUtils . max ( 10 , 5 ); Will become object-oriented: int max = new Max ( 10 , 5 ). intValue (); Potato, potato? Not really; just read on... Objects Instead of Data Structures This is how I would design the same file-transforming functionality as above but in an object-oriented manner: 1 2 3 4 5 6 7 8 9 void transform ( File in , File out ) { Collection < String > src = new Trimmed ( new FileLines ( new UnicodeFile ( in )) ); Collection < String > dest = new FileLines ( new UnicodeFile ( out ) ); dest . addAll ( src ); } FileLines implements Collection<String> and encapsulates all file reading and writing operations. An instance of FileLines behaves exactly as a collection of strings and hides all I/O operations. When we iterate it — a file is being read. When we addAll() to it — a file is being written. Trimmed also implements Collection<String> and encapsulates a collection of strings ( Decorator pattern ). Every time the next line is retrieved, it gets trimmed. All classes taking participation in the snippet are rather small: Trimmed , FileLines , and UnicodeFile . Each of them is responsible for its own single feature, thus following perfectly the single responsibility principle . On our side, as users of the library, this may be not so important, but for their developers it is an imperative. It is much easier to develop, maintain and unit-test class FileLines rather than using a readLines() method in a 80+ methods and 3000 lines utility class FileUtils . Seriously, look at its source code . An object-oriented approach enables lazy execution. The in file is not read until its data is required. If we fail to open out due to some I/O error, the first file won't even be touched. The whole show starts only after we call addAll() . All lines in the second snippet, except the last one, instantiate and compose smaller objects into bigger ones. This object composition is rather cheap for the CPU since it doesn't cause any data transformations. Besides that, it is obvious that the second script runs in O(1) space, while the first one executes in O(n). This is the consequence of our procedural approach to data in the first script. In an object-oriented world, there is no data; there are only objects and their behavior! "},{"title":"Why NULL is Bad?","url":"/2014/05/13/why-null-is-bad.html","date":"2014-05-13 00:00:00 +0000","categories":["2014","may"],"body":"A simple example of NULL usage in Java: 1 2 3 4 5 6 7 public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { return null ; } return new Employee ( id ); } What is wrong with this method? It may return NULL instead of an object — that's what is wrong. NULL is a terrible practice in an object-oriented paradigm and should be avoided at all costs. There have been a number of opinions about this published already, including Null References, The Billion Dollar Mistake presentation by Tony Hoare and the entire Object Thinking book by David West. Here, I'll try to summarize all the arguments and show examples of how NULL usage can be avoided and replaced with proper object-oriented constructs. Basically, there are two possible alternatives to NULL . The first one is Null Object design pattern (the best way is to make it a constant): public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { return Employee . NOBODY ; } return Employee ( id ); } The second possible alternative is to fail fast by throwing an Exception when you can't return an object: public Employee getByName ( String name ) { int id = database . find ( name ); if ( id == 0 ) { throw new EmployeeNotFoundException ( name ); } return Employee ( id ); } Now, let's see the arguments against NULL . Besides Tony Hoare's presentation and David West's book mentioned above, I read these publications before writing this post: Clean Code by Robert Martin, Code Complete by Steve McConnell, Say \"No\" to \"Null\" by John Sonmez, Is returning null bad design? discussion at StackOverflow. Ad-hoc Error Handling Every time you get an object as an input you must check whether it is NULL or a valid object reference. If you forget to check, a NullPointerException (NPE) may break execution in runtime. Thus, your logic becomes polluted with multiple checks and if/then/else forks: // this is a terrible design, don't reuse Employee employee = dept . getByName ( \"Jeffrey\" ); if ( employee == null ) { System . out . println ( \"can't find an employee\" ); System . exit (- 1 ); } else { employee . transferTo ( dept2 ); } This is how exceptional situations are supposed to be handled in C and other imperative procedural languages. OOP introduced exception handling primarily to get rid of these ad-hoc error handling blocks. In OOP, we let exceptions bubble up until they reach an application-wide error handler and our code becomes much cleaner and shorter: dept . getByName ( \"Jeffrey\" ). transferTo ( dept2 ); Consider NULL references an inheritance of procedural programming, and use 1) Null Objects or 2) Exceptions instead. Ambiguous Semantic In order to explicitly convey its meaning, the function getByName() has to be named getByNameOrNullIfNotFound() . The same should happen with every function that returns an object or NULL . Otherwise, ambiguity is inevitable for a code reader. Thus, to keep semantic unambiguous, you should give longer names to functions. To get rid of this ambiguity, always return a real object, a null object or throw an exception. Some may argue that we sometimes have to return NULL , for the sake of performance. For example, method get() of interface Map in Java returns NULL when there is no such item in the map: Employee employee = employees . get ( \"Jeffrey\" ); if ( employee == null ) { throw new EmployeeNotFoundException (); } return employee ; This code searches the map only once due to the usage of NULL in Map . If we would refactor Map so that its method get() will throw an exception if nothing is found, our code will look like this: if (! employees . containsKey ( \"Jeffrey\" )) { // first search throw new EmployeeNotFoundException (); } return employees . get ( \"Jeffrey\" ); // second search Obviously, this is method is twice as slow as the first one. What to do? The Map interface (no offense to its authors) has a design flaw. Its method get() should have been returning an Iterator so that our code would look like: Iterator found = Map . search ( \"Jeffrey\" ); if (! found . hasNext ()) { throw new EmployeeNotFoundException (); } return found . next (); BTW, that is exactly how C++ STL map::find() method is designed. Computer Thinking vs. Object Thinking Statement if (employee == null) is understood by someone who knows that an object in Java is a pointer to a data structure and that NULL is a pointer to nothing ( 0x00000000 , in Intel x86 processors). However, if you start thinking as an object, this statement makes much less sense. This is how our code looks from an object point of view: - Hello, is it a software department? - Yes. - Let me talk to your employee \"Jeffrey\" please. - Hold the line please... - Hello. - Are you NULL? The last question in this conversation sounds weird, doesn’t it? Instead, if they hang up the phone after our request to speak to Jeffrey, that causes a problem for us (Exception). At that point, we try to call again or inform our supervisor that we can't reach Jeffrey and complete a bigger transaction. Alternatively, they may let us speak to another person, who is not Jeffrey, but who can help with most of our questions or refuse to help if we need something \"Jeffrey specific\" (Null Object). Slow Failing Instead of failing fast , the code above attempts to die slowly, killing others on its way. Instead of letting everyone know that something went wrong and that an exception handling should start immediately, it is hiding this failure from its client. This argument is close to the \"ad-hoc error handling\" discussed above. It is a good practice to make your code as fragile as possible, letting it break when necessary. Make your methods extremely demanding as to the data they manipulate. Let them complain by throwing exceptions, if the provided data provided is not sufficient or simply doesn’t fit with the main usage scenario of the method. Otherwise, return a Null Object, that exposes some common behavior and throws exceptions on all other calls: public Employee getByName ( String name ) { int id = database . find ( name ); Employee employee ; if ( id == 0 ) { employee = new Employee () { @Override public String name () { return \"anonymous\" ; } @Override public void transferTo ( Department dept ) { throw new AnonymousEmployeeException ( \"I can't be transferred, I'm anonymous\" ); } }; } else { employee = Employee ( id ); } return employee ; } Mutable and Incomplete Objects In general, it is highly recommended to design objects with immutability in mind. This means that an object gets all necessary knowledge during its instantiating and never changes its state during the entire lifecycle. Very often, NULL values are used in lazy loading , to make objects incomplete and mutable. For example: public class Department { private Employee found = null ; public synchronized Employee manager () { if ( this . found == null ) { this . found = new Employee ( \"Jeffrey\" ); } return this . found ; } } This technology, although widely used, is an anti-pattern in OOP. Mostly because it makes an object responsible for performance problems of the computational platform, which is something an Employee object should not be aware of. Instead of managing a state and exposing its business-relevant behavior, an object has to take care of the caching of its own results — this is what lazy loading is about. Caching is not something an employee does in the office, does he? The solution? Don't use lazy loading in such a primitive way, as in the example above. Instead, move this caching problem to another layer of your application. For example, in Java, you can use aspect-oriented programming aspects. For example, jcabi-aspects has @Cacheable annotation that caches the value returned by a method: import com.jcabi.aspects.Cacheable ; public class Department { @Cacheable ( forever = true ) public Employee manager () { return new Employee ( \"Jacky Brown\" ); } } I hope this analysis was convincing enough that you will stop NULL -ing your code :) "},{"title":"Object-Oriented Github API","url":"/2014/05/14/object-oriented-github-java-sdk.html","date":"2014-05-14 00:00:00 +0000","categories":["2014","may"],"body":" Github is an awesome platform for maintaining Git sources and tracking project issues. I moved all my projects (both private and public) to Github about three years ago and have no regrets. Moreover, Github gives access to almost all of its features through RESTful JSON API. There are a few Java SDKs that wrap and expose the API. I tried to use them, but faced a number of issues: They are not really object-oriented (even though one of them has a description that says it is) They are not based on JSR-353 (JSON Java API) They provide no mocking instruments They don't cover the entire API and can't be extended Keeping in mind all those drawbacks, I created my own library — jcabi-github . Let's look at its most important advantages. Object Oriented for Real Github server is an object. A collection of issues is an object, an individual issue is an object, its author is an author, etc. For example, to retrieve the name of the author we use: Github github = new RtGithub ( /* credentials */ ); Repos repos = github . repos (); Repo repo = repos . get ( new Coordinates . Simple ( \"jcabi/jcabi-github\" )); Issues issues = github . issues (); Issue issue = issues . get ( 123 ); User author = new Issue . Smart ( issue ). author (); System . out . println ( author . name ()); Needless to say, Github , Repos , Repo , Issues , Issue , and User are interfaces. Classes that implement them are not visible in the library. Mock Engine MkGithub class is a mock version of a Github server. It behaves almost exactly the same as a real server and is the perfect instrument for unit testing. For example, say that you're testing a method that is supposed to post a new issue to Github and add a message into it. Here is how the unit test would look: public class FooTest { @Test public void createsIssueAndPostsMessage () { Github github = new MkGithub ( \"jeff\" ); github . repos (). create ( Json . createObjectBuilder (). add ( \"name\" , owner ). build () ); new Foo (). doTheThing ( github ); MatcherAssert . assertThat ( github . issues (). get ( 1 ). comments (). iterate (), Matchers . not ( Matchers . emptyIterable ()) ); } } This is much more convenient and compact than traditional mocking via Mockito or a similar framework. Extendable It is based on JSR-353 and uses jcabi-http for HTTP request processing. This combination makes it highly customizable and extendable, when some Github feature is not covered by the library (and there are many of them). For example, you want to get the value of hireable attribute of a User . Class User.Smart doesn't have a method for it. So, here is how you would get it: User user = // get it somewhere // name() method exists in User.Smart, let's use it System . out . println ( new User . Smart ( user ). name ()); // there is no hireable() method there System . out . println ( user . json (). getString ( \"hireable\" )); We're using method json() that returns an instance of JsonObject from JSR-353 (part of Java7). No other library allows such direct access to JSON objects returned by the Github server. Let's see another example. Say, you want to use some feature from Github that is not covered by the API. You get a Request object from Github interface and directly access the HTTP entry point of the server: Github github = new RtGithub ( oauthKey ); int found = github . entry () . uri (). path ( \"/search/repositories\" ). back () . method ( Request . GET ) . as ( JsonResponse . class ) . getJsonObject () . getNumber ( \"total_count\" ) . intValue (); jcabi-http HTTP client is used by jcabi-github . Immutable All classes are truly immutable and annotated with @Immutable . This may sound like a minor benefit, but it was very important for me. I'm using this annotation in all my projects to ensure my classes are truly immutable. Version 0.8 A few days ago we released the latest version 0.8 . It is a major release, that included over 1200 commits. It covers the entire Github API and is supposed to be very stable. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-github </artifactId> </dependency> "},{"title":"Atomic Counters at Stateful.co","url":"/2014/05/18/cloud-autoincrement-counters.html","date":"2014-05-18 00:00:00 +0000","categories":["2014","may"],"body":" Amazon DynamoDB is a great NoSQL cloud database. It is cheap, highly reliable and rather powerful. I'm using it in many web systems. There is one feature that it lacks, though — auto-increment attributes. Say that you have a table with a list of messages: +------+----------------------------+ | id | Attributes | +------+----------------------------+ | 205 | author=\"jeff\", text=\"...\" | | 206 | author=\"bob\", text=\"...\" | | 207 | author=\"alice\", text=\"...\" | +------+----------------------------+ Every time you add a new item to the table, a new value of id has to be set. And this has to be done with concurrency in mind. SQL databases like PostgreSQL, Oracle, MySQL and others support auto-increment features. When you add a new record to the table, the value of the primary key is omitted and the server retrieves the next one automatically. If a number of INSERT requests arrive at the same time the server guarantees that the numbers won't be duplicated. However, DynamoDB doesn't have this feature. Instead, DynamoDB has Atomic Counters and Conditional Updates , which are very similar features. Still, they're not exactly the same. In case of an atomic counter, you should create a supplementary table and keep the latest value of id in it. In case of conditional updates, you should retry a few times in case of collisions. To make life easier in a few of my applications, I created a simple web service — stateful.co . It provides a simple atomic counter feature through its RESTful API. First, you create a counter with a unique name. Then, you set its initial value (it is zero by default). And, that's it. Every time you need to obtain a new value for id column in DynamoDB table, you make an HTTP request to stateful.co asking to increment your counter by one and return its next value. stateful.co guarantees that values returned will never duplicate each other — no matter how many clients are using a counter or how fast they request increments simultaneously. Moreover, I designed a small Java SDK for stateful.co . All you need to do is add this java-sdk.jar Maven dependency to your project: <dependency> <groupId> co.stateful </groupId> <artifactId> java-sdk </artifactId> <version> 0.6 </version> </dependency> And, you can use stateful.co counters from Java code: Sttc sttc = new RtSttc ( new URN ( \"urn:github:526301\" ), \"9FF3-41E0-73FB-F900\" ); Counters counters = sttc . counters (); Counter counter = counters . get ( \"foo\" ); long value = counter . incrementAndGet ( 1L ); System . out . println ( \"new value: \" + value ); You can review authentication parameters for RtSttc constructor at stateful.co . The service is absolutely free of charge. "},{"title":"MySQL Maven Plugin","url":"/2014/05/21/mysql-maven-plugin.html","date":"2014-05-21 00:00:00 +0000","categories":["2014","may"],"body":"I was using MySQL in a few Java web projects and found out there was no Maven plugin that would help me to test my DAO classes against a real MySQL server. There are plenty of mechanisms to mock a database persistence layer both in memory and on disc. However, it is always good to make sure that your classes are tested against a database identical to the one you have in production environment. I've created my own Maven plugin, jcabi-mysql-maven-plugin , that does exactly two things: starts a MySQL server on pre-integration-test phase and shuts it down on post-integration-test . This is how you configure it in pom.xml (see also its full usage instructions ): <project> <build> <plugins> <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> build-helper-maven-plugin </artifactId> <executions> <execution> <goals> <goal> reserve-network-port </goal> </goals> <configuration> <portNames> <portName> mysql.port </portName> </portNames> </configuration> </execution> </executions> </plugin> <plugin> <artifactId> maven-dependency-plugin </artifactId> <executions> <execution> <goals> <goal> unpack </goal> </goals> <configuration> <artifactItems> <artifactItem> <groupId> com.jcabi </groupId> <artifactId> mysql-dist </artifactId> <version> 5.6.14 </version> <classifier> ${mysql.classifier} </classifier> <type> zip </type> <overWrite> false </overWrite> <outputDirectory> ${project.build.directory}/mysql-dist </outputDirectory> </artifactItem> </artifactItems> </configuration> </execution> </executions> </plugin> <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-mysql-maven-plugin </artifactId> <executions> <execution> <id> mysql-test </id> <goals> <goal> classify </goal> <goal> start </goal> <goal> stop </goal> </goals> <configuration> <port> ${mysql.port} </port> <data> ${project.build.directory}/mysql-data </data> </configuration> </execution> </executions> </plugin> <plugin> <artifactId> maven-failsafe-plugin </artifactId> <configuration> <systemPropertyVariables> <mysql.port> ${mysql.port} </mysql.port> </systemPropertyVariables> </configuration> <executions> <execution> <goals> <goal> integration-test </goal> <goal> verify </goal> </goals> </execution> </executions> </plugin> </plugins> </build> [...] </project> There are two plugins configured above. Let's take a look at what each does. build-helper-maven-plugin is reserving a temporary random TCP port, which will be used by MySQL server. We don't want to start a server on its default 3306 port, because there could be another server already running there. Besides that, if we use a hard-coded TCP port, we won't be able to run multiple builds in parallel. Maybe not a big deal when you're developing locally, but in continuous integration environment this can be a problem. That's why we're reserving a TCP port first. maven-dependency-plugin is downloading a MySQL distribution in a zip archive (rather big file, over 300Mb for Linux), and unpacks it. This archive contains exactly the same files as you would use for a traditional MySQL installation. When the archive is unpacked, it is ready to start serving SQL requests as a normal MySQL server. jcabi-mysql-maven-plugin starts a server, binding it to a TCP port reserved randomly. The main responsibility of my Maven plugin is to make sure that MySQL server starts correctly on every platform (Mac OS, Linux, Windows) and stops when it's not needed any more. All the rest is done by the MySQL distribution itself. maven-failsafe-plugin is running unit tests on integration-test phase. Its main difference from maven-surefire-plugin is that it doesn't fail a build when some tests fail. Instead, it saves all failures into supplementary files in target directory and allows the build continue. Later, when we call its verify goal, it will fail a build if there were any errors during its integration-test goal execution. To be precise, this is the order in which Maven will execute configured goals: jcabi-mysql-maven-plugin:classify maven-dependency-plugin:unpack build-helper-maven-plugin:reserve-network-port jcabi-mysql-maven-plugin:start maven-failsafe-plugin:integration-test jcabi-mysql-maven-plugin:stop maven-failsafe-plugin:verify Run mvn clean install and see how it works. If it doesn't work for some reason, don't hesitate to report an issue to Github . Now it's time to create an integration test, which will connect to the temporary MySQL server, create a table there and insert some data into it. This is just an example to show that MySQL server is running and is capable of serving transactions (I'm using jcabi-jdbc ): public class FooITCase { private static final String PORT = System . getProperty ( \"mysql.port\" ); @Test public void worksWithMysqlServer () { Connection conn = DriverManager . getConnection ( String . format ( \"jdbc:mysql://localhost:%s/root?user=root&password=root\" , FooITCase . PORT ) ); new JdbcSession ( conn ) . sql ( \"CREATE TABLE foo (id INT PRIMARY KEY)\" ) . execute (); } } If you're using Hibernate, just create a db.properties file in src/test/resources directory. In that file you would do something like: hibernate.connection.url = jdbc:mysql://localhost:${mysql.port}/root hibernate.connection.username = root hibernate.connection.password = root Maven will replace that ${mysql.port} with the number of reserved TCP port, during resources copying. This operation is called \"resources filtering\", and you can read about it here . That's pretty much it. I'm using jcabi-mysql-maven-plugin in a few projects, and it helps me to stay confident that my code works with a real MySQL server. I'm also using the Liquibase Maven plugin in order to populate an empty server with tables required for the application. Nevertheless, that is a story for the next post :) "},{"title":"Get Rid of Java Static Loggers","url":"/2014/05/23/avoid-java-static-logger.html","date":"2014-05-23 00:00:00 +0000","categories":["2014","may"],"body":"This is a very common practice in Java (using LoggerFactory from slf4j ): import org.slf4j.LoggerFactory ; public class Foo { private static final Logger LOGGER = LoggerFactory . getLogger ( Foo . class ); public void save ( String file ) { // save the file if ( Foo . LOGGER . isInfoEnabled ()) { Foo . LOGGER . info ( \"file {} saved successfuly\" , file ); } } } What's wrong with it? Code duplication. This static LOGGER property has to be declared in every class where logging is required. Just a few lines of code, but this is pure noise, as I see it. To make life easier, I created a library about two years ago, jcabi-log , which has a convenient utility class Logger (yes, I know that utility classes are evil ). import com.jcabi.log.Logger ; public class Foo { public void save ( String file ) { // save the file Logger . info ( this , \"file %s saved successfuly\" , file ); } } This looks much cleaner to me and does exactly the same — sends a single log line to the SLF4J logging facility. Besides, it check automatically whether a given logging level is enabled (for performance optimization) and formats the given string using Formatter (same as String.format() ). For convenience, there are also a number of \"decors\" implemented in the library. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-log </artifactId> </dependency> "},{"title":"Object-Oriented Java Adapter of Amazon S3 SDK","url":"/2014/05/26/amazon-s3-java-oop-adapter.html","date":"2014-05-26 00:00:00 +0000","categories":["2014","may"],"body":" I'm a big fan of Amazon Web Services (AWS). I'm using them in almost all of my projects. One of their most popular services is Simple Storage Service (S3) . It is a storage for binary objects (files) with unique names, accessible through HTTP or RESTful API. Using S3 is very simple. You create a \"bucket\" with a unique name, upload your \"object\" into the bucket through their web interface or through RESTful API, and then download it again (either through HTTP or the API.) Amazon ships the Java SDK that wraps their RESTful API. However, this SDK is not object-oriented at all. It is purely imperative and procedural — it just mirrors the API. For example, in order to download an existing object doc.txt from bucket test-1 , you have to do something like this: AWSCredentials creds = new BasicAWSCredentials ( key , secret ); AmazonS3 aws = new AmazonS3Client ( creds ); S3Object obj = aws . getObject ( new GetObjectRequest ( \"test-1\" , \"doc.txt\" ) ); InputStream input = obj . getObjectContent (); String content = IOUtils . toString ( input , \"UTF-8\" ); input . close (); As always, procedural programming has its inevitable disadvantages. To overcome them all, I designed jcabi-s3 , which is a small object-oriented adapter for Amazon SDK. This is how the same object-reading task can be accomplished with jcabi-s3 : Region region = new Region . Simple ( key , secret ); Bucket bucket = region . bucket ( \"test-1\" ); Ocket ocket = bucket . ocket ( \"doc.txt\" ); String content = new Ocket . Text ( ocket ). read (); Why is this approach better? Well, there are a number of obvious advantages. S3 Object is an Object in Java S3 object get its representative in Java. It is not a collection of procedures to be called in order to get its properties (as with AWS SDK). Rather, it is a Java object with certain behaviors. I called them \"ockets\" (similar to \"buckets\"), in order to avoid clashes with java.lang.Object . Ocket is an interface, that exposes the behavior of a real AWS S3 object: read, write, check existence. There is also a convenient decorator Ocket.Text that simplifies working with binary objects: Ocket . Text ocket = new Ocket . Text ( ocket_from_s3 ); if ( ocket . exists ()) { System . out . print ( ocket . read ()); } else { ocket . write ( \"Hello, world!\" ); } Now, you can pass an object to another class, instead of giving it your AWS credentials, bucket name, and object name. You simply pass a Java object, which encapsulates all AWS interaction details. Extendability Through Decoration Since jcabi-s3 exponses all entities as interfaces, they can easily be extended through encapsulation ( Decorator Pattern ). For example, you want your code to retry S3 object read operations a few times before giving up and throwing an IOException (by the way, this is a very good practice when working with web services). So, you want all your S3 reading operations to be redone a few times if first attempts fail. You define a new decorator class, say, RetryingOcket , which encapsulates an original Ocket : public RetryingOcket implements Ocket { private final Ocket origin ; public RetryingOcket ( Ocket ocket ) { this . origin = ocket ; } @Override public void read ( OutputStream stream ) throws IOException { int attempt = 0 ; while ( true ) { try { this . origin . read ( stream ); } catch ( IOException ex ) { if ( attempt ++ > 3 ) { throw ex ; } } } } // same for other methods } Now, everywhere where Ocket is expected you send an instance of RetryingOcket that wraps your original object: foo . process ( new RetryingOcket ( ocket )); Method foo.process() won't see a difference, since it is the same Ocket interface it is expecting. By the way, this retry functionality is implemented out-of-the-box in jcabi-s3 , in com.jcabi.s3.retry package. Easy Mocking Again, due to the fact that all entities in jcabi-s3 are interfaces, they are very easy to mock. For example, your class expects an S3 object, reads its data and calculates the MD5 hash (I'm using DigestUtils from commons-codec ): import com.jcabi.s3.Ocket ; import org.apache.commons.codec.digest.DigestUtils ; public class S3Md5Hash { private final Ocket ocket ; public S3Md5Hash ( Ocket okt ) { this . ocket = okt ; } public hash () throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream (); this . ocket . read ( baos ); return DigestUtils . md5hex ( baos . toByteArray ()); } } Here is how simple a unit test will look (try to create a unit test for a class using AWS SDK and you will see the difference): import com.jcabi.s3.Ocket ; import org.junit.Test ; public class S3Md5HashTest { @Test public void generatesHash () { Ocket ocket = Mockito . mock ( Ocket . class ); Mockito . doAnswer ( new Answer < Void >() { public Void answer ( final InvocationOnMock inv ) throws IOException { OutputStream . class . cast ( inv . getArguments ()[ 0 ]). write ( ' ' ); } } ). when ( ocket ). read ( Mockito . any ( OutputStream . class )); String hash = new S5Md5Hash ( ocket ); Assert . assertEquals ( hash , \"7215ee9c7d9dc229d2921a40e899ec5f\" ); } } I'm using JUnit and Mockito in this test. Immutability All classes in jcabi-s3 are annotated with @Immutable and are truly immutable. The library ships as a JAR dependency in Maven Central (get its latest versions in Maven Central ): <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-s3 </artifactId> </dependency> As always, your comments and criticism are welcome as Github issues . "},{"title":"Java Method Logging with AOP and Annotations","url":"/2014/06/01/aop-aspectj-java-method-logging.html","date":"2014-06-01 00:00:00 +0000","categories":["2014","jun"],"body":"Sometimes, I want to log (through slf4j and log4j ) every execution of a method, seeing what arguments it receives, what it returns and how much time every execution takes. This is how I'm doing it, with help of AspectJ , jcabi-aspects and Java 6 annotations: public class Foo { @Loggable public int power ( int x , int p ) { return Math . pow ( x , p ); } } This is what I see in log4j output: [INFO] com.example.Foo #power(2, 10): 1024 in 12μs [INFO] com.example.Foo #power(3, 3): 27 in 4μs Nice, isn't it? Now, let's see how it works. Annotation with Runtime Retention Annotations is a technique introduced in Java 6. It is a meta-programming instrument that doesn't change the way code works, but gives marks to certain elements (methods, classes or variables). In other words, annotations are just markers attached to the code that can be seen and read. Some annotations are designed to be seen at compile time only — they don't exist in .class files after compilation. Others remain visible after compilation and can be accessed in runtime. For example, @Override is of the first type (its retention type is SOURCE ), while @Test from JUnit is of the second type (retention type is RUNTIME ). @Loggable — the one I'm using in the script above — is an annotation of the second type, from jcabi-aspects . It stays with the bytecode in the .class file after compilation. Again, it is important to understand that even though method power() is annotated and compiled, it doesn't send anything to slf4j so far. It just contains a marker saying \"please, log my execution\". Aspect Oriented Programming (AOP) AOP is a useful technique that enables adding executable blocks to the source code without explicitly changing it. In our example, we don't want to log method execution inside the class. Instead, we want some other class to intercept every call to method power() , measure its execution time and send this information to slf4j. We want that interceptor to understand our @Loggable annotation and log every call to that specific method power() . And, of course, the same interceptor should be used for other methods where we'll place the same annotation in the future. This case perfectly fits the original intent of AOP — to avoid re-implementation of some common behavior in multiple classes. Logging is a supplementary feature to our main functionality, and we don't want to pollute our code with multiple logging instructions. Instead, we want logging to happen behind the scenes. In terms of AOP, our solution can be explained as creating an aspect that cross-cuts the code at certain join points and applies an around advice that implements the desired functionality. AspectJ Let's see what these magic words mean. But, first, let's see how jcabi-aspects implements them using AspectJ (it's a simplified example, full code you can find in MethodLogger.java ): @Aspect public class MethodLogger { @Around ( \"execution(* *(..)) && @annotation(Loggable)\" ) public Object around ( ProceedingJoinPoint point ) { long start = System . currentTimeMillis (); Object result = point . proceed (); Logger . info ( \"#%s(%s): %s in %[msec]s\" , MethodSignature . class . cast ( point . getSignature ()). getMethod (). getName (), point . getArgs (), result , System . currentTimeMillis () - start ); return result ; } } This is an aspect with a single around advice around() inside. The aspect is annotated with @Aspect and advice is annotated with @Around . As discussed above, these annotations are just markers in .class files. They don't do anything except provide some meta-information to those w ho are interested in runtime. Annotation @Around has one parameter, which — in this case — says that the advice should be applied to a method if: its visibility modifier is * ( public , protected or private ); its name is name * (any name); its arguments are .. (any arguments); and it is annotated with @Loggable When a call to an annotated method is to be intercepted, method around() executes before executing the actual method. When a call to method power() is to be intercepted, method around() receives an instance of class ProceedingJoinPoint and must return an object, which will be used as a result of method power() . In order to call the original method, power() , the advice has to call proceed() of the join point object. We compile this aspect and make it available in classpath together with our main file Foo.class . So far so good, but we need to take one last step in order to put our aspect into action — we should apply our advice. Binary Aspect Weaving Aspect weaving is the name of the advice applying process. Aspect weaver modifies original code by injecting calls to aspects. AspectJ does exactly that. We give it two binary Java classes Foo.class and MethodLogger.class ; it gives back three — modified Foo.class , Foo$AjcClosure1.class and unmodified MethodLogger.class . In order to understand which advices should be applied to which methods, AspectJ weaver is using annotations from .class files. Also, it uses reflection to browse all classes on classpath. It analyzes which methods satisfy the conditions from the @Around annotation. Of course, it finds our method power() . So, there are two steps. First, we compile our .java files using javac and get two files. Then, AspectJ weaves/modifies them and creates its own extra class. Our Foo class looks something like this after weaving: public class Foo { private final MethodLogger logger ; @Loggable public int power ( int x , int p ) { return this . logger . around ( point ); } private int power_aroundBody ( int x , int p ) { return Math . pow ( x , p ); } } AspectJ weaver moves our original functionality to a new method, power_aroundBody() , and redirects all power() calls to the aspect class MethodLogger . Instead of one method power() in class Foo now we have four classes working together. From now on, this is what happens behind the scenes on every call to power() : Original functionality of method power() is indicated by the small green lifeline on the diagram. As you see, the aspect weaving process connects together classes and aspects, transferring calls between them through join points. Without weaving, both classes and aspects are just compiled Java binaries with attached annotations. jcabi-aspects jcabi-aspects is a JAR library that contains Loggable annotation and MethodLogger aspect (btw, there are many more aspects and annotations). You don't need to write your own aspect for method logging. Just add a few dependencies to your classpath and configure jcabi-maven-plugin for aspect weaving (get their latest versions in Maven Central ): <project> <depenencies> <dependency> <dependency> <groupId> com.jcabi </groupId> <artifactId> jcabi-aspects </artifactId> </dependency> <dependency> <groupId> org.aspectj </groupId> <artifactId> aspectjrt </artifactId> </dependency> </dependency> </depenencies> <build> <plugins> <plugin> <groupId> com.jcabi </groupId> <artifactId> jcabi-maven-plugin </artifactId> <executions> <execution> <goals> <goal> ajc </goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> Since this weaving procedure takes a lot of configuration effort, I created a convenient Maven plugin with an ajc goal, which does the entire aspect weaving job. You can use AspectJ directly, but I recommend that you use jcabi-maven-plugin . That's it. Now you can use @com.jcabi.aspects.Loggable annotation and your methods will be logged through slf4j. If something doesn't work as explained, don't hesitate to submit a Github issue . "},{"title":"Objects Should Be Immutable","url":"/2014/06/09/objects-should-be-immutable.html","date":"2014-06-09 00:00:00 +0000","categories":["2014","jun"],"body":"In object-oriented programming, an object is immutable if its state can't be modified after it is created. In Java, a good example of an immutable object is String . Once created, we can't modify its state. We can request that it creates new strings, but its own state will never change. However, there are not so many immutable classes in JDK. Take, for example, class Date . It is possible to modify its state using setTime() . I don't know why the JDK designers decided to make these two very similar classes differently. However, I believe that the design of a mutable Date has a many flaws, while the immutable String is much more in the spirit of the object-oriented paradigm. Moreover, I think that all classes should be immutable in a perfect object-oriented world . Unfortunately, sometimes, it is technically not possible due to limitations in JVM. Nevertheless, we should always aim for the best. This is an incomplete list of arguments in favor of immutability: immutable objects are simpler to construct, test, and use truly immutable objects are always thread-safe they help to avoid temporal coupling their usage is side-effect free (no defensive copies) identity mutability problem is avoided they always have failure atomicity they are much easier to cache they prevent NULL references, which are bad Let's discuss the most important arguments one by one. Thread Safety The first and the most obvious argument is that immutable objects are thread-safe. This means that multiple threads can access the same object at the same time, without clashing with another thread. If no object methods can modify its state, no matter how many of them and how often are being called parallel — they will work in their own memory space in stack. Goetz et al. explained the advantages of immutable objects in more details in their very famous book Java Concurrency in Practice (highly recommended). Avoiding Temporal Coupling Here is an example of temporal coupling (the code makes two consecutive HTTP POST requests, where the second one contains HTTP body): 1 2 3 4 5 Request request = new Request ( \"http://example.com\" ); request . method ( \"POST\" ); String first = request . fetch (); request . body ( \"text=hello\" ); String second = request . fetch (); This code works. However, you must remember that the first request should be configured before the second one may happen. If we decide to remove the first request from the script, we will remove the second and the third line, and won't get any errors from the compiler: Request request = new Request ( \"http://example.com\" ); // request.method(\"POST\"); // String first = request.fetch(); request . body ( \"text=hello\" ); String second = request . fetch (); Now, the script is broken although it compiled without errors. This is what temporal coupling is about — there is always some hidden information in the code that a programmer has to remember. In this example, we have to remember that the configuration for the first request is also used for the second one. We have to remember that the second request should always stay together and be executed after the first one. If Request class were immutable, the first snippet wouldn't work in the first place, and would have been rewritten like: final Request request = new Request ( \"\" ); String first = request . method ( \"POST\" ). fetch (); String second = request . method ( \"POST\" ). body ( \"text=hello\" ). fetch (); Now, these two requests are not coupled. We can safely remove the first one, and the second one will still work correctly. You may point out that there is a code duplication. Yes, we should get rid of it and re-write the code: final Request request = new Request ( \"\" ); final Request post = request . method ( \"POST\" ); String first = post . fetch (); String second = post . body ( \"text=hello\" ). fetch (); See, refactoring didn't break anything and we still don't have temporal coupling. The first request can be removed safely from the code without affecting the second one. I hope this example demonstrates that the code manipulating immutable objects is more readable and maintainable, b ecause it doesn't have temporal coupling. Avoiding Side Effects Let's try to use our Request class in a new method (now it is mutable): public String post ( Request request ) { request . method ( \"POST\" ); return request . fetch (); } Let's try to make two requests — the first with GET method and the second with POST: Request request = new Request ( \"http://example.com\" ); request . method ( \"GET\" ); String first = this . post ( request ); String second = request . fetch (); Method post() has a \"side effect\" — it makes changes to the mutable object request . These changes are not really expected in this case. We expect it to make a POST request and return its body. We don't want to read its documentation just to find out that behind the scene it also modifies the request we're passing to it as an argument. Needless to say, such side effects lead to bugs and maintainability issues. It would be much better to work with an immutable Request : public String post ( Request request ) { return request . method ( \"POST\" ). fetch (); } In this case, we may not have any side effects. Nobody can modify our request object, no matter where it is used and how deep through the call stack it is passed by method calls: Request request = new Request ( \"http://example.com\" ). method ( \"GET\" ); String first = this . post ( request ); String second = request . fetch (); This code is perfectly safe and side effect free. Avoiding Identity Mutability Very often, we want objects to be identical if their internal states are the same. Date class is a good example: Date first = new Date ( 1L ); Date second = new Date ( 1L ); assert first . equals ( second ); // true There are two different objects; however, they are equal to each other because their encapsulated states are the same. This is made possible through their custom overloaded implementation of equals() and hashCode() methods. The consequence of this convenient approach being used with mutable objects is that every time we modify object's state it changes its identity: Date first = new Date ( 1L ); Date second = new Date ( 1L ); first . setTime ( 2L ); assert first . equals ( second ); // false This may look natural, until you start using your mutable objects as keys in maps: Map < Date , String > map = new HashMap <>(); Date date = new Date (); map . put ( date , \"hello, world!\" ); date . setTime ( 12345L ); assert map . containsKey ( date ); // false When modifying the state of date object, we're not expecting it to change its identity. We're not expecting to lose an entry in the map just because the state of its key is changed. However, this is exactly what is happening in the example above. When we add an object to the map, its hashCode() returns one value. This value is used by HashMap to place the entry into the internal hash table. When we call containsKey() hash code of the object is different (because it is based on its internal state) and HashMap can't find it in the internal hash table. It is a very annoying and difficult to debug side effects of mutable objects. Immutable objects avoid it completely. Failure Atomicity Here is a simple example: public class Stack { private int size ; private String [] items ; public void push ( String item ) { size ++; if ( size > items . length ) { throw new RuntimeException ( \"stack overflow\" ); } items [ size ] = item ; } } It is obvious that an object of class Stack will be left in a broken state if it throws a runtime exception on overflow. Its size property will be incremented, while items won't get a new element. Immutability prevents this problem. An object will never be left in a broken state because its state is modified only in its constructor. The constructor will either fail, rejecting object instantiation, or succeed, making a valid solid object, which never changes its encapsulated state. For more on this subject, read Effective Java, 2nd Edition by Joshua Bloch. Arguments Against Immutability There are a number of arguments against immutability. “Immutability is not for enterprise systems”. Very often, I hear people say that immutability is a fancy feature, while absolutely impractical in real enterprise systems. As a counter-argument, I can only show some examples of real-life applications that contain only immutable Java objects: jcabi-http , jcabi-xml , jcabi-github , jcabi-s3 , jcabi-dynamo , jcabi-simpledb The above are all Java libraries that work solely with immutable classes/objects. netbout.com and stateful.co are web applications that work solely with immutable objects. “It's cheaper to update an existing object than create a new one”. Oracle thinks that “The impact of object creation is often overestimated and can be offset by some of the efficiencies associated with immutable objects. These include decreased overhead due to garbage collection, and the elimination of code needed to protect mutable objects from corruption.” I agree. If you have some other arguments, please post them below and I'll try to comment. "},{"title":"Avoid String Concatenation","url":"/2014/06/19/avoid-string-concatenation.html","date":"2014-06-19 00:00:00 +0000","categories":["2014","jun"],"body":"This is \"string concatentation\", and it is a bad practice: // bad practice, don't reuse! String text = \"Hello, \" + name + \"!\" ; Why? Some may say that it is slow, mostly because parts of the resulting string are copied multiple times. Indeed, on every + operator, String class allocates a new block in memory and copies everything it has into it; plus a suffix being concatenated. This is true, but this is not the point here. Actually, I don't think performance in this case is a big issue. Moreover, there were multiple experiments showing that concatenation is not that slow when compared to other string building methods and sometimes is even faster. Some say that concatenated strings are not localizable because in different languages text blocks in a phrase may be positioned in a different order. The example above can't be translated to, say, Russian, where we would want to put a name in front of \"привет\". We will need to localize the entire block of code, instead of just translating a phrase. However, my point here is different. I strongly recommend avoiding string concatenation because it is less readable than other methods of joining texts together. Let's see these alternative methods. I'd recommend three of them (in order of preference): String.format() , Apache StringUtils and Guava Joiner . There is also a StringBuilder , but I don't find it as attractive as StringUtils . It is a useful builder of strings, but not a proper replacer or string concatenation tool when readability is important. String.format() String.format() is my favorite option. It makes text phrases easy to understand and modify. It is a static utility method that mirrors sprintf() from C. It allows you to build a string using a pattern and substitutors: String text = String . format ( \"Hello, %s!\" , name ); When the text is longer, the advantages of the formatter become much more obvious. Look at this ugly code: String msg = \"Dear \" + customer . name () + \", your order #\" + order . number () + \" has been shipped at \" + shipment . date () + \"!\" ; This one looks much more beautiful doesn’t it: String msg = String . format ( \"Dear %1$s, your order #%2$d has been shipped at %3$tR!\" , customer . name (), order . number (), shipment . date () ); Please note that I'm using argument indexes in order to make the pattern even more localizable. Let's say, I want to translate it to Greek. This is how will it look: Αγαπητέ %1$s, στις %3$tR στείλαμε την παραγγελία σου με αριθμό #%2$d! I'm changing the order of substitutions in the pattern, but not in the actual list of methods arguments. Apache StringUtils.join() When the text is rather long (longer than your screen width), I would recommend that you use the utility class StringUtils from Apache commons-lang3 : import org.apache.commons.lang3.StringUtils ; String xml = StringUtils . join ( \"<?xml version='1.0'?>\" , \"<html><body>\" , \"<p>This is a test XHTML document,\" , \" which would look ugly,\" , \" if we would use a single line,\" \" or string concatenation or String format().</p>\" \"</body></html>\" ); The need to include an additional JAR dependency to your classpath may be considered a downside with this method (get its latest versions in Maven Central ): <dependency> <groupId> org.apache.commons </groupId> <artifactId> commons-lang3 </artifactId> </dependency> Guava Joiner Similar functionality is provided by Joiner from Google Guava : import com.google.common.base.Joiner ; String text = Joiner . on ( '' ). join ( \"WE HAVE BUNNY.\\n\" , \"GATHER ONE MILLION DOLLARS IN UNMARKED \" , \"NON-CONSECUTIVE TWENTIES.\\n\" , \"AWAIT INSTRUCTIONS.\\n\" , \"NO FUNNY STUFF\" ); It is a bit less convenient than StringUtils since you always have to provide a joiner (character or a string placed between text blocks). Again, a dependency is required in this case: <dependency> <groupId> com.google.guava </groupId> <artifactId> guava </artifactId> </dependency> Yes, in most cases, all of these methods work slower than a plain simple concatenation. However, I strongly believe that computers are cheaper than people . What I mean is that the time spent by programmers understanding and modifying ugly code is much more expensive than a cost of an additional server that will make beautifully written code work faster. If you know any other methods of avoiding string concatenation, please comment below. "},{"title":"Limit Java Method Execution Time","url":"/2014/06/20/limit-method-execution-time.html","date":"2014-06-20 00:00:00 +0000","categories":["2014","jun"],"body":" Say, you want to allow a Java method to work for a maximum of five seconds and want an exception to be thrown if the timeframe is exceeded. Here is how you can do it with jcabi-aspects and AspectJ : public class Resource { @Timeable ( limit = 5 , unit = TimeUnit . SECONDS ) public String load ( URL url ) { return url . openConnection (). getContent (); } } Keep in mind that you should weave your classes after compilation, as explained here . Let's discuss how this actually works, but first, I recommend you read this post , which explains how AOP aspects work together with Java annotations. Due to @Timeable annotation and class weaving, every call to a method load() is intercepted by an aspect from jcabi-aspects . That aspect starts a new thread that monitors the execution of a method every second, checking whether it is still running. If the method runs for over five seconds, the thread calls interrupt() on the method's thread. Despite a very common expectation that a thread should be terminated immediately on that call, it is not happening at all. This article explains the mechanism in more detail. Let's discuss it briefly: interrupt() sets a marker in a thread; The thread checks interrupted() as often as it can; If the marker is set, the thread stops and throws InterruptedException This method will not react to interrupt() call and will work until JVM is killed (very bad design): public void work () { while ( true ) { // do something } } This is how we should refactor it in order to make sensitive to interruption requests: public void work () { while ( true ) { if ( Thread . interruped ()) { throw new InterruptedException (); } // do something } } In other words, your method can only stop itself. Nothing else can do it. The thread it is running in can't be terminated by another thread. The best thing that the other thread can do is to send your thread a \"message\" (through interrupt() method) that it's time to stop. If your thread ignores the message, nobody can do anything. Most I/O operations in JDK are designed this way. They check the interruption status of their threads while waiting for I/O resources. Thus, use @Timeable annotation, but keep in mind that there could be situations when a thread can't be interrupted. "},{"title":"CasperJS Tests in Maven Build","url":"/2014/06/21/casperjs-with-maven.html","date":"2014-06-21 00:00:00 +0000","categories":["2014","jun"],"body":"I'm a big fan of automated testing in general and integration testing in particular. I strongly believe that effort spent on writing tests are direct investments into quality and stability of the product under development. CasperJS is a testing framework on top of PhantomJS , which is a headless browser. Using CasperJS, we can ensure that our application responds correctly to requests sent by a regular web browser. This is a sample CasperJS test, which makes an HTTP request to a home page of a running WAR application and asserts that the response has 200 HTTP status code: casper . test . begin ( 'home page can be rendered' , function ( test ) { casper . start ( casper . cli . get ( 'home' ), // URL of home page function () { test . assertHttpStatus ( 200 ); } ); casper . run ( function () { test . done (); } ); } ); I keep this test in the src/test/casperjs/home-page.js file. Let's see how CasperJS can be executed automatically on every Maven build. Here is the test scenario, implemented with a combination of Maven plugins: Install PhantomJS Install CasperJS Reserve a random TCP port Start Tomcat on that TCP port (with WAR inside) Run CasperJS tests and point them to the running Tomcat Shutdown Tomcat I'm using a combination of plugins. Let's go through the steps one by one. BTW, I'm not showing plugin versions in the examples below, primarily because most of them are in active development. Check their versions at Maven Central (yes, all of them are available there). 1. Install PhantomJS First of all, we have to download the PhantomJS executable. It is a platform-specific binary. Thanks to Kyle Lieber , we have an off-the-shelf Maven plugin: phantomjs-maven-plugin that understands what the current platform is and downloads the appropriate binary automatically, placing it into the target directory. <plugin> <groupId> com.github.klieber </groupId> <artifactId> phantomjs-maven-plugin </artifactId> <executions> <execution> <goals> <goal> install </goal> </goals> </execution> </executions> <configuration> <version> 1.9.2 </version> </configuration> </plugin> The exact name of the downloaded binary is stored in the ${phantomjs.binary} Maven property. 2. Install CasperJS Unfortunately, there is no similar plugin for the CasperJS installation (at least I haven't found any as of yet). That's why I'm using plain old git (you should have it installed on your build machine). <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> exec-maven-plugin </artifactId> <executions> <execution> <id> casperjs-install </id> <phase> pre-integration-test </phase> <goals> <goal> exec </goal> </goals> <configuration> <executable> git </executable> <arguments> <argument> clone </argument> <argument> --depth=1 </argument> <argument> https://github.com/n1k0/casperjs.git </argument> <argument> ${project.build.directory}/casperjs </argument> </arguments> </configuration> </execution> </executions> </plugin> 3. Reserve TCP Port I need to obtain a random TCP port where Tomcat will be started. The port has to be available on the build machine. I want to be able to run multiple Maven builds in parallel, so that's why I get a random port on every build. In other examples, you may see people using fixed port numbers, like 5555 or something similar. This is a very bad practice. Always reserve a new random port when you need it. <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> build-helper-maven-plugin </artifactId> <executions> <execution> <id> tomcat-port </id> <goals> <goal> reserve-network-port </goal> </goals> <configuration> <portNames> <portName> tomcat.port </portName> </portNames> </configuration> </execution> </executions> </plugin> The plugin reserves a port and sets it value to the ${tomcat.port} Maven property. 4. Start Tomcat Now, it's time to start Tomcat with the WAR package inside. I'm using tomcat7-maven-plugin that starts a real Tomcat7 server and configures it to serve on the port reserved above. <plugin> <groupId> org.apache.tomcat.maven </groupId> <artifactId> tomcat7-maven-plugin </artifactId> <configuration> <path> / </path> </configuration> <executions> <execution> <id> start-tomcat </id> <phase> pre-integration-test </phase> <goals> <goal> run-war-only </goal> </goals> <configuration> <port> ${tomcat.port} </port> <fork> true </fork> </configuration> </execution> </executions> </plugin> Due to the option fork being set to true , Tomcat7 continues to run when the plugin execution finishes. That's exactly what I need. 5. Run CasperJS Now, it's time to run CasperJS. Even though there are some plugins exist for this, I'm using plain old exec-maven-plugin , mostly because it is more configurable. <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> exec-maven-plugin </artifactId> <executions> <execution> <id> casperjs-test </id> <phase> integration-test </phase> <goals> <goal> exec </goal> </goals> <configuration> <executable> ${project.build.directory}/casperjs/bin/casperjs </executable> <workingDirectory> ${basedir} </workingDirectory> <arguments> <argument> test </argument> <argument> --verbose </argument> <argument> --no-colors </argument> <argument> --concise </argument> <argument> --home=http://localhost:${tomcat.port} </argument> <argument> ${basedir}/src/test/casperjs </argument> </arguments> <environmentVariables> <PHANTOMJS_EXECUTABLE> ${phantomjs.binary} </PHANTOMJS_EXECUTABLE> </environmentVariables> </configuration> </execution> </executions> </plugin> The environment variable PHANTOMJS_EXECUTABLE is the undocumented feature that makes this whole scenario possible. It configures the location of the PhantomJS executable, which was downloaded a few steps above. 6. Shutdown Tomcat In the last step, I shut down the Tomcat server. <plugin> <groupId> org.apache.tomcat.maven </groupId> <artifactId> tomcat7-maven-plugin </artifactId> <executions> <execution> <id> stop-tomcat </id> <phase> post-integration-test </phase> <goals> <goal> shutdown </goal> </goals> </execution> </executions> </plugin> Real Example If you want to see how this all works in action, take a look at stateful.co . It is a Java Web application hosted at CloudBees . Its source code is open and available in Github . Its pom.xml contains exactly the same configurations explained above, but joined together. If you have any questions, please don't hesitate to ask below. "},{"title":"Deploy Jekyll to Github Pages","url":"/2014/06/24/jekyll-github-deploy.html","date":"2014-06-24 00:00:00 +0000","categories":["2014","jun"],"body":"This blog is written in Jekyll and is hosted at Github Pages . It uses half a dozen custom plugins, which are not allowed there . Here is how I deploy it: $ jgd That's it. jgd is my Ruby gem (stands for \"Jekyll Github Deploy\"), which does the trick. Here is what it does : It clones your existing repository from the current directory to a temporary one (guessing the URL of the repo from .git/config file). Runs jekyll build in that temporary directory, which saves the output in another temporary directory. Checks out gh-pages branch or creates one if it doesn't exist. Copies the content of the site built by jekyll build into the branch, thus overwriting existing files, commits and pushes to Github. Cleans up all temporary directories. Using this gem is very easy. Just install it with gem install jgd and then run in the root directory of your Jekyll blog. What is important is that your Jekyll site files be located in the root directory of the repository. Just as they do on this blog; see its sources in Github . You can easily integrate jgd with Travis. See .travis.yml of this blog. Full documentation about the gem is located here . "},{"title":"XML+XSLT in a Browser","url":"/2014/06/25/xml-and-xslt-in-browser.html","date":"2014-06-25 00:00:00 +0000","categories":["2014","jun"],"body":"Separating data and their presentation is a great concept. Take HTML and CSS for example. HTML is supposed to have pure data and CSS is supposed to format that data in order to make it readable by a human. Years ago, that was probably the intention of HTML/CSS, but in reality it doesn't work like that. Mostly because CSS is not powerful enough. We still have to format our data using HTML tags, while CSS can help slightly with positioning and decorating. On the other hand, XML with XSLT implements perfectly the idea of separating data and presentation. XML documents, like HTML, are supposed to contain data only without any information about positioning or formatting. XSL stylesheets position and decorate the data. XSL is a much more powerful language. That's why it's possible to avoid any formatting inside XML. The latest versions of Chrome, Safari, FireFox and IE all support this mechanism. When a browser retrieves an XML document from a server, and the document has an XSL stylesheet associated with it — the browser transforms XML into HTML on-fly. Working Example Let's review a simple Java web application that works this way. It is using ReXSL framework that makes this mechanism possible. In the next post, I'll explain how ReXSL works. For now, though, let's focus on the idea of delivering bare data in XML and formatting it with an XSL stylesheet. Open http://www.stateful.co — it is a collection of stateful web primitives, explained in the Atomic Counters at Stateful.co article. Open it in Chrome or Safari. When you do, you should see a normal web page with a logo, some text, some links, a footer, etc. Now check its sources (I assume you know how to do this). This is approximately what you will see (I assume you understand XML, if not, start learning it immediately): <?xml-stylesheet type='text/xsl' href='/xsl/index.xsl'?> <page date= \"2014-06-15T15:30:49.521Z\" ip= \"10.168.29.135\" > <menu> home </menu> <documentation> .. some text here .. </documentation> <version> <name> 1.4 </name> <revision> 5c7b5af </revision> <date> 2014-05-29 07:58 </date> </version> <links> <link href= \"...\" rel= \"rexsl:google\" type= \"text/xml\" /> <link href= \"...\" rel= \"rexsl:github\" type= \"text/xml\" /> <link href= \"...\" rel= \"rexsl:facebook\" type= \"text/xml\" /> </links> <millis> 70 </millis> </page> As you see, it is a proper XML document with attributes, elements and data. It contains absolutely no information about how its elements have to be presented to an end-user. Actually, this document is more suitable for machine parsing instead of reading by a human. The document contains data, which is important for its requestor. It's up to the requestor on how to render the data or to not render it at all. Its second line associates the document with the XSL stylesheet /xsl/index.xsl that is loaded by the browser separately: <?xml-stylesheet type='text/xsl' href='/xsl/index.xsl'?> Open developer tools in Chrome and you will see that right after the page is loaded, the browser loads the XSL stylesheet and then all other resources including a few CSS stylesheets, jQuery and an SVG logo: index.xsl includes layout.xsl , that's why it is loaded right after. Let's consider an example of index.xsl (in reality it is much more complex, check layout.xsl . For example: <xsl:stylesheet version= \"2.0\" xmlns:xsl= \"http://www.w3.org/1999/XSL/Transform\" xmlns= \"http://www.w3.org/1999/xhtml\" > <xsl:template match= \"page\" > <html> <body> <p> Current version of the application is <xsl:value-of select= \"version/name\" /> </p> </body> </html> </xsl:template> </xsl:stylesheet> I think it's obvious how the HTML page will look like after applying this XSL stylesheet to our XML document. For me, this XSL looks clean and easy to understand. However, I often hear people say that XSLT is a hard-to-understand programming language. I don't find it hard to understand at all. Of course, I'm not using all of its features. But, for simple page rendering, all I need to know are a few simple commands and the principle of XML transformation. Why Not a Templating Engine? Now, why is this approach better than all that widely use Java templating engines, including JSP , JSF , Velocity , FreeMarker , Tiles , etc? Well, I see a number of reasons. But, the most important are: Web UI and API are same pages . There is no need to develop separate pages for RESTful API — Web user interface, being accessed by a computer, is an API. In my experience, this leads to massive avoidance of code duplication. XSL is testable by itself without a server . In order to test how our web site will look with certain data, we just create a new XML document with necessary test data, associate it with an XSL and open it in a browser. We can also modify XML and refresh the page in browser. This makes the work of HTML/CSS designer much easier and independent of programmers. XSL is a powerful functional language . Compared with all other templating engines, which look mostly like workarounds, XSL is a complete and well-designed environment. Writing XSL (after you get used to its syntax and programming concepts) is a pleasure in itself. You're not injecting instructions into a HTML document (like in JSP and all others). Instead, you are programming transformation of data into presentation — a different mindset and much better feeling. XML output is perfectly testable . A controller in MVC that generates an XML document with all data required for the XSL stylesheet can easily be tested in a single unit test using simple XPath expressions. Testing of a controller that injects data into a templating engine is a much more complex operation — even impossible sometimes. I'm also writing in PHP and Ruby. They have exactly the same problems — even though their templating engines are much more powerful due to the interpretation nature of the languages. Is It Fully Supported? Everything would be great if all browsers would support XML+XSL rendering. However, this is far from being true. Only the latest versions of modern browsers support XSL. Check this comparison done by Julian Reschke. Besides that, XSLT 2.0 is not supported at all. There is a workaround, though. We can understand which browser is making a request (via its User-Agent HTTP header) and transform XML into HTML on the server side. Thus, for modern browsers that support XSL, we will deliver XML and for all others — HTML. This is exactly how ReXSL framework works. Open http://www.stateful.co in Internet Explorer and you will see an HTML document, not an XML document as is the case with Chrome. In the next post, I'll explain ReXSL framework . "},{"title":"SASS in Java Webapp","url":"/2014/06/26/sass-in-java-webapp.html","date":"2014-06-26 00:00:00 +0000","categories":["2014","jun"],"body":"SASS is a powerful and very popular language for writing CSS style sheets. This is how I'm using SASS in my Maven projects. First, I change the extensions of .css files to .scss and move them from src/main/webapp/css to src/main/scss . Then, I configure the sass-maven-plugin (get its latest versions in Maven Central ): <plugin> <groupId> org.jasig.maven </groupId> <artifactId> sass-maven-plugin </artifactId> <executions> <execution> <id> generate-css </id> <phase> generate-resources </phase> <goals> <goal> update-stylesheets </goal> </goals> <configuration> <sassSourceDirectory> ${basedir}/src/main/scss </sassSourceDirectory> <destination> ${project.build.directory}/css </destination> </configuration> </execution> </executions> </plugin> The SASS compiler will compile .scss files from src/main/scss and place .css files into target/css . Then, I configure the minify-maven-plugin to compress/minify the style sheets produced by the SASS compiler: <plugin> <groupId> com.samaxes.maven </groupId> <artifactId> minify-maven-plugin </artifactId> <configuration> <charset> UTF-8 </charset> <nosuffix> true </nosuffix> <webappTargetDir> ${project.build.directory}/css-min </webappTargetDir> </configuration> <executions> <execution> <id> minify-css </id> <goals> <goal> minify </goal> </goals> <configuration> <webappSourceDir> ${project.build.directory} </webappSourceDir> <cssSourceDir> css </cssSourceDir> <cssSourceIncludes> <include> *.css </include> </cssSourceIncludes> <skipMerge> true </skipMerge> </configuration> </execution> </executions> </plugin> Minified .css files will be placed into target/css-min . The final step is to configure the maven-war-plugin to pick up .css files and package them into the final WAR archive: <plugin> <artifactId> maven-war-plugin </artifactId> <configuration> [..other configuration options..] <webResources combine.children= \"append\" > <resource> <directory> ${project.build.directory}/css-min </directory> </resource> </webResources> </configuration> </plugin> That's it. "},{"title":"Custom Pygments Lexer in Jekyll","url":"/2014/06/29/custom-lexer-in-jekyll.html","date":"2014-06-29 00:00:00 +0000","categories":["2014","jun"],"body":"I needed to create a custom syntax highlighting for requs.org on which I'm using Jekyll for site rendering. This is how my code blocks look in markdown pages: { % highlight requs %} User is a \"human being\". { % endhighlight %} I created a custom Pygments lexer : from pygments.lexer import RegexLexer from pygments.token import Punctuation , Text , Keyword , Name , String from pygments.util import shebang_matches class RequsLexer ( RegexLexer ): name = 'requs' aliases = [ 'requs' ] tokens = { 'root' : [ ( r'\"[^\"]+\"' , String ), ( r'\"\"\".+\"\"\"' , Text ), ( r'\\b(needs|includes|requires|when|fail|since|must|is|a|the)\\s*\\b' , Keyword ), ( r'([A-Z][a-z]+)+' , Name ), ( r'[,;:]' , Punctuation ), ], } def analyse_text ( text ): return shebang_matches ( text , r'requs' ) Then, I packaged it for easy_install and installed locally: $ easy_install src/requs_pygment Processing requs_pygment Running setup.py -q bdist_egg --dist-dir /Volumes/ssd2/code/requs/src/requs_pygment/egg-dist-tmp-ISj8Nx zip_safe flag not set ; analyzing archive contents... Adding requs-pygment 0.1 to easy-install.pth file Installed /Library/Python/2.7/site-packages/requs_pygment-0.1-py2.7.egg Processing dependencies for requs-pygment == 0.1 Finished processing dependencies for requs-pygment == 0.1 It's done. Now I run jekyll build and my syntax is highlighted according to the custom rules I specified in the lexer. "},{"title":"How to Read MANIFEST.MF Files","url":"/2014/07/03/how-to-read-manifest-mf.html","date":"2014-07-03 00:00:00 +0000","categories":["2014","jul"],"body":" Every Java package (JAR, WAR, EAR, etc.) has a MANIFEST.MF file in the META-INF directory. The file contains a list of attributes, which describe this particular package. For example: Manifest-Version: 1.0 Created-By: 1.7.0_06 (Oracle Corporation) Main-Class: MyPackage.MyClass When your application has multiple JAR dependencies, you have multiple MANIFEST.MF files in your class path. All of them have the same location: META-INF/MANIFEST.MF . Very often it is necessary to go through all of them in runtime and find the attribute by its name. jcabi-manifests makes it possible with a one-liner: import com.jcabi.manifests.Manifests ; String created = Manifests . read ( \"Created-By\" ); Let's see why you would want to read attributes from manifest files, and how it works on a low level. Package Versioning When you package a library or even a web application, it is a good practice to add an attribute to its MANIFEST.MF with the package version name and build number. In Maven, maven-jar-plugin can help you (almost the same configuration for maven-war-plugin ): <plugin> <artifactId> maven-jar-plugin </artifactId> <configuration> <archive> <manifestEntries> <Foo-Version> ${project.version} </Foo-Version> <Foo-Hash> ${buildNumber} </Foo-Hash> </manifestEntries> </archive> </configuration> </plugin> buildnumber-maven-plugin will help you to get ${buildNumber} from Git, SVN or Mercurial: <plugin> <groupId> org.codehaus.mojo </groupId> <artifactId> buildnumber-maven-plugin </artifactId> <executions> <execution> <goals> <goal> create </goal> </goals> </execution> </executions> </plugin> After all these manipulations, MANIFEST.MF , in your JAR will contain these two extra lines (on top of all others added there by Maven by default): Foo-Version: 1.0-SNAPSHOT Foo-Hash: 7ef4ac3 In runtime, you can show these values to the user to help him understand which version of the product he is working with at any given moment. Look at stateful.co , for example. At the bottom of its front page, you see the version number and Git hash. They are retrieved from MANIFEST.MF of the deployed WAR package, on every page click. Credentials Although this may be considered as a bad practice (see Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation by Jez Humble and David Farley), sometimes it is convenient to package production credentials right into the JAR/WAR archive during the continuous integration/delivery cycle. For example, you can encode your PostgreSQL connection details right into MANIFEST.MF : <plugin> <artifactId> maven-war-plugin </artifactId> <configuration> <archive> <manifestEntries> <Pgsql> jdbc:postgresql://${pg.host}:${pg.port}/${pg.db} </Pgsql> </manifestEntries> </archive> </configuration> </plugin> Afterwards, you can retrieve them in runtime using jcabi-manifests : String url = Manifests . read ( \"Pgsql\" ); If you know of any other useful purposes for MANIFEST.MF , let me know :) "},{"title":"Liquibase with Maven","url":"/2014/07/20/liquibase-in-maven.html","date":"2014-07-20 00:00:00 +0000","categories":["2014","jul"],"body":"Liquibase is a migration management tool for relational databases. It versionalizes schema and data changes in a database; similar to the way Git or SVN works for source code. Thanks to their Maven plugin , Liquibase can be used as a part of a build automation scenario. Maven Plugin Let's assume you're using MySQL (PostgreSQL or any other database configuration will be very similar.) Add liquibase-maven-plugin to your pom.xml (get its latest version in Maven Central ): <project> [...] <build> [...] <plugins> <plugin> <groupId> org.liquibase </groupId> <artifactId> liquibase-maven-plugin </artifactId> <configuration> <changeLogFile> ${basedir}/src/main/liquibase/master.xml </changeLogFile> <driver> com.mysql.jdbc.Driver </driver> <url> jdbc:mysql://${mysql.host}:${mysql.port}/${mysql.db} </url> <username> ${mysql.login} </username> <password> ${mysql.password} </password> </configuration> </plugin> </plugins> </build> </project> To check that it works, run mvn liquibase:help . I would recommend you keep database credentials in settings.xml and in their respective profiles. For example: <settings> <profiles> <profile> <id> production </id> <properties> <mysql.host> db.example.com </mysql.host> <mysql.port> 3306 </mysql.port> <mysql.db> example </mysql.db> </properties> </profile> <profile> <id> test </id> <properties> <mysql.host> test-db.example.com </mysql.host> <mysql.port> 3306 </mysql.port> <mysql.db> example-db </mysql.db> </properties> </profile> </profiles> </settings> When you run Maven, don't forget to turn on one of the profiles. For example: mvn -Pproduction . Initial Schema I assume you already have a database with a schema (tables, triggers, views, etc.) and some data. You should \"reverse engineer\" it and create an initial schema file for Liquibase. In other words, we should inform Liquibase where we are at the moment, so that it starts to apply changes from this point. Maven plugin doesn't support it, so you will have to run Liquibase directly. But, it's not that difficult. First, run mvn liquibase:help in order to download all artifacts. Then, replace placeholders with your actual credentials: $ java -jar ~/.m2/repository/org/liquibase/liquibase-core/3.1.1/liquibase-core-3.1.1.jar \\ --driver = com.mysql.jdbc.Driver \\ --url = jdbc:mysql://db.example.com:3306/example \\ --username = example --password = example \\ generateChangeLog > src/main/liquibase/000-initial-schema.xml Liquibase will analyze your current database schema and copy its own schema into src/main/liquibase/000-initial-schema.xml . Master Changeset Now, create XML master changeset and save it to src/main/liquibase/master.xml : <databaseChangeLog xmlns= \"http://www.liquibase.org/xml/ns/dbchangelog\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-2.0.xsd\" > <includeAll path= \"src/main/liquibase\" /> </databaseChangeLog> It is an entry point for Liquibase. It starts from this file and loads all other changesets available in src/main/liquibase . They should be either .xml or .sql . I recommend that you use XML mostly because it is easier to maintain and works faster. Incremental Changesets Let's create a simple changeset, which adds a new column to an existing table: <databaseChangeLog xmlns= 'http://www.liquibase.org/xml/ns/dbchangelog' xmlns:xsi= 'http://www.w3.org/2001/XMLSchema-instance' xsi:schemaLocation= 'http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-2.0.xsd' > <changeSet id= \"002\" author= \"Yegor\" > <sql> ALTER TABLE user ADD COLUMN address VARCHAR(1024); </sql> </changeSet> </databaseChangeLog> We save this file we in src/main/liquibase/002-add-user-address.xml . In big projects, you can name your files by the names of the tickets they are produced in. For example, 045-3432.xml , which means changeset number 45 coming from ticket #3432. The important thing is to have this numeric prefix in front of file names, in order to sort them correctly. We want changes to be applied in their correct chronological order. That's it. We're ready to run mvn liquibase:update -Pproduction and our production database will be updated — a new column will be added to the user table. Also, see how MySQL Maven Plugin can help you to automate integration testing of database-connected classes. "},{"title":"Master Branch Must Be Read-Only","url":"/2014/07/21/read-only-master-branch.html","date":"2014-07-21 00:00:00 +0000","categories":["2014","jul"],"body":"Continuous integration is easy. Download Jenkins, install, create a job, click the button, and get a nice email saying that your build is broken (I assume your build is automated). Then, fix broken tests (I assume you have tests), and get a much better looking email saying that your build is clean. Then, tweet about it, claiming that your team is using continuous integration. Then, in a few weeks, start filtering out Jenkins alerts, into their own folder, so that they don't bother you anymore. Anyway, your team doesn't have the time or desire to fix all unit tests every time someone breaks them. After all, we all know that unit testing is not for a team working with deadlines, right? Wrong. Continuous integration can and must work. What is Continuous Integration? Nowadays, software development is done in teams. We develop in feature branches and isolate changes while they are in development. Then, we merge branches into master . After every merge, we test the entire product, executing all available unit and integration tests. This is called continuous integration (aka \"CI\"). Sometimes, some tests fail. When this happens, we say that our \"build is broken\". Such a failure is a positive side effect of quality control because it raises a red flag immediately after an error gets into master . It is a well-known practice, when fixing that error becomes a top priority for its author and the entire team. The error should be fixed right after a red flag is raised by the continuous integration server. Continuous Delivery by Jez Humble et. al. explains this approach perfectly in Chapter 7, pages 169–186. There are a few good tools on the market, which automate CI. Some of them are open source, you can download and install them on your own servers. For example: Jenkins , Go , and CruiseControl . Some of them are available as a service in cloud, such as: Travis , Drone , Wercker , and many others. Why CI Doesn't Work? CI is great, but the bigger the team (and the code base), the more often builds get broken. And, the longer it takes to fix them. I've seen many examples where a hard working team starts to ignore red flags, raised by Jenkins, after a few weeks or trying to keep up. The team simply becomes incapable of fixing all errors in time. Mostly because the business has other priorities. Product owners do not understand the importance of a \"clean build\" and technical leaders can't buy time for fixing unit tests. Moreover, the code that broke them was already in master and, in most cases, has been already deployed to production and delivered to end-users. What's the urgency of fixing some tests if business value was already delivered? In the end, most development teams don't take continuous integration alerts seriously. Jenkins or Travis are just fancy tools for them that play no role in the entire development and delivery pipeline. No matter what continuous integration server says, we still deliver new features to our end-users. We'll fix our build later. And it's only logical. What Is a Solution? Four years ago I published an article in php|Architect called \"Prevent Conflicts in Distributed Agile PHP Projects\". In the article, a solution was proposed (full article in PDF ) for Subversion and PHP. Since that time, I used experimentally that approach in multiple open source projects and a few commercial ones with PHP, Java, Ruby and JavaScript, Git and Subversion. In all cases, my experience was only positive, and that's why rultor.com was born (later about that though). So, the solution is simple — prohibit anyone from merging anything into master and create scripts that anyone can call. The script will merge, test, and commit. The script will not make any exceptions. If any branch is breaking at even one unit test, the entire branch will be rejected. In other words, we should raise that red flag before the code gets into master . We should put the blame for broken tests on the shoulders of its author. Say, I'm developing a feature in my own branch. I finished the development and broke a few tests, accidentally. It happens, we all make mistakes. I can't merge my changes into master . Git simply rejects my push , because I don't have the appropriate permissions. All I can do is call a magic script, asking it to merge my branch. The script will try to merge, but before pushing into master , it will run all tests. And if any of them break, my branch will be rejected. My changes won't be merged. Now it's my responsibility — to fix them and call the script again. In the beginning, this approach slows down the development, because everybody has to start writing cleaner code. At the end, though, this method pays off big time. Pre-flight Builds Some CI servers offer pre-flight builds feature, which means testing branches before they get merged into master . Travis, for example, has this feature and it is very helpful. When you make a new commit to a branch, Travis immediately tries to build it, and reports in Github pull request, if there are problems. Pay attention, pre-flight builds don't merge. They just check whether your individual branch is clean. After merge, it can easily break master . And, of course, this mechanism doesn't guarantee that no collaborators can commit directly to master , breaking it accidentally. Pre-flight builds are a preventive measure, but do not solve the problem entirely. Rultor.com In order to start working as explained above, all you have to do is to revoke write permissions to master branch (or /trunk , in Subversion). Unfortunately, this is not possible in Github. The only solution is to work through forks and pull requests only. Simply remove everybody from the list of \"collaborators\" and they will have to submit changes through pull requests. Then, start using Rultor.com , which will help you to test, merge and push every pull request. Basically, Rultor is the script we were talking about above. It is available as a free cloud service. "},{"title":"Rultor.com, a Merging Bot","url":"/2014/07/24/rultor-automated-merging.html","date":"2014-07-24 00:00:00 +0000","categories":["2014","jul"],"body":" You get a Github pull request. You review it. It looks correct — it's time to merge it into master . You post a comment in it, asking @rultor to test and merge. Rultor starts a new Docker container, merges the pull request into master , runs all tests and, if everything looks clean — merges, pushes, and closes the request. Then, you ask @rultor to deploy the current version to production environment. It checks out your repository, starts a new Docker container, executes your deployment scripts and reports to you right there in the Github issue. Why not Jenkins or Travis? There are many tools on the market, which automate continuous integration and continuous delivery. For example, downloadable open-source Jenkins and hosted Travis both perform these tasks. So, why do we need one more? Well, there are three very important features that we need for our projects, but we can't find all of them in any of the CI tools currently available on the market: Merging . We make master branch read-only in our projects, as this article recommends. All changes into master we pass through a script that validates them and merges. Docker . Every build should work in its own Docker container, in order to simplify configuration, isolate resources and make errors easily reproduceable. Tell vs. Trigger . We need to communicate with CI tool through commands, right from our issue tracking system (Github issues, in most projects). All existing CI systems trigger builds on certain conditions. We need our developers to be able to talk to the tool, through human-like commands in the tickets they are working with. A combination of these three features is what differs Rultor from all other existing systems. How Rultor Merges Once Rultor finds a merge command in one of your Github pull requests, it does exactly this: Reads the .rultor.yml YAML config file from the root directory of your repository. Gets automated build execution command from it, for example bundle test . Checks out your repository into a temporary directory on one of its servers. Merges pull request into master branch. Starts a new Docker container and runs bundle test in it. If everything is fine, pushes modified master branch to Github. Reports back to you, in the Github pull request. You can see it in action, for example, in this pull request: jcabi/jcabi-github#878 . "},{"title":"Every Build in Its Own Docker Container","url":"/2014/07/29/docker-in-rultor.html","date":"2014-07-29 00:00:00 +0000","categories":["2014","jul"],"body":" Docker is a command line tool that can run a shell command in a virtual Linux, inside an isolated file system. Every time we build our projects, we want them to run in their own Docker containers. Take this Maven project for example: $ sudo docker run -i -t ubuntu mvn clean test This command will start a new Ubuntu system and execute mvn clean test inside it. Rultor.com , our virtual assistant, does exactly that with our builds, when we deploy, package, test and merge them. Why Docker? What benefits does it give us? And why Docker, when there are many other virtualization technologies , like LXC, for example? Well, there are a few very important benefits: Image repository (hub.docker.com) Versioning Application-centric Let's discuss them in details. Image Repository Docker enables image sharing through its public repository at hub.docker.com . This means that after I prepare a working environment for my application, I make an image out of it and push it to the hub. Let's say, I want my Maven build to be executed in a container with a pre-installed graphviz package (in order to enable dot command line tool). First, I would start a plain vanilla Ubuntu container, and install graphviz inside it: $ sudo docker run -i -t ubuntu /bin/bash root@215d2696e8ad:/# sudo apt-get install -y graphviz Reading package lists... Done Building dependency tree Reading state information... Done The following extra packages will be installed: ... root@215d2696e8ad:/# exit $ sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 215d2696e8ad ubuntu:14.04 /bin/bash About a minute ago Exited ( 0 ) 3 seconds ago high_mccarthy I have a container that stopped a few seconds ago. Container's ID is 215d2696e8ad . Now, I want to make it reusable for all further tests in Rultor.com. I have to create an image from it: $ sudo docker commit 215d2696e8ad yegor256/beta c5ad7718fc0e20fe4bf2c8a9bfade4db8617a25366ca5b64be2e1e8aa0de6e52 I just made my new commit to a new image yegor256/beta . This image can be reused right now. I can create a new container from this image and it will have graphviz installed inside! Now it's time to share my image at Docker hub, in order to make it available for Rultor: $ sudo docker push yegor256/beta The push refers to a repository [ yegor256/beta ] ( len: 1 ) Sending image list Pushing repository yegor256/beta ( 1 tags ) 511136ea3c5a: Image already pushed, skipping d7ac5e4f1812: Image already pushed, skipping 2f4b4d6a4a06: Image already pushed, skipping 83ff768040a0: Image already pushed, skipping 6c37f792ddac: Image already pushed, skipping e54ca5efa2e9: Image already pushed, skipping c5ad7718fc0e: Image successfully pushed Pushing tag for rev [ c5ad7718fc0e ] on { https://registry-1.docker.io/v1/repositories/yegor256/beta/tags/latest } The last step is to configure Rultor to use this image in all builds. To do this, I will edit .rultor.yml in the root directory of my Github repository: docker : image : yegor256/beta That's it. From now on, Rultor will use my custom Docker image with pre-installed graphviz, in every build (merge, release, deploy, etc.) Moreover, if and when I want to add something else to the image, it's easy to do. Say, I want to install Ruby into my build image. I start a container from the image and install it (pay attention, I'm starting a container not from ubuntu image, as I did before, but from yegor256/beta ): $ sudo docker run -i -t yegor256/beta /bin/bash root@7e0fbd9806c9:/# sudo apt-get install -y ruby Reading package lists... Done Building dependency tree Reading state information... Done The following extra packages will be installed: ... root@7e0fbd9806c9:/# exit $ sudo docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e0fbd9806c9 yegor256/beta:latest /bin/bash 28 seconds ago Exited ( 0 ) 2 seconds ago pensive_pare 215d2696e8ad ubuntu:14.04 /bin/bash 10 minutes ago Exited ( 0 ) 8 minutes ago high_mccarthy You can now see that I have two containers. The first one is the one I am using right now; it contains Ruby. The second one is the one I was using before and it contains graphviz. Now I have to commit again and push: $ sudo docker commit 7e0fbd9806c9 yegor256/beta 6cbfb7a6b18a2182f42171f6bb5aef67c4819b5c2795edffa6a63ba78aaada2d $ sudo docker push yegor256/beta ... Thus, this Docker hub is a very convenient feature for Rultor and similar systems. Versioning As you saw in the example above, every change to a Docker image has its own version (hash) and it's possible to track changes. It is also possible to roll back to any particular change. Rultor is not using this functionality itself, but Rultor users are able to control their build configurations with much better precision. Application-Centric Docker, unlike LXC or Vagrant, for example, is application-centric. This means that when we start a container — we start an application. With other virtualization technologies, when you get a virtual machine — you get a fully functional Unix environment, where you can login through SSH and do whatever you want. Docker makes things simpler. It doesn't give you SSH access to container, but runs an application inside and shows you its output. This is exactly what we need in Rultor. We need to run an automated build (for example Maven or Bundler), see its output and get its exit code. If the code is not zero, we fail the build and report to the user. This is how we run Maven build: $ sudo docker run --rm -i -t yegor256/rultor mvn clean test [ INFO ] ------------------------------------------------------------------------ [ INFO ] Building jcabi-github 0.13 [ INFO ] ------------------------------------------------------------------------ [ INFO ] [ INFO ] --- maven-clean-plugin:2.5:clean ( default-clean ) @ jcabi-github --- [ INFO ] ... As you can see, Maven starts immediately. We don't worry about the internals of the container. We just start an application inside it. Furthermore, thanks to the --rm option, the container gets destroyed immediately after Maven execution is finished. This is what application-centric is about. Our overall impression of Docker is highly positive. "},{"title":"Rultor + Travis","url":"/2014/07/31/travis-and-rultor.html","date":"2014-07-31 00:00:00 +0000","categories":["2014","jul"],"body":" Rultor is a coding team assistant. Travis is a hosted continuous integration system. In this article I'll show how our open source projects are using them in tandem to achieve seamless continuous delivery. I'll show a few practical scenarios. Scenario #1: Merge Pull Request jcabi-mysql-maven-plugin is a Maven plugin for MySQL integration testing . @ChristianRedl submitted pull request #35 with a new feature. I reviewed the request and asked Rultor to merge it into master : As you can see, an actual merge operation was made by Rultor. I gave him access to the project by adding his Github account to the list of project collaborators. Before giving a \"go ahead\" to Rultor I checked the status of the pre-build reported by Travis: Travis found a new commit in the pull request and immediately (without any interaction from my side) triggered a build in that branch. The build didn't fail, that's why Travis gave me a green sign. I looked at that sign and at the code. Since all problems in the code were corrected by the pull request author and Travis didn't complain — I gave a \"go\" to Rultor. Scenario #2: Continuous Integration Even though the previous step guarantees that master branch is always clean and stable, we're using Travis to continuously integrate it. Every commit made to master triggers a new build in Travis. The result of the build changes the status of the project in Travis: either \"failing\" or \"passing\". jcabi-aspects is a collection of AOP AspectJ aspects . We configured Travis to build it continuously. This is the badge it produces (the left one): Again, let me stress that even through read-only master is a strong protection against broken builds, it doesn't guarantee that at any moment master is stable. For example, sometimes unit tests fail sporadically due to changes in calendar, in environment, in dependencies, in network connection qualities, etc. Well, ideally, unit tests should either fail or pass because they are environment independent. However, in reality, unit tests are far from being ideal. That's why a combination of read-only master with Rultor and continuous integration with Travis gives us higher stability. Scenario #3: Release to RubyGems jekyll-github-deploy is a Ruby gem that automates deployment of Jekyll sites to Github Pages . @leucos submitted a pull request #4 with a new feature. The request was merged successfully into master branch. Then, Rultor was instructed by myself that master branch should be released to RubyGems and a new version to set is 1.5: Rultor executed a simple script, pre-configured in its .rultor.yml : release : script : - \"./test.sh\" - \"rm -rf *.gem\" - \"sed -i \\\"s/2.0-SNAPSHOT/${tag}/g\\\" jgd.gemspec\" - \"gem build jgd.gemspec\" - \"chmod 0600 ../rubygems.yml\" - \"gem push *.gem --config-file ../rubygems.yml\" The script is parameterized, as you see. There is one parameter that is passed by Rultor into the script: ${tag} . This parameter was provided by myself in the Github issue, when I submitted a command to Rultor. The script tests that the gem works (integration testing) and clean up afterwords: $ ./test.sh $ rm -rf *.gem Then, it changes the version of itself in jgd.gemspec to the one provided in the ${tag} (it is an environment variable), and builds a new .gem : $ sed -i \"s/2.0-SNAPSHOT/${tag}/g\" jgd.gemspec $ gem build jgd.gemspec Finally, it pushes a newly built .gem to RubyGems, using login credentials from ../rubygems.yml . This file is created by Rultor right before starting the script (this mechanism is discussed below): $ chmod 0600 ../rubygems.yml $ gem push *.gem --config-file ../rubygems.yml If everything works fine and RubyGems confirms successful deployment, Rultor reports to Github. This is exactly what happened in pull request #4 . Scenario #4: Deploy to CloudBees s3auth.com is a Basic HTTP authentication gateway for Amazon S3 Buckets . It is a Java web app. In its pull request #195 , a resource leakage problem was fixed by @carlosmiranda and the pull request was merged by Rultor. Then, @davvd instructed Rultor to deploy master branch to production environment. Rultor created a new Docker container and ran mvn clean deploy in it. Maven deployed the application to CloudBees : The overall procedure took 21 minutes, as you see the in the report generated by Rultor. There is one important trick worth mentioning. Deployment to production always means using secure credentials, like login, password, SSH keys, etc. In this particular example, Maven CloudBees Plugin needed API key, secret and web application name. These three parameters are kept secure and can't be revealed in an \"open source\" way. So, there is a mechanism that configures Rultor accordingly through its .rultor.yml file (pay attention to the first few lines): assets : settings.xml : \"yegor256/home#assets/s3auth/settings.xml\" pubring.gpg : \"yegor256/home#assets/pubring.gpg\" secring.gpg : \"yegor256/home#assets/secring.gpg\" These YAML entries inform Rultor that it has to get assets/s3auth/settings.xml file from yegor256/home private (!) Github repository and put it into the working directory of Docker container, right before starting the Maven build. This settings.xml file contains that secret data CloudBees plugin needs in order to deploy the application. You Can Do The Same Both Rultor and Travis are free hosted products, provided your projects are open source and hosted at Github. Other good examples of Rultor+Travis usage can be seen in these Github issues: jcabi/jcabi-http#47 , jcabi/jcabi-http#48 "}]}